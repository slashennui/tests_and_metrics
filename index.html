<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Data Science Model Evaluation Metrics Dashboard</title>

<!-- MathJax for mathematical notation -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style>
body { font-family: sans-serif; line-height: 1.5; margin: 20px; }
h1 { font-size: 1.8em; margin-bottom: 0; }
h2 { font-size: 1.4em; margin-top: 1.5em; margin-bottom: 0.5em; }

details summary {
  font-weight: bold;
  font-size: 1.1em;
  cursor: pointer;
  margin: 1em 0;
}
details summary::-webkit-details-marker { display: none; }

table { width: 100%; border-collapse: collapse; margin-bottom: 1.5em; }

/* No outer borders, just separators */
th, td {
  border: none;
  border-bottom: 1px solid #ddd;
  padding: 12px 8px;
  vertical-align: top;
}
th {
  background: none;
  text-align: left;
}

/* Alternating row shading */
tbody tr:nth-child(even) {
  background-color: #f9f9f9;
}

/* Status badges */
.good {
  background-color: #c8e6c9;
  color: #256029;
  font-weight: bold;
  padding: 2px 5px;
  border-radius: 3px;
}
.moderate {
  background-color: #fff9c4;
  color: #827717;
  font-weight: bold;
  padding: 2px 5px;
  border-radius: 3px;
}
.poor {
  background-color: #ffccbc;
  color: #b71c1c;
  font-weight: bold;
  padding: 2px 5px;
  border-radius: 3px;
}

/* ====== COLOR BARS (shared) ====== */
.colorbar {
  height: 15px;
  border-radius: 8px;
  margin-bottom: 4px;
}

/* 0‚Äì1 metrics: higher is better */
.colorbar-standard {
  background: linear-gradient(to right,
    #ff8c00 0%,    /* low = poor */
    #ff8c00 50%,
    #ffecb3 50%,   /* mid = moderate */
    #ffecb3 70%,
    #ffeb3b 70%,   /* good */
    #ffeb3b 80%,
    #81c784 80%,   /* very good */
    #81c784 100%
  );
}

/* F1 Score */
.colorbar-f1 {
  background: linear-gradient(to right,
    #ff8c00 0%,    /* <0.5 poor */
    #ff8c00 50%,
    #ffeb3b 50%,   /* 0.5‚Äì0.8 good */
    #ffeb3b 80%,
    #81c784 80%,   /* >0.8 very good */
    #81c784 100%
  );
}

/* ROC / PR AUC */
.colorbar-auc {
  background: linear-gradient(to right,
    #ff8c00 0%,    /* <0.7 poor */
    #ff8c00 70%,
    #ffecb3 70%,   /* 0.7‚Äì0.8 moderate */
    #ffecb3 80%,
    #81c784 80%,   /* >0.8 good */
    #81c784 100%
  );
}

/* R¬≤ / Adjusted R¬≤ (0‚Äì1) */
.colorbar-r2 {
  background: linear-gradient(to right,
    #ff8c00 0%,    /* weak */
    #ff8c00 25%,
    #ffecb3 25%,   /* moderate */
    #ffecb3 50%,
    #81c784 50%,   /* strong */
    #81c784 100%
  );
}

/* Correlations & symmetric -1..+1 type scales (|value| small = poor, large = good) */
.colorbar-correlation {
  background: linear-gradient(to right,
    #256029 0%,    /* strong negative (large |r|, good strength) */
    #c8e6c9 20%,   /* moderate negative */
    #ffccbc 45%,   /* near 0 (weak, poor) */
    #ffccbc 55%,
    #c8e6c9 80%,   /* moderate positive */
    #256029 100%   /* strong positive */
  );
}

/* Cohen's kappa and MCC: -1 (bad) .. 1 (good) */
.colorbar-kappa,
.colorbar-mcc {
  background: linear-gradient(to right,
    #b71c1c 0%,    /* -1 worse than random (bad) */
    #ffccbc 40%,   /* around 0 chance-level */
    #c8e6c9 70%,   /* decent positive */
    #256029 100%   /* near 1 excellent */
  );
}

/* P-values for testing effects: small p (evidence for effect) = good */
.colorbar-pvalue-effect {
  background: linear-gradient(to right,
    #c8e6c9 0%,    /* 0‚Äì0.05 significant (good) */
    #c8e6c9 5%,
    #ffccbc 5%,    /* >0.05 non-significant (less useful) */
    #ffccbc 100%
  );
}

/* P-values for assumption checks: small p (violation) = bad, large p (no violation) = good */
.colorbar-pvalue-assump {
  background: linear-gradient(to right,
    #ffccbc 0%,    /* 0‚Äì0.05 violation (bad) */
    #ffccbc 5%,
    #c8e6c9 5%,    /* >0.05 no evidence of violation (good) */
    #c8e6c9 100%
  );
}

/* t-statistics |t|: bigger = stronger evidence */
.colorbar-tstat {
  background: linear-gradient(to right,
    #ffccbc 0%,    /* ~0‚Äì1 weak (bad) */
    #fff9c4 50%,   /* ~2 marginal */
    #c8e6c9 75%,   /* ~3 strong */
    #256029 100%   /* very strong */
  );
}

/* Chi-square (0 ‚Üí critical ‚Üí large). For effect tests: small œá¬≤ = little evidence, large = strong evidence */
.colorbar-chi2 {
  background: linear-gradient(to right,
    #ffccbc 0%,    /* near 0: no detectable difference (poor for effect detection) */
    #ffccbc 40%,
    #c8e6c9 60%,   /* around/above critical: some evidence */
    #256029 100%   /* far above: strong evidence of difference */
  );
}

/* F-statistic (ANOVA): larger = stronger evidence some means differ */
.colorbar-fstat {
  background: linear-gradient(to right,
    #ffccbc 0%,    /* ~1 no difference */
    #fff9c4 40%,   /* ~2 marginal */
    #c8e6c9 70%,   /* ~3 clear */
    #256029 100%   /* 5+ very strong */
  );
}

/* CI width / precision: narrower = better */
.colorbar-ciwidth {
  background: linear-gradient(to right,
    #c8e6c9 0%,    /* narrow/precise (good) */
    #fff9c4 50%,   /* moderate */
    #ffccbc 100%   /* very wide/uncertain (bad) */
  );
}

/* Generic robustness / stability bar: more stable = better */
.colorbar-robustness {
  background: linear-gradient(to right,
    #b71c1c 0%,    /* very unstable (bad) */
    #ffccbc 30%,
    #c8e6c9 70%,
    #256029 100%   /* very stable (good) */
  );
}

/* Bias‚ÄìVariance tradeoff: both extremes bad, middle good */
.colorbar-biasvar {
  background: linear-gradient(to right,
    #ffccbc 0%,    /* underfit: high bias */
    #c8e6c9 50%,   /* balanced */
    #ffccbc 100%   /* overfit: high variance */
  );
}

/* Error magnitude (RMSE / MAE etc.): lower better */
.colorbar-error {
  background: linear-gradient(to right,
    #c8e6c9 0%,    /* 0 error (good) */
    #fff9c4 50%,   /* moderate */
    #ffccbc 100%   /* large error (bad) */
  );
}

/* Skewness / Kurtosis / Z-score: 0 best, extremes bad */
.colorbar-skewness,
.colorbar-kurtosis,
.colorbar-zscore {
  background: linear-gradient(to right,
    #b71c1c 0%,    /* extreme negative (bad) */
    #ffccbc 20%,
    #c8e6c9 50%,   /* near 0 (good) */
    #ffccbc 80%,
    #b71c1c 100%   /* extreme positive (bad) */
  );
}

/* IQR fences: central IQR good, outside fences bad */
.colorbar-iqr {
  background: linear-gradient(to right,
    #b71c1c 0%,    /* beyond lower fence (bad) */
    #b71c1c 10%,
    #fff9c4 10%,   /* between fence & Q1 */
    #fff9c4 25%,
    #c8e6c9 25%,   /* IQR (Q1‚ÄìQ3, good) */
    #c8e6c9 75%,
    #fff9c4 75%,   /* between Q3 & upper fence */
    #fff9c4 90%,
    #b71c1c 90%,   /* beyond upper fence (bad) */
    #b71c1c 100%
  );
}

/* Cook's distance: 0 low influence (good), large = bad */
.colorbar-cookd {
  background: linear-gradient(to right,
    #c8e6c9 0%,    /* low influence (good) */
    #c8e6c9 25%,
    #fff9c4 50%,   /* moderate */
    #ffccbc 75%,   /* high */
    #b71c1c 100%   /* extreme */
  );
}

/* Mahalanobis distance squared vs chi-square cutoffs */
.colorbar-mahal {
  background: linear-gradient(to right,
    #c8e6c9 0%,    /* inside main ellipsoid (good) */
    #c8e6c9 60%,
    #fff9c4 80%,   /* near cutoff (watch) */
    #b71c1c 100%   /* extreme outliers (bad) */
  );
}

/* VIF thresholds */
.colorbar-vif {
  background: linear-gradient(to right,
    #c8e6c9 0%,    /* ~1‚Äì5 OK (good) */
    #c8e6c9 33%,
    #fff9c4 33%,   /* 5‚Äì10 moderate concern */
    #fff9c4 66%,
    #ffccbc 66%,   /* >10 serious (bad) */
    #b71c1c 100%
  );
}

/* Condition Number thresholds */
.colorbar-cond {
  background: linear-gradient(to right,
    #c8e6c9 0%,    /* <10 low (good) */
    #c8e6c9 33%,
    #fff9c4 33%,   /* 10‚Äì30 moderate */
    #fff9c4 66%,
    #ffccbc 66%,   /* >30 severe (bad) */
    #b71c1c 100%
  );
}

/* ŒîAIC / ŒîBIC vs best model: larger = worse */
.colorbar-delta {
  background: linear-gradient(to right,
    #c8e6c9 0%,    /* 0‚Äì2 essentially tied (good) */
    #c8e6c9 20%,
    #fff9c4 20%,   /* 2‚Äì10 some loss */
    #fff9c4 60%,
    #ffccbc 60%,   /* >10 much worse (bad) */
    #b71c1c 100%
  );
}

/* Mallows Cp difference vs (p+1) (0 best) */
.colorbar-cp {
  background: linear-gradient(to right,
    #ffccbc 0%,    /* far below p+1 (overfit, bad) */
    #c8e6c9 50%,   /* near p+1 good */
    #ffccbc 80%,   /* moderately above (underfit) */
    #b71c1c 100%   /* far above (underfit, bad) */
  );
}

/* Durbin‚ÄìWatson: 0..4, best near 2 */
.colorbar-dw {
  background: linear-gradient(to right,
    #b71c1c 0%,    /* strong positive autocorr (bad) */
    #ffccbc 25%,
    #c8e6c9 50%,   /* around 2 good */
    #ffccbc 75%,
    #b71c1c 100%   /* strong negative autocorr (bad) */
  );
}















/* ===== Cutting-edge metrics badges ===== */

.cutting-list {
  margin-top: 0.75rem;
}

.cutting-header {
  display: flex;
  align-items: center;
  gap: 0.5rem;
  margin-bottom: 0.25rem;
}

.cutting-badge {
  display: inline-block;
  padding: 2px 8px;
  border-radius: 999px;
  font-size: 11px;
  line-height: 1.3;
  border: 1px solid transparent;
}

.cutting-badge--prod {
  background: #dcfce7;
  color: #166534;
}

.cutting-badge--practice {
  background: #fef9c3;
  color: #854d0e;
}

.cutting-badge--research {
  background: #e0e7ff;
  color: #3730a3;
}

.dark-mode .cutting-badge--prod {
  background: rgba(22, 163, 74, 0.15);
  border-color: rgba(22, 163, 74, 0.4);
  color: #bbf7d0;
}

.dark-mode .cutting-badge--practice {
  background: rgba(202, 138, 4, 0.15);
  border-color: rgba(202, 138, 4, 0.4);
  color: #facc15;
}

.dark-mode .cutting-badge--research {
  background: rgba(59, 130, 246, 0.12);
  border-color: rgba(59, 130, 246, 0.45);
  color: #bfdbfe;
}

.cutting-legend {
  display: flex;
  flex-wrap: wrap;
  gap: 0.5rem;
  margin-top: 0.5rem;
}















/* Wrapper + numeric scale under color bars */
.colorbar-wrapper { margin-bottom: 8px; }
.colorbar-scale {
  position: relative;
  height: 18px;
  margin-top: 2px;
  font-size: 0.75em;
}
.colorbar-scale span {
  position: absolute;
  bottom: 0;
  transform: translateX(-50%);
  white-space: nowrap;
}
.colorbar-scale span::before {
  content: "";
  position: absolute;
  top: -6px;
  left: 50%;
  width: 1px;
  height: 6px;
  background-color: #555;
  transform: translateX(-50%);
}

/* Tick positions: generic 0‚Äì1 */
.tick-0   { left: 0%; }
.tick-02  { left: 20%; }
.tick-025 { left: 25%; }
.tick-05  { left: 50%; }
.tick-07  { left: 70%; }
.tick-08  { left: 80%; }
.tick-10  { left: 100%; }

/* Correlation ticks (-1..1) */
.tick-m1  { left: 0%; }
.tick-m05 { left: 25%; }
.tick-0c  { left: 50%; }
.tick-05c { left: 75%; }
.tick-1c  { left: 100%; }

/* P-value ticks */
.tick-p0   { left: 0%; }
.tick-p005 { left: 5%; }
.tick-p1   { left: 100%; }

/* t-stat ticks */
.tick-t0 { left: 0%; }
.tick-t1 { left: 33%; }
.tick-t2 { left: 66%; }
.tick-t3 { left: 100%; }

/* Chi-square ticks */
.tick-chi2-0    { left: 0%; }
.tick-chi2-crit { left: 50%; }
.tick-chi2-high { left: 100%; }

/* F-stat ticks */
.tick-f1 { left: 20%; }
.tick-f2 { left: 45%; }
.tick-f3 { left: 70%; }
.tick-f5 { left: 100%; }

/* CI width ticks */
.tick-ci-narrow { left: 0%; }
.tick-ci-mid    { left: 50%; }
.tick-ci-wide   { left: 100%; }

/* Robustness ticks */
.tick-robust-low  { left: 0%; }
.tick-robust-mid  { left: 50%; }
.tick-robust-high { left: 100%; }

/* Error ticks */
.tick-err0   { left: 0%; }
.tick-errmid { left: 50%; }
.tick-errhi  { left: 100%; }

/* Skewness ticks (-3..3) */
.tick-skew-m3 { left: 0%; }
.tick-skew-m1 { left: 25%; }
.tick-skew-0  { left: 50%; }
.tick-skew-1  { left: 75%; }
.tick-skew-3  { left: 100%; }

/* Kurtosis ticks (-2..2) */
.tick-kurt-m2 { left: 0%; }
.tick-kurt-0  { left: 50%; }
.tick-kurt-2  { left: 100%; }

/* Z-score ticks (-3..3) */
.tick-z-m3 { left: 0%; }
.tick-z-m2 { left: 25%; }
.tick-z-0  { left: 50%; }
.tick-z-2 { left: 75%; }
.tick-z-3 { left: 100%; }

/* IQR ticks */
.tick-iqr-lf  { left: 10%; }  /* lower fence */
.tick-iqr-q1  { left: 25%; }
.tick-iqr-med { left: 50%; }
.tick-iqr-q3  { left: 75%; }
.tick-iqr-uf  { left: 90%; }  /* upper fence */

/* Cook's D ticks */
.tick-cook-0  { left: 0%; }
.tick-cook-05 { left: 25%; }
.tick-cook-1  { left: 50%; }
.tick-cook-2  { left: 100%; }

/* Mahalanobis ticks */
.tick-mah-0   { left: 0%; }
.tick-mah-975 { left: 60%; }
.tick-mah-99  { left: 80%; }
.tick-mah-ext { left: 100%; }

/* VIF ticks */
.tick-vif1  { left: 0%; }
.tick-vif5  { left: 33%; }
.tick-vif10 { left: 66%; }
.tick-vif20 { left: 100%; }

/* Condition number ticks */
.tick-cond1  { left: 0%; }
.tick-cond10 { left: 33%; }
.tick-cond30 { left: 66%; }
.tick-cond60 { left: 100%; }

/* ŒîAIC/BIC ticks */
.tick-delta0  { left: 0%; }
.tick-delta2  { left: 20%; }
.tick-delta10 { left: 60%; }
.tick-delta20 { left: 100%; }

/* Mallows Cp ticks (difference vs p+1) */
.tick-cp-neg10 { left: 0%; }
.tick-cp-0     { left: 50%; }
.tick-cp-pos10 { left: 100%; }

/* Durbin‚ÄìWatson ticks (0,2,4) */
.tick-dw0 { left: 0%; }
.tick-dw2 { left: 50%; }
.tick-dw4 { left: 100%; }

/* Table wrapper */
.table-responsive { overflow-x: auto; }

/* MathJax background fix */
mjx-container {
  display: inline-block;
  margin: 0 2px;
  background-color: transparent !important;
}

/* Header + buttons */
.page-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 20px;
}
.controls button {
  background-color: #f9f9f9;
  border: 1px solid #ccc;
  padding: 8px 15px;
  margin-left: 10px;
  border-radius: 20px;
  cursor: pointer;
  font-size: 0.9em;
}
.controls button:hover {
  background-color: #eee;
}

/* Mobile tweaks */
@media (max-width: 768px) {
  body {
    margin: 10px;
    font-size: 14px;
  }

  h1 {
    font-size: 1.4em;
  }

  .page-header {
    flex-direction: column;
    align-items: flex-start;
    gap: 0.5rem;
  }

  .controls button {
    margin-left: 0;
    margin-top: 6px;
    font-size: 0.85em;
    padding: 6px 12px;
  }

  table {
    font-size: 0.85em;
  }

  .table-responsive {
    overflow-x: auto;
    -webkit-overflow-scrolling: touch;
  }
}

/* Mobile layout for colorbar labels */
@media (max-width: 600px) {
  .colorbar-wrapper {
    max-width: 220px;
  }

  .colorbar-scale {
    position: static;
    height: auto;
    margin-top: 4px;
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    flex-wrap: wrap;
  }

  .colorbar-scale span {
    position: static;
    transform: none;
    font-size: 0.7em;
    margin: 0 2px;
    white-space: nowrap;
  }

  .colorbar-scale span::before {
    position: relative;
    display: block;
    top: -4px;
    left: 50%;
    transform: translateX(-50%);
    width: 1px;
    height: 6px;
    background-color: #555;
    margin-bottom: 2px;
  }
}

/* === Dark mode === */
body.dark {
  background-color: #111;
  color: #eee;
}
body.dark table tbody tr:nth-child(even) {
  background-color: #222;
}
body.dark th,
body.dark td {
  border-bottom-color: #444;
}
body.dark .page-header {
  border-bottom: 1px solid #444;
}
body.dark .controls button {
  background-color: #222;
  border-color: #555;
  color: #eee;
}
body.dark .controls button:hover {
  background-color: #333;
}
body.dark details summary {
  color: #fff;
}
body.dark .good {
  background-color: #336838;
  color: #e8f5e9;
}
body.dark .moderate {
  background-color: #8d6e63;
  color: #fffde7;
}
body.dark .poor {
  background-color: #c62828;
  color: #ffebee;
}
  <title>Python Code Snippets</title>
  <style>
    /* General styling for code blocks */
    pre {
      background: #f8f8f8;
      padding: 15px;
      border-radius: 5px;
      overflow-x: auto;
      font-family: 'Courier New', monospace;
      font-size: 14px;
      line-height: 1.5;
    }

    /* Syntax highlighting classes */
    .keyword { color: #0077aa; font-weight: bold; }
    .string { color: #dd1144; }
    .comment { color: #999988; font-style: italic; }
    .function { color: #7d9029; }
    .number { color: #009999; }
    .operator { color: #aa22ff; }
        <title>Embed Google Colab Notebook</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f9;
        }
        h1 {
            color: #333;
        }
        .colab-container {
            width: 100%;
            max-width: 1000px;
            margin: 20px auto;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
            overflow: hidden;
        }
        iframe {
            width: 100%;
            height: 600px;
            border: none;
        }
            <title>Interactive Python Notebook</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f9;
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .colab-container {
            width: 100%;
            max-width: 1000px;
            margin: 20px auto;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
            overflow: hidden;
        }
        iframe {
            width: 100%;
            height: 600px;
            border: none;
        }
            <title>Embed GitHub Gist</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f9;
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .gist-container {
            width: 100%;
            max-width: 1000px;
            margin: 20px auto;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
            overflow: hidden;
            background: white;
        }
        /* ====== ADDED: COLOR LEGEND STYLING ====== */
        .color-legend {
          margin: 2em 0;
          padding: 1.2em;
          background-color: #f8f9fa;
          border-radius: 8px;
          border-left: 5px solid #4CAF50;
          box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        .legend-title {
          margin-top: 0;
          color: #2c3e50;
          font-size: 1.2em;
        }
        .legend-items {
          display: flex;
          flex-wrap: wrap;
          gap: 1.5em;
          margin-top: 0.8em;
        }
        .legend-item {
          display: flex;
          align-items: center;
        }
        .legend-color-box {
          width: 22px;
          height: 22px;
          border-radius: 4px;
          margin-right: 10px;
          flex-shrink: 0;
        }
        .legend-note {
          margin-top: 0.8em;
          font-size: 0.9em;
          color: #666;
          font-style: italic;
          padding-top: 0.8em;
          border-top: 1px dashed #ddd;
        }
        body.dark .color-legend {
          background-color: #1e1e1e;
          border-left-color: #81c784;
          box-shadow: 0 2px 6px rgba(0,0,0,0.6);
        }
        
        body.dark .legend-title {
          color: #fff;
        }
        
        body.dark .legend-note {
          color: #ccc;
          border-top-color: #444;
        }

        .pitfall-intro {
          margin: 12px 0 18px 0;
          padding: 10px 14px;
          border-left: 4px solid #4b5563;     /* subtle accent bar */
          background: rgba(148, 163, 184, 0.08); /* very light slate */
          font-size: 0.9rem;
          line-height: 1.4;
        }
        
        .pitfall-intro ul {
          margin: 6px 0 0 1.25rem;
          padding: 0;
        }
        
        .pitfall-intro li {
          margin-bottom: 4px;
        }
        
        
                .card.quick-overview {
          margin: 1rem 0 1.5rem;
          padding: 1rem 1.25rem;
          border-radius: 8px;
          background: var(--card-bg, #fafafa);
          border: 1px solid var(--border-soft, #e0e0e7);
          font-size: 0.9rem;
        }
        
        .card.quick-overview details > summary {
          cursor: pointer;
          font-weight: 600;
          list-style: none;
        }
        
        .card.quick-overview details > summary::-webkit-details-marker {
          display: none;
        }
        
        .card.quick-overview details[open] > summary::after {
          content: " ‚ñ¥";
        }
        
        .card.quick-overview details:not([open]) > summary::after {
          content: " ‚ñæ";
        }
        
        .quick-overview-content h4 {
          margin-top: 0.75rem;
          margin-bottom: 0.3rem;
          font-size: 0.9rem;
        }
        
        .quick-overview-content ul,
        .quick-overview-content ol {
          margin-top: 0;
          padding-left: 1.25rem;
        }


        /* --- Page title: light mode --- */
        .page-title {
          margin: 0;
          padding: 0.6rem 1.2rem;
          font-size: 1.1rem;
          font-weight: 600;
          background: #f5f5f7;
          color: #111827;
          border-bottom: 1px solid #e5e7eb;
        }
        
        /* optional: keep header layout tight */
        .page-header {
          margin: 0 0 0.5rem 0;
        }
        
        /* --- Page title: dark mode override --- */
        body.dark-mode .page-title {
          background: #020617;   /* dark panel */
          color: #f9fafb;        /* light text */
          border-color: #1f2937;
        }
        
                





        .section-banner.condensed-banner {
          margin: 0;
          padding: 0.45rem 1.2rem;
          font-size: 0.95rem;
          font-weight: 600;
          background: #f5f5f7;
          color: #111827;
          border-top: 1px solid #e5e7eb;
          border-bottom: 1px solid #e5e7eb;
        }
        
        /* dark mode */
        body.dark-mode .section-banner.condensed-banner {
          background: #020617;
          color: #f9fafb;
          border-color: #1f2937;
        }
        
        



        
        /* ============================
           Condensed Reference: base (light mode)
           ============================ */
        
        #condensed-reference-wrapper {
          margin: 1.5rem 0 2rem;
        }
        
        .ref-toggle-btn {
          display: inline-block;
          width: 100%;
          text-align: left;
          padding: 0.55rem 0.9rem;
          font-size: 0.95rem;
          font-weight: 600;
          border-radius: 6px;
          border: 1px solid #e5e7eb;
          background: #f9fafb;
          color: #111827;
          cursor: pointer;
          transition: background 0.15s ease, color 0.15s ease, border-color 0.15s ease;
        }
        
        .ref-toggle-btn:hover {
          background: #eef2ff;
          border-color: #c7d2fe;
        }
        
        /* The expanded content panel */
        .ref-section {
          margin-top: 0.6rem;
          padding: 1rem 1.1rem 1.25rem;
          border-radius: 8px;
          border: 1px solid #e5e7eb;
          background: #ffffff;
        }
        
        /* Header inside the condensed sheet */
        .ref-header h2 {
          margin: 0;
          font-size: 1.05rem;
          font-weight: 650;
          color: #111827;
        }
        
        .ref-subtitle {
          margin: 0.25rem 0 0.75rem;
          font-size: 0.85rem;
          color: #4b5563;
        }
        
        /* Cards / subsections inside the sheet */
        .ref-card {
          padding: 0.75rem 0 0.4rem;
          border-top: 1px solid #e5e7eb;
        }
        
        .ref-card:first-of-type {
          border-top: none;
          padding-top: 0;
        }
        
        .ref-card h3 {
          margin: 0 0 0.4rem;
          font-size: 0.9rem;
          font-weight: 600;
          color: #111827;
        }
        
        .ref-list {
          padding-left: 1.2rem;
          margin: 0;
          font-size: 0.85rem;
          color: #111827;
        }
        
        .ref-list li {
          margin-bottom: 0.4rem;
        }
        
        /* ============================
           Condensed Reference: dark mode
           ============================ */
        
        body.dark-mode .ref-toggle-btn {
          background: #020617;
          color: #e5e7eb;
          border-color: #1f2937;
        }
        
        body.dark-mode .ref-toggle-btn:hover {
          background: #0f172a;
          border-color: #4b5563;
        }
        
        body.dark-mode .ref-section {
          background: #020617;
          border-color: #1f2937;
        }
        
        body.dark-mode .ref-header h2 {
          color: #f9fafb;
        }
        
        body.dark-mode .ref-subtitle {
          color: #9ca3af;
        }
        
        body.dark-mode .ref-card {
          border-top-color: #1f2937;
        }
        
        body.dark-mode .ref-card h3 {
          color: #e5e7eb;
        }
        
        body.dark-mode .ref-list {
          color: #e5e7eb;
        }
        
        





















        
        
        
        /* =========================================
           Fix dark-mode for header + ref sheet
           ========================================= */
        
/* Default (light mode) page header */
.page-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 0.75rem 1.25rem;
  background: #f9fafb;
  color: #111827;
  border-bottom: 1px solid #e5e7eb;
}

/* Dark mode override for the header bar */
body.dark-mode .page-header {
  background: #020617;          /* very dark navy/black */
  color: #f9fafb;               /* light text */
  border-bottom: 1px solid #1f2937;
}

/* Make sure inner text picks up the dark-mode color */
body.dark-mode .page-header h1,
body.dark-mode .page-header .legend-note {
  color: inherit;
}

/* Light mode (you can keep your existing styles if you like) */
#condensed-reference-wrapper {
  background: #ffffff;
  color: #111827;
}

#condensed-reference {
  background: #ffffff;
  border: 1px solid #e5e7eb;
}

/* Dark mode overrides */
body.dark-mode #condensed-reference-wrapper {
  background: #020617;
  color: #e5e7eb;
}

body.dark-mode #condensed-reference {
  background: #020617;
  border-color: #1f2937;
}

/* The toggle button bar at the top of the condensed sheet */
#condensed-ref-toggle {
  display: block;
  width: 100%;
  text-align: left;
  padding: 0.5rem 1rem;
  border: 0;
  background: #f3f4f6;
  color: #111827;
  cursor: pointer;
}

/* Dark-mode styling for the toggle button */
body.dark-mode #condensed-ref-toggle {
  background: #111827;
  color: #e5e7eb;
  border-bottom: 1px solid #1f2937;
}

/* Cards / sub-sections inside the condensed sheet */
.ref-card {
  background: #ffffff;
  border: 1px solid #e5e7eb;
  border-radius: 4px;
  padding: 1rem;
  margin-bottom: 1rem;
}

/* Dark-mode version */
body.dark-mode .ref-card {
  background: #020617;
  border-color: #1f2937;
}

/* Tables inside the condensed sheet */
.ref-table th,
.ref-table td {
  background: #ffffff;
  color: #111827;
  border-color: #e5e7eb;
}

/* Dark-mode tables */
body.dark-mode .ref-table th,
body.dark-mode .ref-table td {
  background: #020617;
  color: #e5e7eb;
  border-color: #1f2937;
}













.metrics-table-compact {
  font-size: 0.85rem;
}

.metrics-table-compact th,
.metrics-table-compact td {
  padding: 0.3rem 0.5rem;
  vertical-align: top;
}










/* Inline glossary hits in the text */
.glossary-inline-term {
  border-bottom: 1px dotted rgba(15, 118, 110, 0.7); /* teal-ish underline */
  cursor: help;
  padding-bottom: 0.05rem;
}

/* Dark mode tweak */
.dark-mode .glossary-inline-term {
  border-bottom-color: rgba(45, 212, 191, 0.9);
}














/* ===== G√∂del / Limits block ===== */
.limits-block {
  margin: 1.5rem 0;
  border-radius: 8px;
  border: 1px solid var(--border-soft, #e5e7eb);
  background: var(--card-bg, #ffffff);
  overflow: hidden;
}

.limits-block > summary {
  cursor: pointer;
  padding: 0.8rem 1rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  background: linear-gradient(
    to right,
    var(--accent-soft, #fff7ed),
    var(--card-bg, #ffffff)
  );
  border-bottom: 1px solid var(--border-soft, #e5e7eb);
  color: var(--accent-strong, #b45309); /* standout heading colour */
}

.limits-block > summary::marker {
  content: '';
}

.limits-block__chevron {
  font-size: 0.85rem;
  transition: transform 0.15s ease-out;
}

.limits-block[open] .limits-block__chevron {
  transform: rotate(90deg);
}

.limits-block__title {
  font-size: 0.98rem;
}

.limits-block__body {
  padding: 1rem 1.25rem 1.1rem;
  font-size: 0.9rem;
  line-height: 1.6;
  color: var(--text-muted, #4b5563);
}

.limits-block__body p {
  margin: 0 0 0.6rem;
}

.limits-block__body p:last-child {
  margin-bottom: 0;
}
























</style>

<script>
function toggleDetails(shouldOpen) {
  const detailsElements = document.querySelectorAll('details');
  detailsElements.forEach(detail => {
    detail.open = shouldOpen;
  });
}
</script>

<script>
function updateConfusionMetrics() {
  const tp = Number(document.getElementById('cm-tp')?.value) || 0;
  const fp = Number(document.getElementById('cm-fp')?.value) || 0;
  const fn = Number(document.getElementById('cm-fn')?.value) || 0;
  const tn = Number(document.getElementById('cm-tn')?.value) || 0;

  const total = tp + fp + fn + tn || 0;

  function safeDiv(num, den) {
    return den === 0 ? 0 : num / den;
  }

  const acc   = safeDiv(tp + tn, total);
  const prec  = safeDiv(tp, tp + fp);
  const rec   = safeDiv(tp, tp + fn);
  const spec  = safeDiv(tn, tn + fp);
  const f1    = safeDiv(2 * tp, 2 * tp + fp + fn);

  const fmt = v => (isFinite(v) ? v.toFixed(3) : 'NA');

  const map = {
    'cm-acc':  acc,
    'cm-prec': prec,
    'cm-rec':  rec,
    'cm-spec': spec,
    'cm-f1':   f1
  };

  Object.keys(map).forEach(id => {
    const el = document.getElementById(id);
    if (el) el.textContent = fmt(map[id]);
  });
}

function toggleDarkMode() {
  const body = document.body;
  body.classList.toggle('dark');
  const isDark = body.classList.contains('dark');
  try {
    localStorage.setItem('metricsDashboardDark', isDark ? '1' : '0');
  } catch (e) {}

  const btn = document.getElementById('darkToggle');
  if (btn) {
    btn.textContent = isDark ? '‚òÄÔ∏è Light mode' : 'üåô Dark mode';
  }
}

document.addEventListener('DOMContentLoaded', function () {
  const cmInputs = document.querySelectorAll('.cm-input');
  if (cmInputs.length) {
    cmInputs.forEach(function (input) {
      input.addEventListener('input', updateConfusionMetrics);
    });
    updateConfusionMetrics();
  }

  let wantsDark = false;
  try {
    wantsDark = localStorage.getItem('metricsDashboardDark') === '1';
  } catch (e) {}
  if (wantsDark) {
    document.body.classList.add('dark');
  }

  const btn = document.getElementById('darkToggle');
  if (btn) {
    const isDark = document.body.classList.contains('dark');
    btn.textContent = isDark ? '‚òÄÔ∏è Light mode' : 'üåô Dark mode';
  }
});
</script>
</head>

<body>
  
  
  
  
<div class="spotify-wrapper">
  <iframe
    src="https://open.spotify.com/embed/playlist/5qggnZgnAAPIa50DQSKe3a?utm_source=generator&theme=0&highlight=0Y0spT0fowl50Ob4Y4VOCW"
    frameborder="0"
    allowfullscreen
    allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"
    loading="lazy"
  ></iframe>
</div>

<style>
  .spotify-wrapper {
    width: 260px;
    height: 115px;
    overflow: hidden;
    margin: 0 auto;          /* center horizontally */
  }

  .spotify-wrapper iframe {
    width: 100%;
    height: 152px;           
    transform: scale(0.75);
    transform-origin: top center;  /* keep it centered while scaling */
    border-radius: 12px;
  }

  @media (min-width: 900px) {
    .spotify-wrapper {
      width: 320px;
      height: 114px; /* 152 * 0.75 */
    }
  }
</style>


  
  
  
  
  
  
  
  
<header class="page-header">
  <h1 class="page-title" style="font-size: 3em;">Comprehensive Model Evaluation Metrics Dashboard</h1>
    <p class="legend-note">
    <em>designed to nourish the human spirit and honor the truth. &#10084;</em>
  </p>

  <div class="controls">
    <button onclick="toggleDetails(true)">‚ñæ Expand all</button>
    <button onclick="toggleDetails(false)">‚ñ¥ Collapse all</button>
    <button id="darkToggle" onclick="toggleDarkMode()">üåô Dark mode</button>
  </div>
</header>









<main id="page-content">





<!-- ===================================== -->
<!-- 0. METRIC SELECTION GUIDE            -->
<!-- ===================================== -->
<details open>
  <summary>Metric Selection Guide (Flowchart‚Äëstyle)</summary>
  <p>High‚Äëlevel guide for picking suitable metrics:</p>
  <ol>
    <li><strong>Problem type?</strong>
      <ul>
        <li>Binary / multiclass classification ‚Üí use metrics in <em>Performance (Classification)</em>.</li>
        <li>Regression ‚Üí use metrics in <em>Regression &amp; Correlation</em>.</li>
        <li>Hypothesis / A/B test ‚Üí use <em>Inference &amp; Hypothesis Testing</em>.</li>
      </ul>
    </li>
    <li><strong>Is the dataset imbalanced?</strong>
      <ul>
        <li>Yes ‚Üí emphasise <strong>recall</strong>, <strong>precision</strong>, <strong>F1</strong>, <strong>PR AUC</strong>, <strong>MCC</strong>; avoid relying on plain accuracy.</li>
        <li>No ‚Üí accuracy + ROC AUC + F1 are typically fine.</li>
      </ul>
    </li>
    <li><strong>Threshold‚Äëfree ranking vs threshold‚Äëspecific performance?</strong>
      <ul>
        <li>Ranking quality ‚Üí ROC AUC / PR AUC.</li>
        <li>Specific operating point ‚Üí confusion matrix, precision/recall/F1 at that threshold.</li>
      </ul>
    </li>
    <li><strong>Regression goals?</strong>
      <ul>
        <li>Penalise big errors heavily ‚Üí RMSE / MSE.</li>
        <li>Interpret ‚Äúaverage absolute deviation‚Äù ‚Üí MAE.</li>
        <li>Targets span orders of magnitude ‚Üí RMSLE or log‚Äëtransforms.</li>
      </ul>
    </li>
    <li><strong>Need interpretability?</strong>
      <ul>
        <li>Global feature importance ‚Üí permutation importance, SHAP.</li>
        <li>Local explanations ‚Üí SHAP, LIME.</li>
      </ul>
    </li>
  </ol>
</details>










<!-- Practical focus / pitfalls notice -->
<div class="pitfall-intro">
  <strong>Go Beyond Definitions: Focus on Pitfalls &amp; Robustness</strong>

  <p>
    This resource prioritizes practical experience, highlighting the critical errors that lead to
    faulty models and bad decisions. Learn to avoid common pitfalls like:
  </p>

  <ul>
    <li><strong>The Accuracy Trap</strong> when working with imbalanced datasets.</li>
    <li>
      <strong>Misinterpreting the p-value</strong> ‚Äì
      it is <em>not</em> the probability that the null hypothesis is true.
    </li>
    <li>
      <strong>Data leakage</strong> from incorrectly applying cross-validation
      to time-series or temporally ordered data.
    </li>
  </ul>
</div>









<!-- ============================
     Condensed Reference Section (Collapsible)
     ============================ -->

<section id="condensed-reference-wrapper">
  <button id="condensed-ref-toggle" class="ref-toggle-btn" type="button">
    ‚ñ∏ Data Science Model Evaluation ‚Äì Condensed Reference Sheet
  </button>

  <!-- Collapsible content -->
  <div id="condensed-reference" class="ref-section" style="background-color: #f8f9fa">
    <div class="ref-header">
      <h2>Data Science Model Evaluation Metrics: Condensed Reference Sheet</h2>
      <p class="ref-subtitle">
        Quick overview of what to use, when to use it, and what to watch out for.
      </p>
    </div>

    <!-- 1. Metric Selection Guide -->
    <section class="ref-card">
      <h3>Metric Selection Guide</h3>
      <ol class="ref-list" >
        <li>
          <strong>Problem Type ‚Üí Metric Category</strong><br>
          Binary/Multiclass Classification ‚Üí <em>Performance (Classification)</em> metrics<br>
          Regression ‚Üí <em>Regression &amp; Correlation</em> metrics<br>
          Hypothesis/A/B Testing ‚Üí <em>Inference &amp; Hypothesis Testing</em>
        </li>
        <li>
          <strong>Dataset Imbalance?</strong><br>
          Yes ‚Üí Focus on <strong>Recall, Precision, F1, PR AUC, MCC</strong>; avoid plain Accuracy<br>
          No ‚Üí <strong>Accuracy, ROC AUC, F1</strong> are fine
        </li>
        <li>
          <strong>Ranking vs. Threshold Performance</strong><br>
          Ranking quality ‚Üí <strong>ROC AUC / PR AUC</strong><br>
          Specific operating point ‚Üí <strong>Confusion matrix, Precision/Recall/F1 at threshold</strong>
        </li>
        <li>
          <strong>Regression Goals</strong><br>
          Heavily penalize big errors ‚Üí <strong>RMSE / MSE</strong><br>
          Interpret ‚Äúaverage absolute deviation‚Äù ‚Üí <strong>MAE</strong><br>
          Targets span orders of magnitude ‚Üí <strong>RMSLE or log transforms</strong>
        </li>
        <li>
          <strong>Need Interpretability?</strong><br>
          Global feature importance ‚Üí <strong>Permutation importance, SHAP</strong><br>
          Local explanations ‚Üí <strong>SHAP, LIME</strong>
        </li>
      </ol>
    </section>

    <!-- 2. Classification Metrics Table -->
    <section class="ref-card">
      <h3>Classification Metrics (Quick Reference)</h3>

      <p><strong>Key formulas</strong></p>
      <pre class="ref-code">
Accuracy  = (TP + TN) / (TP + FP + TN + FN)
Precision = TP / (TP + FP)
Recall    = TP / (TP + FN)
F1        = 2 √ó (Precision √ó Recall) / (Precision + Recall)
MCC       = (TP√óTN ‚àí FP√óFN) / ‚àö[(TP+FP)(TP+FN)(TN+FP)(TN+FN)]      </pre>

      <div class="ref-table-wrapper">
        <table class="ref-table">
          <thead>
            <tr>
              <th>Metric</th>
              <th>Range</th>
              <th>Good</th>
              <th>Fair</th>
              <th>Poor</th>
              <th>When to Use</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Accuracy</td>
              <td>0 ‚Äì 1</td>
              <td>&gt; 0.8</td>
              <td>0.7 ‚Äì 0.8</td>
              <td>&lt; 0.5</td>
              <td>Balanced data only</td>
            </tr>
            <tr>
              <td>Precision</td>
              <td>0 ‚Äì 1</td>
              <td>&gt; 0.8</td>
              <td>0.7 ‚Äì 0.8</td>
              <td>&lt; 0.5</td>
              <td>False positives costly</td>
            </tr>
            <tr>
              <td>Recall</td>
              <td>0 ‚Äì 1</td>
              <td>&gt; 0.8</td>
              <td>0.7 ‚Äì 0.8</td>
              <td>&lt; 0.5</td>
              <td>False negatives costly</td>
            </tr>
            <tr>
              <td>F1 Score</td>
              <td>0 ‚Äì 1</td>
              <td>&gt; 0.8</td>
              <td>0.5 ‚Äì 0.8</td>
              <td>&lt; 0.5</td>
              <td>Imbalanced data</td>
            </tr>
            <tr>
              <td>ROC AUC</td>
              <td>0 ‚Äì 1</td>
              <td>&gt; 0.8</td>
              <td>0.7 ‚Äì 0.8</td>
              <td>&lt; 0.7</td>
              <td>Ranking problems</td>
            </tr>
            <tr>
              <td>PR AUC</td>
              <td>0 ‚Äì 1</td>
              <td>Much &gt; baseline</td>
              <td>Moderate &gt; baseline</td>
              <td>‚âà baseline</td>
              <td>Imbalanced binary</td>
            </tr>
            <tr>
              <td>MCC</td>
              <td>‚àí1 to 1</td>
              <td>&gt; 0.5</td>
              <td>0.3 ‚Äì 0.5</td>
              <td>&lt; 0</td>
              <td>Imbalanced binary</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- 3. Regression Metrics Table -->
    <section class="ref-card">
      <h3>Regression Metrics (Quick Reference)</h3>

      <p><strong>When to use:</strong></p>
      <ul class="ref-list">
        <li><strong>RMSE</strong> ‚Üí Large errors are critically bad</li>
        <li><strong>MAE</strong> ‚Üí Want ‚Äúaverage error‚Äù interpretation</li>
        <li><strong>RMSLE</strong> ‚Üí Targets span orders of magnitude</li>
      </ul>

      <div class="ref-table-wrapper">
        <table class="ref-table">
          <thead>
            <tr>
              <th>Metric</th>
              <th>Good</th>
              <th>Fair</th>
              <th>Poor</th>
              <th>Characteristics</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>R¬≤</td>
              <td>&gt; 0.7</td>
              <td>0.5 ‚Äì 0.7</td>
              <td>&lt; 0.3</td>
              <td>% variance explained</td>
            </tr>
            <tr>
              <td>Adjusted R¬≤</td>
              <td>Close to R¬≤</td>
              <td>Lower than R¬≤</td>
              <td>Much lower</td>
              <td>Accounts for predictors</td>
            </tr>
            <tr>
              <td>RMSE</td>
              <td>&lt;&lt; target SD</td>
              <td>‚âà target SD</td>
              <td>‚â• target SD</td>
              <td>Penalizes large errors</td>
            </tr>
            <tr>
              <td>MAE</td>
              <td>&lt;&lt; target mean</td>
              <td>‚âà 10‚Äì20% mean</td>
              <td>‚â• 30% mean</td>
              <td>Robust to outliers</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- 4. Statistical Testing Essentials -->
    <section class="ref-card">
      <h3>Statistical Testing Essentials</h3>

      <h4>Hypothesis Testing Flow</h4>
      <p>
        State H‚ÇÄ &amp; H‚ÇÅ ‚Üí Choose Œ± (0.05) ‚Üí Compute p-value ‚Üí Compare:
      </p>
      <ul class="ref-list">
        <li><strong>p &lt; Œ±</strong> ‚Üí Reject H‚ÇÄ (statistically significant)</li>
        <li><strong>p ‚â• Œ±</strong> ‚Üí Fail to reject H‚ÇÄ</li>
      </ul>

      <h4>Common Tests</h4>
      <div class="ref-table-wrapper">
        <table class="ref-table">
          <thead>
            <tr>
              <th>Test</th>
              <th>Use Case</th>
              <th>Key Assumptions</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>t-test</td>
              <td>Compare 2 group means</td>
              <td>Normality, equal variance (or use Welch‚Äôs)</td>
            </tr>
            <tr>
              <td>ANOVA</td>
              <td>Compare ‚â• 3 group means</td>
              <td>Normality, equal variance, independence</td>
            </tr>
            <tr>
              <td>Chi-square</td>
              <td>Test independence in categorical data</td>
              <td>Expected counts ‚â• 5</td>
            </tr>
            <tr>
              <td>Permutation Test</td>
              <td>Non-parametric alternative</td>
              <td>Exchangeability of labels</td>
            </tr>
            <tr>
              <td>Kolmogorov‚ÄìSmirnov (KS)</td>
              <td>
                Compare a sample to a reference distribution (1-sample)
                or compare two samples (2-sample)
              </td>
              <td>
                Continuous data, specifies distribution under H‚ÇÄ.
                Sensitive to differences in shape and location.
              </td>
            </tr>
            <tr>
              <td>Levene‚Äôs test</td>
              <td>
                Test equality of variances across groups
                (do we trust ‚Äúequal variance‚Äù for t-test / ANOVA?)
              </td>
              <td>
                Groups independent. Works reasonably well
                even when data are not normal.
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <h4>Multiple Testing Corrections</h4>
      <div class="ref-table-wrapper narrow">
        <table class="ref-table">
          <thead>
            <tr>
              <th>Method</th>
              <th>Controls</th>
              <th>When to Use</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Bonferroni</td>
              <td>FWER (strict)</td>
              <td>Few tests, false positives very costly</td>
            </tr>
            <tr>
              <td>Benjamini‚ÄìHochberg</td>
              <td>FDR (less strict)</td>
              <td>Many tests (genomics, feature screening)</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- 5. Model Selection Criteria -->
    <section class="ref-card">
      <h3>Model Selection Criteria</h3>
      <p>Rule of thumb: <strong>ŒîAIC/BIC &lt; 2</strong> ‚Üí models are essentially equivalent.</p>

      <div class="ref-table-wrapper narrow">
        <table class="ref-table">
          <thead>
            <tr>
              <th>Criterion</th>
              <th>Preference</th>
              <th>Best For</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>AIC</td>
              <td>Lower is better</td>
              <td>Prediction quality, larger models</td>
            </tr>
            <tr>
              <td>BIC</td>
              <td>Lower is better</td>
              <td>True model identification, smaller models</td>
            </tr>
            <tr>
              <td>Cross-validated Error</td>
              <td>Lower is better</td>
              <td>Direct out-of-sample performance</td>
            </tr>
            <tr>
              <td>Adjusted R¬≤</td>
              <td>Higher is better</td>
              <td>Regression with multiple predictors</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- 6. Interpretability Methods -->
    <section class="ref-card">
      <h3>Interpretability Methods</h3>
      <p class="ref-warning">
        These show <strong>association</strong>, not <strong>causation</strong>!
      </p>

      <div class="ref-table-wrapper narrow">
        <table class="ref-table">
          <thead>
            <tr>
              <th>Method</th>
              <th>Scope</th>
              <th>Key Insight</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Permutation Importance</td>
              <td>Global</td>
              <td>Feature importance by performance drop</td>
            </tr>
            <tr>
              <td>SHAP Values</td>
              <td>Global + Local</td>
              <td>Additive feature contributions</td>
            </tr>
            <tr>
              <td>LIME</td>
              <td>Local</td>
              <td>Local surrogate model explanations</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- 7. Common Pitfalls & Checklist -->
    <section class="ref-card ref-grid">
      <div>
        <h3>Top 10 Common Pitfalls</h3>
        <ol class="ref-list">
          <li>Using Accuracy on imbalanced data</li>
          <li>Ignoring false negatives in medical/safety applications</li>
          <li>Optimizing only Precision or Recall (neglecting the other)</li>
          <li>Treating p-value as probability H‚ÇÄ is true</li>
          <li>Not correcting for multiple testing</li>
          <li>Comparing R¬≤ across different datasets</li>
          <li>Using RMSE when outliers are unimportant</li>
          <li>Interpreting SHAP/LIME as causal effects</li>
          <li>Selecting models based on tiny metric differences</li>
          <li>Ignoring confidence intervals for metrics</li>
        </ol>
      </div>

      <div>
        <h3>Quick Decision Checklist</h3>
        <h4>Before choosing metrics</h4>
        <ul class="ref-list">
          <li>What‚Äôs the business objective?</li>
          <li>Balanced or imbalanced data?</li>
          <li>Cost of false positives vs false negatives?</li>
          <li>Need probability rankings or binary decisions?</li>
          <li>Require interpretability?</li>
        </ul>

        <h4>After model evaluation</h4>
        <ul class="ref-list">
          <li>Check multiple metrics (not just one)</li>
          <li>Examine confusion matrix / error patterns</li>
          <li>Validate on holdout / test set</li>
          <li>Consider confidence intervals</li>
          <li>Compare to reasonable baselines</li>
        </ul>
      </div>
    </section>

    <!-- 8. Essential Python Snippets -->
    <section class="ref-card">
      <h3>Essential Python Snippets</h3>

      <h4>Classification Metrics</h4>
      <pre class="ref-code">
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

acc     = accuracy_score(y_true, y_pred)
prec    = precision_score(y_true, y_pred)
rec     = recall_score(y_true, y_pred)
f1      = f1_score(y_true, y_pred)
roc_auc = roc_auc_score(y_true, y_proba)      </pre>

      <h4>Regression Metrics</h4>
      <pre class="ref-code">
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae  = mean_absolute_error(y_true, y_pred)
rmse = mean_squared_error(y_true, y_pred, squared=False)
r2   = r2_score(y_true, y_pred)      </pre>

      <h4>Statistical Tests</h4>
      <pre class="ref-code">
from scipy import stats

# t-test (two independent groups, equal variance assumed)
t_stat, p_value = stats.ttest_ind(group1, group2)

# Chi-square test of independence for a contingency table
chi2, p_chi, dof, expected = stats.chi2_contingency(contingency_table)

# Kolmogorov‚ÄìSmirnov:
# 1-sample KS: does sample follow a normal distribution?
ks_stat, p_ks = stats.kstest(sample, 'norm')

# 2-sample KS: are two samples from the same distribution?
ks2_stat, p_ks2 = stats.ks_2samp(sample1, sample2)

# Levene‚Äôs test for equality of variances
lev_stat, p_lev = stats.levene(group1, group2, group3)      </pre>

    </section>

    <!-- 9. Key Takeaways -->
    <section class="ref-card">
      <h3>Key Takeaways</h3>
      <ol class="ref-list">
        <li>No single metric tells the whole story ‚Üí always use multiple.</li>
        <li>Context is everything ‚Üí choose metrics aligned with business goals.</li>
        <li>Visualize ‚Üí confusion matrices, ROC/PR curves, residual plots.</li>
        <li>Uncertainty matters ‚Üí report confidence intervals, not just point estimates.</li>
        <li>Baseline comparison ‚Üí always compare to simple benchmarks.</li>
      </ol>
      <p class="ref-footnote">
        Based on the ‚ÄúData Science Model Evaluation Metrics Dashboard‚Äù condensed sheet.
      </p>
    </section>
  </div>
</section>

<!-- ===== Styling for reference section + toggle ===== -->
<style>
  .ref-toggle-btn {
    width: 100%;
    text-align: left;
    border: 1px solid #e5e7eb;
    border-radius: 10px;
    background: #f3f4f6;
    padding: 0.55rem 0.8rem;
    font-size: 0.95rem;
    font-weight: 600;
    cursor: pointer;
    margin: 1.5rem 0 0.25rem;
  }
  .ref-toggle-btn:hover {
    background: #e5e7eb;
  }
  .dark-mode .ref-toggle-btn {
    background: #111827;
    border-color: #1f2937;
    color: #e5e7eb;
  }
  .dark-mode .ref-toggle-btn:hover {
    background: #1f2937;
  }

  .ref-section {
    margin: 0 0 2rem;
    padding: 1rem 1.5rem 2rem;
    border-radius: 12px;
    background: #f9fafb;
    border: 1px solid #e5e7eb;
  }
  .dark-mode .ref-section {
    background: #050816;
    border-color: #1f2937;
  }
  .ref-header h2 {
    margin: 0 0 .5rem;
    font-size: 1.4rem;
  }
  .ref-subtitle {
    margin: 0;
    font-size: 0.9rem;
    color: #4b5563;
  }
  .dark-mode .ref-subtitle {
    color: #9ca3af;
  }
  .ref-card {
    margin-top: 1.5rem;
    padding: 1rem 1.2rem;
    border-radius: 10px;
    background: #ffffff;
    border: 1px solid #e5e7eb;
  }
  .dark-mode .ref-card {
    background: #0b1120;
    border-color: #111827;
  }
  .ref-card h3 {
    margin-top: 0;
    margin-bottom: .5rem;
    font-size: 1.1rem;
  }
  .ref-card h4 {
    margin-top: .8rem;
    margin-bottom: .3rem;
    font-size: 0.95rem;
  }
  .ref-list {
    padding-left: 1.1rem;
    margin: 0.3rem 0 0;
    font-size: 0.9rem;
  }
  .ref-list li {
    margin-bottom: 0.3rem;
  }
  .ref-table-wrapper {
    overflow-x: auto;
    margin-top: .6rem;
  }
  .ref-table-wrapper.narrow {
    max-width: 600px;
  }
  .ref-table {
    width: 100%;
    border-collapse: collapse;
    font-size: 0.85rem;
  }
  .ref-table th,
  .ref-table td {
    border: 1px solid #e5e7eb;
    padding: 0.4rem 0.5rem;
    text-align: left;
    white-space: nowrap;
  }
  .dark-mode .ref-table th,
  .dark-mode .ref-table td {
    border-color: #1f2937;
  }
  .ref-table th {
    background: #f3f4f6;
    font-weight: 600;
  }
  .dark-mode .ref-table th {
    background: #111827;
  }
  .ref-code {
    background: #111827;
    color: #e5e7eb;
    padding: 0.6rem 0.7rem;
    border-radius: 8px;
    font-size: 0.8rem;
    overflow-x: auto;
  }
  .dark-mode .ref-code {
    background: #020617;
  }
  .ref-warning {
    font-size: 0.9rem;
    margin-top: 0.4rem;
  }
  .ref-grid {
    display: grid;
    gap: 1rem;
  }
  @media (min-width: 900px) {
    .ref-grid {
      grid-template-columns: 1.1fr 1fr;
    }
  }
  .ref-footnote {
    margin-top: .6rem;
    font-size: 0.8rem;
    color: #6b7280;
  }
  .dark-mode .ref-footnote {
    color: #9ca3af;
  }
</style>

<!-- ===== Simple JS to toggle collapsible sections ===== -->
<script>
  document.addEventListener('DOMContentLoaded', function () {

    function setupCollapsible(btnId, contentId, labelText) {
      const btn = document.getElementById(btnId);
      const content = document.getElementById(contentId);
      if (!btn || !content) return;

      // start collapsed
      content.style.display = 'none';
      btn.textContent = '‚ñ∏ ' + labelText;

      btn.addEventListener('click', () => {
        const isHidden = content.style.display === 'none';
        content.style.display = isHidden ? 'block' : 'none';
        btn.textContent = (isHidden ? '‚ñº ' : '‚ñ∏ ') + labelText;
      });
    }

    // 1) existing condensed reference sheet
    setupCollapsible(
      'condensed-ref-toggle',
      'condensed-reference',
      'Data Science Model Evaluation ‚Äì Condensed Reference Sheet'
    );

    // 2) new Statistical Tests & Assumptions section
    setupCollapsible(
      'stat-tests-toggle',
      'stat-tests-section',
      'Statistical Tests & Assumptions ‚Äì Quick Reference'
    );
  });
</script>











<!-- ====== ADDED: COLOR LEGEND ====== -->
<section class="color-legend">
  <h3 class="legend-title">How to Read the Color Bars</h3>
  <p>The colors indicate the qualitative interpretation of a metric's value, following a consistent rule:</p>
  <div class="legend-items">
    <div class="legend-item">
      <div class="legend-color-box" style="background-color: #c8e6c9;"></div>
      <span><strong>Green</strong>: A <strong>desirable</strong> or <strong>good</strong> result for the metric's purpose.</span>
    </div>
    <div class="legend-item">
      <div class="legend-color-box" style="background-color: #fff9c4;"></div>
      <span><strong>Yellow</strong>: A <strong>moderate</strong>, acceptable, or cautionary result.</span>
    </div>
    <div class="legend-item">
      <div class="legend-color-box" style="background-color: #ffccbc;"></div>
      <span><strong>Red</strong>: An <strong>undesirable</strong>, poor, or problematic result.</span>
    </div>
  </div>
  <p class="legend-note">
    <em>Key Context: "Desirable" depends on context. For a p-value testing an effect (e.g., "is there a difference?"), a <strong>low value</strong> (significant) is good (green). For a p-value checking an assumption (e.g., "are residuals normal?"), a <strong>high value</strong> (no violation) is good (green). Correlation bars now show strength (absolute value) from weak to strong.
    </em>
  </p>
</section>






<!-- ===================================== -->
<!-- 1. PERFORMANCE (CLASSIFICATION)      -->
<!-- ===================================== -->
<details open>
<summary>Performance (Classification)</summary>
<div class="table-responsive">
<table>
<thead>
<tr>
  <th>Metric</th>
  <th>Decision Criterion (Value Range)</th>
  <th>Purpose</th>
  <th>Description</th>
  <th>Working Mechanism</th>
  <th>Example</th>
  <th>Limitations</th>
</tr>
</thead>
<tbody>

<tr>
<td><strong>Accuracy</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-07">0.7</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  Aim as high as possible.<br><br>
  <span class="poor">&lt; 0.5</span> = poor overall performance.<br><br>
  <span class="moderate">0.5‚Äì0.7</span> = fair (better than chance but weak).<br><br>
  <span class="good">0.7‚Äì0.8</span> = good.<br><br>
  <span class="good">&gt; 0.8</span> = very good (relative to chance level).
</td>
<td>Gauge overall classification success rate. Useful for quick assessment on
balanced data, but unreliable alone on strongly imbalanced data.</td>
<td>Proportion of all predictions that are correct:
\[
\text{Accuracy} = \frac{TP + TN}{TP+FP+TN+FN}.
\]</td>
<td>Counts correct vs total predictions with all errors weighted equally.</td>
<td>90 correct predictions out of 100 ‚Üí accuracy = 0.90.</td>
<td>Can be very misleading for imbalanced data ‚Äì a majority‚Äëclass predictor can
have high accuracy but be useless.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Comparing models by accuracy without checking class balance or baseline (e.g. majority‚Äëclass accuracy).</li>
  <li>Interpreting small accuracy differences as meaningful without considering confidence intervals or variability.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Precision</strong> (Positive Predictive Value, PPV)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-07">0.7</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  Higher is better.<br><br>
  <span class="poor">&lt; 0.5</span> = more than half of predicted positives are wrong.<br><br>
  <span class="moderate">0.5‚Äì0.7</span> = fair.<br><br>
  <span class="good">0.7‚Äì0.8</span> = good.<br><br>
  <span class="good">&gt; 0.8</span> (especially &gt; 0.9) = very few false positives.
</td>
<td>Measures how reliable positive predictions are, crucial when false
positives are costly.</td>
<td>
\[
\text{Precision} = \frac{TP}{TP+FP}.
\]</td>
<td>Improves as false positives decrease, even if recall suffers.</td>
<td>If 100 flagged spam emails contain 90 real spam, precision = 0.90.</td>
<td>Can be gamed by predicting very few positives; must be balanced with recall.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Optimising precision alone and ending up with a model that rarely predicts positives (very low recall).</li>
  <li>Comparing precision across datasets with very different prevalence without context.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Recall</strong> (Sensitivity / True Positive Rate)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-07">0.7</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  Higher is better.<br><br>
  <span class="poor">&lt; 0.5</span> = more than half of positives are missed.<br><br>
  <span class="moderate">0.5‚Äì0.7</span> = fair coverage.<br><br>
  <span class="good">0.7‚Äì0.8</span> = good.<br><br>
  <span class="good">&gt; 0.8</span> (especially &gt; 0.9) = very high detection.
</td>
<td>Measures completeness of positive detection; key when missing positives is
costly.</td>
<td>
\[
\text{Recall} = \frac{TP}{TP+FN}.
\]</td>
<td>Improves when false negatives decrease.</td>
<td>Recall = 0.95 means 95% of true positives are found.</td>
<td>Ignores false positives; predicting everything positive yields recall 1.0.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Maximising recall at the expense of an unacceptably high false‚Äëpositive rate.</li>
  <li>Interpreting high recall as ‚Äúgood model‚Äù without looking at precision or class prevalence.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Specificity</strong> (True Negative Rate)</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-07">0.7</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  Higher is better.<br><br>
  <span class="poor">&lt; 0.5</span> = majority of negatives misclassified.<br><br>
  <span class="moderate">0.5‚Äì0.7</span> = fair.<br><br>
  <span class="good">0.7‚Äì0.8</span> = good.<br><br>
  <span class="good">&gt; 0.8</span> = very low false‚Äëalarm rate.
</td>
<td>Measures ability to correctly identify negatives; important when false
positives are costly.</td>
<td>
\[
\text{Specificity} = \frac{TN}{TN+FP}.
\]</td>
<td>Complement of false positive rate: FPR = 1 ‚àí specificity.</td>
<td>Specificity 0.98 means 98% of real negatives are correctly left unflagged.</td>
<td>Trivially high if model predicts almost everything negative.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Quoting high specificity while recall on the positive class is extremely low.</li>
  <li>Confusing specificity with NPV or accuracy when explaining results to stakeholders.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>False Positive Rate / False Negative Rate</strong><br>(FPR / FNR)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-error"></div>
    <div class="colorbar-scale">
      <span class="tick-err0">0</span>
      <span class="tick-errmid">moderate</span>
      <span class="tick-errhi">high</span>
    </div>
  </div>

  Lower is better for both.<br><br>
  <span class="good">&lt; 0.05</span> on either side = very low error rate there.<br><br>
  <span class="moderate">‚âà 0.10‚Äì0.20</span> = modest error rate.<br><br>
  <span class="poor">&gt; 0.30</span> = high error rate; typically unacceptable.
</td>
<td>Quantify false alarms (FPR) and missed positives (FNR).</td>
<td>
\[
\text{FPR} = \frac{FP}{FP+TN}, \quad
\text{FNR} = \frac{FN}{FN+TP}.
\]</td>
<td>Threshold choice trades FPR vs FNR; ROC and PR curves illustrate this trade‚Äëoff.</td>
<td>A cancer test may tolerate FPR 0.15 for FNR 0.01 (very few missed cases).</td>
<td>Need domain‚Äëspecific cost trade‚Äëoffs; no single ‚Äúcorrect‚Äù target.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Optimising only one of FPR or FNR without considering the business/clinical cost of the other side.</li>
  <li>Reporting FPR but not stating what threshold or prevalence it corresponds to.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>F<sub>1</sub> Score</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-f1"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  Higher is better.<br><br>
  <span class="poor">&lt; 0.5</span> = poor precision‚Äìrecall balance.<br><br>
  <span class="good">0.5‚Äì0.8</span> = acceptable to good.<br><br>
  <span class="good">&gt; 0.8</span> = strong overall performance.
</td>
<td>Single number that balances precision and recall, especially on imbalanced
datasets.</td>
<td>
\[
F_1 = 2 \frac{\text{Precision} \times \text{Recall}}
{\text{Precision} + \text{Recall}}.
\]</td>
<td>Harmonic mean, so dominated by the smaller of precision and recall.</td>
<td>Precision 0.9, recall 0.9 ‚Üí F1 0.9; precision 0.9, recall 0.1 ‚Üí F1 ‚âà 0.18.</td>
<td>Weights precision and recall equally; may not match domain priorities.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using F1 when precision and recall have very different real‚Äëworld costs (e.g. medical diagnosis).</li>
  <li>Comparing F1 scores across datasets with very different class imbalance.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>ROC AUC</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-auc"></div>
    <div class="colorbar-scale">
      <span class="tick-05">0.5</span>
      <span class="tick-07">0.7</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  <span class="poor">&lt; 0.7</span> = weak discrimination.<br><br>
  <span class="moderate">0.7‚Äì0.8</span> = moderate.<br><br>
  <span class="good">&gt; 0.8</span> = good; 0.9+ often excellent.
</td>
<td>Measures how well model ranks positives above negatives across thresholds.</td>
<td>Equivalent to probability a random positive has higher score than a random
negative.</td>
<td>Integrates TPR vs FPR curve over all thresholds.</td>
<td>AUC 0.85 ‚Üí model ranks positives higher 85% of the time.</td>
<td>Doesn‚Äôt reflect calibration; can look good on heavily imbalanced data even
if practical performance is poor.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using ROC AUC on extreme class imbalance where PR AUC is more informative.</li>
  <li>Assuming high AUC always implies good performance at the specific threshold used in production.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Negative Predictive Value</strong> (NPV)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-07">0.7</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  Higher is better.<br><br>
  <span class="poor">&lt; 0.5</span> = ‚Äúnegative‚Äù predictions often wrong.<br><br>
  <span class="moderate">0.5‚Äì0.8</span> = moderate trust.<br><br>
  <span class="good">&gt; 0.8</span> = ‚Äúall clear‚Äù predictions usually correct.
</td>
<td>Probability that a predicted negative is truly negative; key when false
negatives are costly.</td>
<td>
\[
\text{NPV} = \frac{TN}{TN+FN}.
\]</td>
<td>Complements precision (PPV).</td>
<td>In disease screening with low prevalence, NPV often very high even for
moderate models.</td>
<td>Strongly dependent on prevalence; high NPV doesn‚Äôt automatically imply a
good model.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Interpreting high NPV as strong evidence of good model quality in very low‚Äëprevalence settings where almost everyone is negative.</li>
  <li>Confusing NPV with specificity when explaining metrics.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Balanced Accuracy</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-07">0.7</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  <span class="poor">0.5</span> = chance‚Äëlevel performance (binary).<br><br>
  <span class="moderate">0.6‚Äì0.7</span> = modest improvement over chance.<br><br>
  <span class="good">&gt; 0.7</span> = good performance on both classes.
</td>
<td>Accounts for class imbalance by averaging recall over classes.</td>
<td>
\[
\text{Balanced Acc} = \frac{\text{TPR} + \text{TNR}}{2}.
\]</td>
<td>Penalises models that perform well on majority but poorly on minority
class.</td>
<td>TPR 0.9, TNR 0.5 ‚Üí balanced accuracy 0.7.</td>
<td>Still hides which side is weak; always inspect TPR and TNR separately.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Reporting balanced accuracy without showing class‚Äëspecific recalls, hiding which class is performing poorly.</li>
  <li>Comparing balanced accuracy across tasks with very different numbers of classes.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Brier Score</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-error"></div>
    <div class="colorbar-scale">
      <span class="tick-err0">0</span>
      <span class="tick-errmid">0.25</span>
      <span class="tick-errhi">1.0</span>
    </div>
  </div>

  Lower is better.<br><br>
  <span class="good">0‚Äì0.05</span> = excellent probabilistic predictions.<br><br>
  <span class="moderate">0.05‚Äì0.25</span> = reasonable calibration / accuracy.<br><br>
  <span class="poor">&gt; 0.25</span> = poor (similar to random or worse).
</td>
<td>Measures quality of probabilistic predictions for binary outcomes.</td>
<td>
\[
\text{Brier} = \frac{1}{N} \sum (\hat{p}_i - y_i)^2.
\]</td>
<td>Combines calibration and sharpness; penalises confident wrong predictions.</td>
<td>A model always predicting p=0.5 on a balanced dataset has Brier 0.25.</td>
<td>Absolute values need baseline for interpretation; doesn‚Äôt distinguish
where (in p‚Äëspace) miscalibration occurs.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Comparing Brier scores across tasks with very different prevalence or outcome scales without normalising.</li>
  <li>Assuming a low Brier score guarantees good ranking performance (it does not replace ROC/PR metrics).</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Calibration Error</strong> (ECE)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-error"></div>
    <div class="colorbar-scale">
      <span class="tick-err0">0</span>
      <span class="tick-errmid">0.1</span>
      <span class="tick-errhi">0.3+</span>
    </div>
  </div>

  Lower is better.<br><br>
  <span class="good">0‚Äì0.02</span> = very well calibrated.<br><br>
  <span class="moderate">0.02‚Äì0.10</span> = minor issues.<br><br>
  <span class="poor">&gt; 0.10</span> = noticeable miscalibration.
</td>
<td>Measures mismatch between predicted probabilities and observed
frequencies.</td>
<td>Bins predictions by confidence; compares average predicted probability to
empirical frequency in each bin, then averages absolute differences.</td>
<td>Low ECE means that ‚Äú70% probability‚Äù events do happen around 70% of the
time.</td>
<td>Used for risk models in credit, medicine, etc. as a complement to AUC.</td>
<td>Depends on binning; can be unstable with small sample sizes.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using a single ECE value without inspecting calibration plots per probability region.</li>
  <li>Ignoring that ECE can be low even when model ranking (AUC/PR AUC) is poor.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Confusion Matrix</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  Interpreted via the share of counts on the diagonal (correct) vs off‚Äëdiagonal (errors).<br><br>
  <span class="poor">Diagonal similar to off‚Äëdiagonal</span> = many misclassifications.<br><br>
  <span class="moderate">Diagonal somewhat dominant</span> = moderate performance.<br><br>
  <span class="good">Diagonal strongly dominant, off‚Äëdiagonals small</span> = strong performance across classes.
</td>
<td>Summarise classification results in terms of true positives, false positives,
false negatives, and true negatives for each class.</td>
<td>Matrix whose rows are actual classes and columns are predicted classes (or vice versa); each cell counts how often that combination occurs.</td>
<td>From predictions and labels, fill contingency table; derived metrics like precision, recall, F1, MCC are computed from its cells.</td>
<td>A binary confusion matrix with large TP and TN and very small FP/FN indicates a strong classifier; see interactive explorer below.</td>
<td>Not a single scalar; becomes large for many classes and may be hard to read without normalisation.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Looking only at totals without normalising per row/column, which hides minority‚Äëclass errors.</li>
  <li>Comparing confusion matrices across datasets with different sizes without converting to rates.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Precision‚ÄìRecall Curve</strong> (PR curve / PR AUC)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-auc"></div>
    <div class="colorbar-scale">
      <span class="tick-05">baseline</span>
      <span class="tick-07">0.7</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  Interpret relative to baseline positive rate œÄ.<br><br>
  <span class="poor">PR AUC ‚âà œÄ</span> = little better than random ranking.<br><br>
  <span class="moderate">Clearly above œÄ but &lt; 0.7</span> = modest improvement.<br><br>
  <span class="good">Substantially above œÄ (e.g. &gt; 0.7)</span> = useful on rare‚Äëpositive problems.
</td>
<td>Assess trade‚Äëoff between precision and recall across thresholds, especially for highly imbalanced binary classification.</td>
<td>Curve of precision vs recall as the decision threshold moves; area under it (PR AUC) summarises overall performance on positives.</td>
<td>Emphasises performance on the positive class; random classifier‚Äôs baseline is the prevalence œÄ rather than 0.5.</td>
<td>In fraud detection with 0.5% positives, PR AUC 0.65 is a huge improvement over baseline 0.005, even if ROC AUC is ‚Äúonly‚Äù moderate.</td>
<td>Baseline depends on prevalence; PR AUC numbers are not directly comparable across datasets with different class balance.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Comparing PR AUC values across datasets without accounting for different positive rates.</li>
  <li>Only inspecting a single operating point on PR curve instead of the region relevant to business constraints.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Cohen‚Äôs Kappa</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-kappa"></div>
    <div class="colorbar-scale">
      <span class="tick-m1">-1</span>
      <span class="tick-0c">0</span>
      <span class="tick-05c">0.6</span>
      <span class="tick-1c">1</span>
    </div>
  </div>

  Agreement beyond chance (‚àí1 to 1).<br><br>
  <span class="poor">‚â§ 0</span> = no better (or worse) than random.<br><br>
  <span class="moderate">0.01‚Äì0.40</span> = slight‚Äìfair agreement.<br><br>
  <span class="moderate">0.41‚Äì0.60</span> = moderate.<br><br>
  <span class="good">&gt; 0.60</span> = substantial to almost perfect agreement.
</td>
<td>Measure inter‚Äërater reliability or agreement between model predictions and labels while adjusting for agreement expected by chance.</td>
<td>Compares observed accuracy \(p_o\) to expected accuracy \(p_e\) under random agreement given class marginals.</td>
<td>
\[
\kappa = \frac{p_o - p_e}{1 - p_e}.
\]
Large Œ∫ implies agreement much higher than chance, small Œ∫ close to or below 0 implies near‚Äërandom agreement.</td>
<td>Often used to compare two human labelers, or model vs clinician, in medical imaging or annotation tasks.</td>
<td>Sensitive to prevalence and marginal distributions; Œ∫ can be low even with high observed accuracy in imbalanced datasets.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Interpreting low Œ∫ as ‚Äúbad model‚Äù without considering skewed class frequencies or label noise.</li>
  <li>Applying generic qualitative cutoffs (e.g. ‚Äúgood‚Äù at 0.6) without domain‚Äëspecific context.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Matthews Correlation Coefficient</strong> (MCC)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-mcc"></div>
    <div class="colorbar-scale">
      <span class="tick-m1">-1</span>
      <span class="tick-0c">0</span>
      <span class="tick-05c">0.5</span>
      <span class="tick-1c">1</span>
    </div>
  </div>

  ‚àí1 ‚â§ MCC ‚â§ 1.<br><br>
  <span class="poor">‚â§ 0</span> = random or worse than random.<br><br>
  <span class="moderate">0.1‚Äì0.3</span> = weak signal.<br><br>
  <span class="moderate">0.3‚Äì0.5</span> = moderate.<br><br>
  <span class="good">&gt; 0.5</span> = strong classifier, particularly on imbalanced data.
</td>
<td>Single‚Äënumber summary of binary classifier quality that is symmetric in classes and robust to class imbalance.</td>
<td>Correlation between predicted and true labels using all four entries of the confusion matrix.</td>
<td>
\[
\text{MCC} =
\frac{TP \cdot TN - FP \cdot FN}
{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}.
\]
</td>
<td>In a fraud‚Äëdetection setting with extreme imbalance, MCC distinguishes a useful classifier (e.g. 0.6) from one that just predicts the majority class (MCC ‚âà 0).</td>
<td>Less intuitive than accuracy or F1; defined for binary classification and needs generalisation for multiclass problems.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Ignoring the sign: a negative MCC means systematic misclassification, not just ‚Äúslightly bad‚Äù.</li>
  <li>Comparing MCC values from confusion matrices built at very different thresholds without explanation.</li>
</ul>
</td>
</tr>

</tbody>
</table>
</div>

<h2>Interactive Confusion Matrix Explorer</h2>
<p>Adjust TP / FP / FN / TN to see how the main metrics change (toy calculator only).</p>

<div class="table-responsive">
  <table>
    <thead>
      <tr>
        <th></th>
        <th>Predicted Positive</th>
        <th>Predicted Negative</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>Actual Positive</th>
        <td>
          TP:
          <input id="cm-tp" class="cm-input" type="number" min="0" value="50" style="width: 80px;">
        </td>
        <td>
          FN:
          <input id="cm-fn" class="cm-input" type="number" min="0" value="10" style="width: 80px;">
        </td>
      </tr>
      <tr>
        <th>Actual Negative</th>
        <td>
          FP:
          <input id="cm-fp" class="cm-input" type="number" min="0" value="5" style="width: 80px;">
        </td>
        <td>
          TN:
          <input id="cm-tn" class="cm-input" type="number" min="0" value="100" style="width: 80px;">
        </td>
      </tr>
    </tbody>
  </table>
</div>

<p><strong>Derived metrics</strong> from the current confusion matrix:</p>
<ul>
  <li>Accuracy: <code id="cm-acc">‚Äì</code></li>
  <li>Precision (PPV): <code id="cm-prec">‚Äì</code></li>
  <li>Recall (TPR): <code id="cm-rec">‚Äì</code></li>
  <li>Specificity (TNR): <code id="cm-spec">‚Äì</code></li>
  <li>F<sub>1</sub> score: <code id="cm-f1">‚Äì</code></li>
</ul>

</details>












<!-- Survival / Time-to-Event Performance -->
<details class="metric-section" >
  <summary>Survival / Time-to-Event Performance</summary>
  <div class="metric-section-body" style="background-color: #f8f9fa">
    
    
       
          <div class="cutting-header">
          <strong> </strong>
          <span  > </span>
        </div>
   
  
    
    
    <p>
      Use these metrics when the outcome is a <strong>time until an event</strong>
      (death, relapse, failure, churn) and some observations are
      <strong>censored</strong> (we only know the event has not happened yet by
      the end of follow-up).
    </p>

    <h4>When to use</h4>
    <ul>
      <li>Outcomes like ‚Äútime from diagnosis to death‚Äù, ‚Äútime to device failure‚Äù, ‚Äútime until customer churn‚Äù.</li>
      <li>Many people are still event-free at the end of the study (right-censoring).</li>
      <li>We care about <em>risk over time</em>, not just a yes/no label at a fixed date.</li>
    </ul>

    <h4>Key tools &amp; metrics</h4>
    <ul>
      <li>
        <strong>Kaplan‚ÄìMeier curve</strong> ‚Äì
        step-shaped curve showing the fraction still event-free over time. Great
        for visualising survival patterns and comparing groups.
      </li>
      <li>
        <strong>Log-rank test</strong> ‚Äì
        tests whether two or more Kaplan‚ÄìMeier curves are systematically different
        over time.
      </li>
      <li>
        <strong>Cox proportional hazards model</strong> ‚Äì
        regression model for time-to-event data that estimates
        <strong>hazard ratios</strong> for predictors.
      </li>
      <li>
        <strong>C-index (concordance index)</strong> ‚Äì
        rank-based performance measure: probability that a person who experiences
        the event earlier gets a higher predicted risk. (1 = perfect, 0.5 = random.)
      </li>
      <li>
        <strong>Time-dependent ROC / AUC(t)</strong> ‚Äì
        ROC-style discrimination at specific time points (e.g. 1-year, 5-year AUC).
      </li>
      <li>
        <strong>Brier score over time</strong> ‚Äì
        mean squared error of predicted event probabilities at a given time
        horizon, adjusted for censoring. Lower is better.
      </li>
    </ul>

    <h4>Common pitfalls</h4>
    <ul>
      <li>
        <strong>Ignoring censoring</strong> ‚Äì
        treating censored cases as if the event never happened biases estimates.
        Always use survival-aware methods (Kaplan‚ÄìMeier, Cox, etc.).
      </li>
      <li>
        <strong>Immortal-time bias</strong> ‚Äì
        giving people ‚Äúrisk-free‚Äù time because they must survive long enough
        to receive a treatment or enter a group.
      </li>
      <li>
        <strong>Competing risks</strong> ‚Äì
        if other events (e.g. death from another cause) prevent the event of
        interest, standard survival curves can overstate risk unless competing-risk
        methods are used.
      </li>
      <li>
        <strong>Too short follow-up</strong> ‚Äì
        if few events occur, any metric (C-index, Brier, log-rank) will be noisy
        and underpowered.
      </li>
    </ul>
    
    
    
       
          <div class="cutting-header">
          <strong> </strong>
          <span  > </span>
        </div>
   
  
    
    
    
    
    
  </div>
</details>















<!-- ===================================== -->
<!-- 2. INFERENCE & HYPOTHESIS TESTING     -->
<!-- ===================================== -->
<details open>
<summary>Inference &amp; Hypothesis Testing</summary>
<div class="table-responsive">
<table>
<thead>
<tr>
  <th>Metric / Test</th>
  <th>Decision Criterion (Test Ranges)</th>
  <th>Purpose</th>
  <th>Description</th>
  <th>Working Mechanism</th>
  <th>Example</th>
  <th>Limitations</th>
</tr>
</thead>
<tbody>

<tr>
<td><strong>p-value</strong> (Significance Level)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-pvalue-effect"></div>
    <div class="colorbar-scale">
      <span class="tick-p0">0</span>
      <span class="tick-p005">0.05</span>
      <span class="tick-p1">1.0</span>
    </div>
  </div>

  Typically compared to Œ± = 0.05.<br><br>
  <span class="good">p &lt; 0.05</span> = statistically significant evidence against H<sub>0</sub> (if an effect is of interest).<br><br>
  <span class="poor">p ‚â• 0.05</span> = no statistically significant evidence; cannot rule out H<sub>0</sub>.
</td>
<td>Quantifies evidence against the null hypothesis and provides a common
decision rule across many tests.</td>
<td>Probability, assuming H<sub>0</sub> is true, of obtaining a result at least
as extreme as observed.</td>
<td>Derived from a test statistic‚Äôs null distribution; if p &lt; Œ±, reject H<sub>0</sub>.</td>
<td>p = 0.03 for a drug effect typically leads to rejecting ‚Äúno effect‚Äù at Œ±=0.05.</td>
<td>Does not measure effect size or importance; heavily sample‚Äësize‚Äëdependent
and often misinterpreted.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Interpreting p as the probability that the null hypothesis is true.</li>
  <li>Equating ‚Äúnot significant‚Äù with ‚Äúno effect‚Äù, especially in low‚Äëpower studies.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Adjusted Œ±</strong> (Bonferroni Correction)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-pvalue-effect"></div>
    <div class="colorbar-scale">
      <span class="tick-p0">0</span>
      <span class="tick-p005">Œ±/m</span>
      <span class="tick-p1">1.0</span>
    </div>
  </div>

  For m tests, use Œ±<sub>adj</sub> = Œ± / m.<br><br>
  <span class="good">p &lt; Œ±<sub>adj</sub></span> = significant even after strict multiple‚Äëtesting control.<br><br>
  <span class="poor">p ‚â• Œ±<sub>adj</sub></span> = not significant after correction.<br><br>
  This keeps family‚Äëwise error rate ‚âà Œ± (e.g. 0.05).
</td>
<td>Control family‚Äëwise probability of any false positive across multiple tests.</td>
<td>Divides Œ± by number of tests; each test uses Œ±<sub>adj</sub> as threshold.</td>
<td>Simple and conservative; strong control of false positives when tests are
independent or mildly correlated.</td>
<td>20 tests with Œ±=0.05 ‚Üí Œ±<sub>adj</sub>=0.0025; only very small p‚Äëvalues survive.</td>
<td>Can be overly conservative for large m, greatly reducing power.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Applying Bonferroni mechanically in exploratory analyses where some false positives are acceptable.</li>
  <li>Ignoring correlation between tests, which can make the correction overly strict.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Benjamini‚ÄìHochberg FDR</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-pvalue-effect"></div>
    <div class="colorbar-scale">
      <span class="tick-p0">0</span>
      <span class="tick-p005">q</span>
      <span class="tick-p1">1.0</span>
    </div>
  </div>

  Choose FDR <span class="good">q (e.g. 0.05)</span>.<br><br>
  Sort p‚Äëvalues p<sub>(1)</sub> ‚â§ ‚Ä¶ ‚â§ p<sub>(m)</sub>, find largest k with
  <span class="good">p<sub>(k)</sub> ‚â§ (k/m)¬∑q</span>.<br><br>
  Tests 1‚Ä¶k are called significant; expected fraction of false discoveries ‚âà q.
</td>
<td>Control expected proportion of false positives among declared discoveries.</td>
<td>Step‚Äëup procedure comparing ordered p‚Äëvalues to increasing thresholds
(k/m)¬∑q.</td>
<td>Less conservative than Bonferroni; more discoveries at the cost of some
false positives.</td>
<td>Common in genomics / high‚Äëdimensional feature screening.</td>
<td>Controls FDR only in expectation and under certain dependence structures.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Treating FDR‚Äëcontrolled discoveries as if they had strict family‚Äëwise error control.</li>
  <li>Misunderstanding that, even at FDR 5%, some proportion of reported ‚Äúhits‚Äù are expected to be false.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Type I &amp; Type II Error</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-error"></div>
    <div class="colorbar-scale">
      <span class="tick-err0">0</span>
      <span class="tick-errmid">moderate</span>
      <span class="tick-errhi">high</span>
    </div>
  </div>

  <span class="good">Œ± ‚âà 0.01‚Äì0.05</span> = standard false‚Äëpositive risk.<br><br>
  <span class="moderate">Œ≤ ‚âà 0.10‚Äì0.20</span> (power 80‚Äì90%) = typical design target.<br><br>
  <span class="poor">Very high Œ± or Œ≤ (&gt; 0.10 or &gt; 0.30)</span> = too many wrong decisions.
</td>
<td>Frame trade‚Äëoff between false positives (Type I) and false negatives (Type II)
when designing tests and studies.</td>
<td>Type I: reject true H<sub>0</sub>. Type II: fail to reject false H<sub>0</sub>.
Power = 1 ‚àí Œ≤.</td>
<td>Power analysis couples Œ±, Œ≤, effect size, and sample size.</td>
<td>A clinical trial might fix Œ±=0.025 (one‚Äësided) and Œ≤=0.10 (90% power).</td>
<td>Reducing Œ± without increasing sample size generally increases Œ≤.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Choosing a very small Œ± without increasing sample size, making studies underpowered.</li>
  <li>Focusing only on Type I error while ignoring the cost of missed true effects (Type II).</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Statistical Power</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  <span class="poor">&lt; 0.5</span> = often misses real effects.<br><br>
  <span class="moderate">0.5‚Äì0.8</span> = moderate.<br><br>
  <span class="good">&gt; 0.8</span> = usually acceptable; &gt;0.9 even better.
</td>
<td>Probability a test detects a true effect of given size.</td>
<td>Depends on Œ±, effect size, variability, and sample size.</td>
<td>Higher n or larger effect sizes raise power.</td>
<td>Power 0.8 means 80% chance to detect the specified effect if it exists.</td>
<td>Post‚Äëhoc power is usually uninformative; better to plan it a priori.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Doing ‚Äúobserved power‚Äù calculations after non‚Äësignificant results and over‚Äëinterpreting them.</li>
  <li>Ignoring that low power inflates the proportion of false positives among significant findings.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Z / t Tests</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-tstat"></div>
    <div class="colorbar-scale">
      <span class="tick-t0">0</span>
      <span class="tick-t1">1</span>
      <span class="tick-t2">2</span>
      <span class="tick-t3">3+</span>
    </div>
  </div>

  Two‚Äësided tests (large df):<br><br>
  <span class="poor">|Z| or |t| &lt; 1</span> = little evidence.<br><br>
  <span class="moderate">|Z| or |t| ‚âà 2</span> = borderline (p ‚âà 0.05).<br><br>
  <span class="good">|Z| or |t| ‚â• 3</span> = strong evidence against H<sub>0</sub>.
</td>
<td>Test if a mean / difference / regression coefficient differs from a null
value.</td>
<td>Statistic = (estimate ‚àí null) / SE, compared against normal or t distribution.</td>
<td>Large |statistic| means estimate is many SE away from null.</td>
<td>|t| = 4 with df‚âà50 usually implies p &lt; 0.001 (strong evidence).</td>
<td>Assumes approximate normality and independence; sensitive to outliers.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using a Z‚Äëtest instead of a t‚Äëtest with small samples or unknown population variance.</li>
  <li>Ignoring multiple‚Äëtesting corrections when running many t‚Äëtests in parallel.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Chi-Square Test</strong> (œá¬≤)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-chi2"></div>
    <div class="colorbar-scale">
      <span class="tick-chi2-0">0</span>
      <span class="tick-chi2-crit">œá¬≤<sub>crit</sub></span>
      <span class="tick-chi2-high">high</span>
    </div>
  </div>

  <span class="poor">œá¬≤ near 0</span> = data close to H<sub>0</sub> (little evidence of association/effect).<br><br>
  <span class="moderate">œá¬≤ around œá¬≤<sub>crit</sub></span> = borderline significance.<br><br>
  <span class="good">œá¬≤ well above œá¬≤<sub>crit</sub></span> = strong evidence of association / lack of fit to H<sub>0</sub>.
</td>
<td>Test independence (contingency tables) or goodness‚Äëof‚Äëfit of categorical
data to expected counts.</td>
<td>
\[
œá^2 = \sum \frac{(O_i - E_i)^2}{E_i}.
\]</td>
<td>Large œá¬≤ means observed counts deviate strongly from expectations.</td>
<td>Used for testing independence between categorical variables or Mendelian
ratios in genetics, etc.</td>
<td>With very large n, tiny practical differences become significant. Needs
effect size (e.g. Cram√©r‚Äôs V) for magnitude.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Applying œá¬≤ when expected frequencies are too small (e.g. &lt; 5 per cell).</li>
  <li>Interpreting a significant œá¬≤ as evidence of large practical effect without reporting effect size.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>ANOVA F-test</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-fstat"></div>
    <div class="colorbar-scale">
      <span class="tick-f1">1</span>
      <span class="tick-f2">2</span>
      <span class="tick-f3">3</span>
      <span class="tick-f5">5+</span>
    </div>
  </div>

  <span class="poor">F ‚âà 1</span> = no detected difference between means.<br><br>
  <span class="moderate">F ‚âà 2‚Äì3</span> = might be marginally significant (depends on df).<br><br>
  <span class="good">F ‚â´ 1</span> (e.g. &gt;5) = strong evidence at least one group mean differs.
</td>
<td>Test if means of 3+ groups are all equal vs at least one differs.</td>
<td>Compares between‚Äëgroup variance to within‚Äëgroup variance.</td>
<td>Large F implies between‚Äëgroup differences are large relative to noise.</td>
<td>Follow significant F with post‚Äëhoc tests to identify which groups differ.</td>
<td>Assumes normality and equal variances; does not identify specific groups
or effect sizes by itself.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Stopping at a significant overall F without reporting effect sizes or post‚Äëhoc comparisons.</li>
  <li>Ignoring heteroscedasticity or unbalanced designs where standard ANOVA assumptions fail.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Confidence Interval</strong> (e.g. 95% CI)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-ciwidth"></div>
    <div class="colorbar-scale">
      <span class="tick-ci-narrow">narrow</span>
      <span class="tick-ci-mid">medium</span>
      <span class="tick-ci-wide">wide</span>
    </div>
  </div>

  <span class="good">95% CI not containing 0 (difference) or 1 (ratio)</span> ‚Üí statistically significant at ‚âà5%.<br><br>
  <span class="good">Narrow CI</span> ‚Üí precise estimate.<br><br>
  <span class="poor">Very wide CI</span> ‚Üí high uncertainty.
</td>
<td>Provide a range of plausible values for a parameter.</td>
<td>Usually estimate ¬± critical value √ó standard error.</td>
<td>Reflects both effect size and uncertainty.</td>
<td>Difference 5 with 95% CI [2, 8] suggests a clearly positive but moderately
uncertain effect.</td>
<td>Relies on model assumptions; misinterpreted as containing the true value
with 95% probability (frequentist CIs don‚Äôt strictly mean that).</td>
</tr>

<tr>
<td><strong>Permutation Test</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-pvalue-effect"></div>
    <div class="colorbar-scale">
      <span class="tick-p0">0</span>
      <span class="tick-p005">0.05</span>
      <span class="tick-p1">1.0</span>
    </div>
  </div>

  Let p<sub>perm</sub> be the permutation p‚Äëvalue.<br><br>
  <span class="good">p<sub>perm</sub> &lt; 0.05</span> ‚Üí statistic is in extreme tail of null distribution (significant effect).<br><br>
  <span class="poor">p<sub>perm</sub> ‚â• 0.05</span> ‚Üí compatible with chance re‚Äëlabeling.
</td>
<td>Distribution‚Äëfree significance test using label shuffling.</td>
<td>Permute labels many times; recompute statistic to get null distribution and
p<sub>perm</sub>.</td>
<td>Useful for complex statistics where analytic null distributions are hard to
derive.</td>
<td>Accuracy 0.8 vs permutation null mean 0.5 with p<sub>perm</sub>=0.01 is strong
evidence of real signal.</td>
<td>Computationally heavy; must respect structure (e.g. grouping or time).<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Permuting labels in time series or clustered data where observations are not exchangeable.</li>
  <li>Using too few permutations, leading to coarse p‚Äëvalue resolution and unstable conclusions.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Effect Size</strong> (Cohen‚Äôs d)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-02">0.2</span>
      <span class="tick-05">0.5</span>
      <span class="tick-08">0.8</span>
      <span class="tick-10">&gt;1</span>
    </div>
  </div>

  <span class="poor">|d| &lt; 0.2</span> = negligible.<br><br>
  <span class="moderate">0.2‚Äì0.5</span> = small.<br><br>
  <span class="moderate">0.5‚Äì0.8</span> = medium.<br><br>
  <span class="good">&gt; 0.8</span> = large effect.
</td>
<td>Quantify standardized mean differences independent of sample size.</td>
<td>
\[
d = \frac{\bar{x}_1 - \bar{x}_2}{s_{pooled}}.
\]</td>
<td>Helps separate statistical from practical significance.</td>
<td>d = 0.6 for treatment vs control often considered practically important.</td>
<td>Assumes similar SDs; thresholds are rough and context‚Äëdependent.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using generic ‚Äúsmall/medium/large‚Äù thresholds without considering what effect size is practically meaningful in context.</li>
  <li>Ignoring unequal variances where standard pooled‚ÄëSD formula is inappropriate.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Cliff‚Äôs Delta</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-correlation"></div>
    <div class="colorbar-scale">
      <span class="tick-m1">-1</span>
      <span class="tick-m05">-0.5</span>
      <span class="tick-0c">0</span>
      <span class="tick-05c">0.5</span>
      <span class="tick-1c">1</span>
    </div>
  </div>

  <span class="poor">|Œ¥| &lt; 0.147</span> = negligible.<br><br>
  <span class="moderate">0.147‚Äì0.33</span> = small.<br><br>
  <span class="moderate">0.33‚Äì0.474</span> = medium.<br><br>
  <span class="good">&gt; 0.474</span> = large effect (strong dominance).
</td>
<td>Non‚Äëparametric effect size based on ranks; robust to non‚Äënormal data.</td>
<td>Œ¥ = P(X&gt;Y) ‚àí P(Y&gt;X), where X/Y from two groups; ranges ‚àí1..1.</td>
<td>Equivalent to rank‚Äëbiserial correlation; sign shows direction, magnitude
shows strength.</td>
<td>Œ¥ = 0.5 means X exceeds Y in 75% of pairs, a very strong effect.</td>
<td>Summarises ordering, not magnitude of differences; can be less intuitive
than differences in means.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Reporting Œ¥ without clarifying the underlying direction (which group is X vs Y).</li>
  <li>Assuming Œ¥ behaves like Pearson r or Cohen‚Äôs d in terms of interpretation thresholds.</li>
</ul>
</td>
</tr>

</tbody>
</table>
</div>

<h2>Real‚ÄëDataset Examples for Common Tests</h2>
<ul>
  <li><strong>t‚Äëtest:</strong> compare mean blood pressure in treatment vs control groups in a clinical trial.</li>
  <li><strong>Paired t‚Äëtest:</strong> before/after measurements of the same patients after an intervention.</li>
  <li><strong>Chi‚Äësquare test:</strong> association between smoking status and lung‚Äëdisease incidence.</li>
  <li><strong>ANOVA F‚Äëtest:</strong> compare average click‚Äëthrough rate across several ad creatives.</li>
  <li><strong>Permutation test:</strong> evaluate whether model accuracy exceeds chance by shuffling labels.</li>
  <li><strong>KS test:</strong> detect distribution shift between training and production feature distributions.</li>
</ul>
</details>













<!-- ============================
     Statistical Tests & Assumptions (Collapsible)
     ============================ -->

<section id="stat-tests-wrapper">
  <button id="stat-tests-toggle" class="ref-toggle-btn" type="button">
    ‚ñ∏ Statistical Tests & Assumptions ‚Äì Quick Reference
  </button>

  <div id="stat-tests-section" class="ref-section">
    <div class="ref-header">
      <h2>Statistical Tests & Assumptions ‚Äì Quick Reference</h2>
      <p class="ref-subtitle">
        What each test is asking, when to use it, and what can go wrong.
      </p>
    </div>

    <!-- Big picture -->
    <section class="ref-card">
      <h3>Big picture</h3>
      <ul class="ref-list">
        <li>
          <strong>Every test asks a question.</strong><br>
          ‚ÄúAre these means equal?‚Äù, ‚ÄúAre these variances equal?‚Äù, ‚ÄúDo these two
          distributions look the same?‚Äù, ‚ÄúIs there autocorrelation?‚Äù.
        </li>
        <li>
          <strong>p-value is not the effect size.</strong><br>
          A tiny p-value can correspond to a tiny, unimportant effect if the sample is huge.
        </li>
        <li>
          <strong>Assumptions matter.</strong><br>
          Many tests assume things like normal residuals, equal variances, or
          independent observations. If those are badly violated, the p-values
          can be misleading.
        </li>
        <li>
          <strong>Multiple testing inflates false positives.</strong><br>
          If you run many tests, you need FWER/FDR control (Bonferroni, Holm, BH).
        </li>
      </ul>
    </section>

    <!-- Normality & distribution shape -->
    <section class="ref-card">
      <h3>Distribution shape & normality</h3>
      <p>
        These tests check whether data or residuals look like they come from a
        particular distribution (usually normal). They are sensitive to
        <strong>sample size</strong> and to <strong>outliers</strong>.
      </p>

      <table class="metrics-table metrics-table-compact">
        <thead>
          <tr>
            <th>Test</th>
            <th>Main question</th>
            <th>Typical use</th>
            <th>Notes &amp; pitfalls</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Shapiro‚ÄìWilk</strong></td>
            <td>‚ÄúDo these data look roughly normal?‚Äù</td>
            <td>Small to medium samples (<em>n</em> &lt; ~2000)</td>
            <td>
              Powerful for normality; very sensitive to even small deviations in
              large samples. Always combine with <em>plots</em> (QQ-plot, histogram).
            </td>
          </tr>
          <tr>
            <td><strong>Kolmogorov‚ÄìSmirnov (KS)</strong></td>
            <td>
              ‚ÄúIs the sample distribution different from a reference distribution
              (or from another sample)?‚Äù
            </td>
            <td>
              Comparing one sample to a known distribution, or two independent samples.
            </td>
            <td>
              Works for continuous data; most sensitive near the centre, less in the tails.
              With estimated parameters, classical p-values need corrections.
            </td>
          </tr>
          <tr>
            <td><strong>Anderson‚ÄìDarling</strong></td>
            <td>‚ÄúDo these data follow a given distribution, especially in the tails?‚Äù</td>
            <td>Checking normality with more tail focus than KS.</td>
            <td>
              Gives more weight to tails than KS. As with other tests, large <em>n</em>
              ‚áí tiny deviations become ‚Äúsignificant‚Äù.
            </td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- Equality of variances -->
    <section class="ref-card">
      <h3>Equality of variances</h3>
      <p>
        Many tests (for example classical t-test, ANOVA) assume
        <strong>similar variances across groups</strong>. These tests check that.
      </p>

      <table class="metrics-table metrics-table-compact">
        <thead>
          <tr>
            <th>Test</th>
            <th>Main question</th>
            <th>Data type</th>
            <th>Notes &amp; pitfalls</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Levene‚Äôs test</strong></td>
            <td>‚ÄúDo these groups have equal variances?‚Äù</td>
            <td>Continuous outcome, groups categorical</td>
            <td>
              More robust to non-normal data than classical tests. A small p-value
              suggests at least one group has a different variance.
            </td>
          </tr>
          <tr>
            <td><strong>Brown‚ÄìForsythe</strong></td>
            <td>Levene‚Äôs test variant using medians instead of means.</td>
            <td>Continuous outcome, heavy tails or outliers</td>
            <td>
              Even more robust when distributions are skewed. Often preferred if
              outliers are expected.
            </td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- Comparing means -->
    <section class="ref-card">
      <h3>Comparing means</h3>
      <p>
        These tests compare group averages. Non-parametric alternatives use
        <strong>ranks</strong> instead of assuming normality.
      </p>

      <table class="metrics-table metrics-table-compact">
        <thead>
          <tr>
            <th>Test</th>
            <th>Main question</th>
            <th>Design</th>
            <th>Notes &amp; pitfalls</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>t-test (independent)</strong></td>
            <td>‚ÄúAre the means of two independent groups equal?‚Äù</td>
            <td>Two groups, continuous outcome</td>
            <td>
              Assumes normal residuals and (often) equal variances. For unequal
              variances, use Welch‚Äôs t-test.
            </td>
          </tr>
          <tr>
            <td><strong>t-test (paired)</strong></td>
            <td>‚ÄúIs the mean difference between paired measurements zero?‚Äù</td>
            <td>Before/after, matched pairs</td>
            <td>
              Applied to <em>differences</em>. Assumes differences are roughly normal.
            </td>
          </tr>
          <tr>
            <td><strong>One-way ANOVA</strong></td>
            <td>‚ÄúAre all group means equal?‚Äù</td>
            <td>3+ groups, continuous outcome</td>
            <td>
              Global test; if significant, follow with post-hoc comparisons
              (and multiple-testing correction). Assumes normal residuals &
              equal variances.
            </td>
          </tr>
          <tr>
            <td><strong>Mann‚ÄìWhitney U</strong></td>
            <td>‚ÄúDo two groups differ in their typical values (medians/ranks)?‚Äù</td>
            <td>Two independent groups, ranked/continuous outcome</td>
            <td>
              Non-parametric alternative to the independent t-test. Tests
              <em>distribution shift</em>, not strictly medians.
            </td>
          </tr>
          <tr>
            <td><strong>Kruskal‚ÄìWallis</strong></td>
            <td>‚ÄúDo 3+ groups differ in their distributions (ranks)?‚Äù</td>
            <td>3+ independent groups, ranked/continuous outcome</td>
            <td>
              Non-parametric analogue of one-way ANOVA. If significant, follow
              with pairwise rank tests + multiple-testing correction.
            </td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- Categorical & independence -->
    <section class="ref-card">
      <h3>Categorical data & independence</h3>
      <p>
        These tests work on <strong>counts</strong> in contingency tables and
        ask whether patterns could be explained by chance alone.
      </p>

      <table class="metrics-table metrics-table-compact">
        <thead>
          <tr>
            <th>Test</th>
            <th>Main question</th>
            <th>Typical table</th>
            <th>Notes &amp; pitfalls</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Chi-square test of independence</strong></td>
            <td>‚ÄúAre two categorical variables independent?‚Äù</td>
            <td>R √ó C contingency table (for example treatment √ó outcome)</td>
            <td>
              Expected counts should not be too small (rules of thumb apply).
              Large samples make tiny deviations ‚Äúsignificant‚Äù.
            </td>
          </tr>
          <tr>
            <td><strong>Chi-square goodness-of-fit</strong></td>
            <td>‚ÄúDo observed category frequencies match a specified distribution?‚Äù</td>
            <td>1 √ó C table (observed vs expected counts)</td>
            <td>
              Used to compare observed counts to a theoretical or historical pattern.
            </td>
          </tr>
          <tr>
            <td><strong>Fisher‚Äôs exact test</strong></td>
            <td>‚ÄúIs there association in a 2√ó2 table?‚Äù</td>
            <td>2 √ó 2 table with small counts</td>
            <td>
              Exact test, no large-sample approximation. Useful when any
              expected count is small (&lt;5).
            </td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- Time series & autocorrelation -->
    <section class="ref-card">
      <h3>Time-series residuals & autocorrelation</h3>
      <p>
        For time-ordered data, errors often correlate over time. These tests
        check whether residuals look ‚Äúindependent‚Äù or show systematic patterns.
      </p>

      <table class="metrics-table metrics-table-compact">
        <thead>
          <tr>
            <th>Test</th>
            <th>Main question</th>
            <th>Typical use</th>
            <th>Notes &amp; pitfalls</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Durbin‚ÄìWatson</strong></td>
            <td>‚ÄúIs there first-order autocorrelation in regression residuals?‚Äù</td>
            <td>Linear regression on time-ordered data</td>
            <td>
              Values near 2 ‚âà no autocorrelation; near 0 ‚âà strong positive
              autocorrelation; near 4 ‚âà strong negative. Not designed for
              complex time-series models.
            </td>
          </tr>
          <tr>
            <td><strong>Ljung‚ÄìBox</strong></td>
            <td>‚ÄúAre a set of autocorrelations jointly zero?‚Äù</td>
            <td>
              Checking whether residuals from a time-series model look like white noise.
            </td>
            <td>
              Tests several lags at once. A small p-value suggests remaining
              structure in residuals (model underfits dynamics).
            </td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- Summary bullets -->
    <section class="ref-card">
      <h3>Summary: how to think about tests</h3>
      <ul class="ref-list">
        <li>
          <strong>Always pair tests with plots.</strong> QQ-plots, residual plots
          and histograms often tell the story faster than p-values.
        </li>
        <li>
          <strong>Large samples detect tiny issues.</strong> A ‚Äúsignificant‚Äù
          deviation may be practically irrelevant.
        </li>
        <li>
          <strong>Small samples lack power.</strong> A non-significant result
          does not prove that assumptions are perfect or effects are zero.
        </li>
        <li>
          <strong>Use non-parametric tests</strong> when normality or equal
          variances are clearly violated and sample sizes are moderate.
        </li>
        <li>
          <strong>Remember multiple testing.</strong> Running many tests on the
          same data requires FWER/FDR control, otherwise false positives
          accumulate fast.
        </li>
      </ul>
    </section>
  </div>
</section>















<!-- ===================================== -->
<!-- 3. ROBUSTNESS & RESAMPLING            -->
<!-- ===================================== -->
<details open>
<summary>Robustness &amp; Resampling</summary>
<div class="table-responsive">
  
  
  
  
  
  
  
    <section id="robustness-playground">
  <style>
    #robustness-playground {
      margin: 2rem 0;
      padding: 1.5rem;
      border-radius: 10px;
      background: #111827;
      color: #e5e7eb;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      box-shadow: 0 4px 10px rgba(0,0,0,0.4);
    }

    #robustness-playground h2 {
      margin-top: 0;
      margin-bottom: 0.75rem;
      font-size: 1.2rem;
    }

    #robustness-playground p.desc {
      margin-top: 0;
      margin-bottom: 1rem;
      font-size: 0.9rem;
      color: #9ca3af;
    }

    #robustness-playground .rp-layout {
      display: grid;
      grid-template-columns: minmax(0, 1.4fr) minmax(0, 1fr);
      gap: 1.25rem;
      align-items: flex-start;
    }

    @media (max-width: 900px) {
      #robustness-playground .rp-layout {
        grid-template-columns: 1fr;
      }
    }

    #robustness-playground .rp-card {
      background: #020617;
      border-radius: 10px;
      padding: 0.85rem 1rem;
      border: 1px solid #1f2937;
    }

    #robustness-playground label {
      font-size: 0.8rem;
      display: block;
      margin-bottom: 0.25rem;
      color: #9ca3af;
    }

    #robustness-playground textarea {
      width: 100%;
      min-height: 70px;
      border-radius: 6px;
      border: 1px solid #374151;
      background: #020617;
      color: #e5e7eb;
      padding: 0.5rem;
      font-family: "Fira Code", Menlo, Consolas, monospace;
      font-size: 0.8rem;
      resize: vertical;
    }

    #robustness-playground .rp-controls-row {
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      margin-top: 0.7rem;
      align-items: center;
    }

    #robustness-playground button {
      border-radius: 6px;
      border: 1px solid #4b5563;
      background: #111827;
      color: #e5e7eb;
      padding: 0.35rem 0.7rem;
      font-size: 0.8rem;
      cursor: pointer;
    }

    #robustness-playground button:hover {
      background: #1f2937;
    }

    #robustness-playground input[type="range"] {
      width: 180px;
    }

    #robustness-playground .rp-slider-label {
      font-size: 0.8rem;
      color: #9ca3af;
    }

    #robustness-playground .rp-slider-value {
      font-weight: 600;
      color: #facc15;
      margin-left: 0.25rem;
    }

    #robustness-playground .rp-metrics {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
      gap: 0.4rem;
      font-size: 0.8rem;
    }

    #robustness-playground .rp-metric {
      padding: 0.35rem 0.45rem;
      border-radius: 6px;
      background: #020617;
      border: 1px solid #111827;
    }

    #robustness-playground .rp-metric span.label {
      display: block;
      color: #9ca3af;
      font-size: 0.75rem;
    }

    #robustness-playground .rp-metric span.value {
      font-weight: 600;
    }

    #robustness-playground .rp-metric span.robust {
      color: #4ade80;
    }

    #robustness-playground .rp-metric span.sensitive {
      color: #fb7185;
    }

    #robustness-playground svg {
      width: 100%;
      max-width: 700px;
      height: 140px;
      background: #020617;
      border-radius: 8px;
      border: 1px solid #111827;
    }

    #robustness-playground .rp-hint {
      margin-top: 0.6rem;
      font-size: 0.8rem;
      color: #9ca3af;
    }
  </style>

  <h2>Robustness Playground: Mean vs Median & Outliers</h2>
  <p class="desc">
    Type any numbers, then drag the outlier slider. Watch how the <strong>mean</strong> swings
    while the <strong>median</strong> and <strong>IQR</strong> stay more stable. This is what ‚Äúrobustness to outliers‚Äù
    looks like in practice.
  </p>

  <div class="rp-layout">
    <!-- Controls -->
    <div class="rp-card">
      <label for="rp-data-input">Data points (comma or space separated):</label>
      <textarea id="rp-data-input"></textarea>

      <div class="rp-controls-row">
        <button id="rp-example-btn" type="button">Load example</button>
        <button id="rp-apply-btn" type="button">Apply data</button>

        <div class="rp-slider-label">
          Extra outlier:
          <input id="rp-outlier-slider" type="range" min="-40" max="40" step="1" value="0">
          <span id="rp-outlier-value" class="rp-slider-value">0</span>
        </div>
      </div>

      <p class="rp-hint">
        Try: <em>1, 2, 3, 4, 5</em> then add an outlier like <em>+30</em>.  
        The mean moves a lot; the median barely moves.<p><br>
        The "Robustness Playground" is a visual tool proving that the median is generally a better measure of the "typical" center for data that might contain errors or extreme values, because it resists the pull of outliers much better than the mean.


Robustness in statistics means that a measurement (like the median) remains relatively unchanged even if a few data points are very unusual or extreme (outliers).

Non-robust measurements (like the mean) are highly sensitive to these extreme values and can be pulled significantly in one direction.

      </p>
    </div>

    <!-- Metrics + Plot -->
    <div class="rp-card">
      <div class="rp-metrics" id="rp-metrics">
        <!-- filled by JS -->
      </div>
      <svg id="rp-plot"></svg>
    </div>
  </div>

  <script>
    (function () {
      const root = document.getElementById('robustness-playground');
      if (!root) return;

      const dataInput = root.querySelector('#rp-data-input');
      const exampleBtn = root.querySelector('#rp-example-btn');
      const applyBtn = root.querySelector('#rp-apply-btn');
      const outlierSlider = root.querySelector('#rp-outlier-slider');
      const outlierValueSpan = root.querySelector('#rp-outlier-value');
      const metricsContainer = root.querySelector('#rp-metrics');
      const svg = root.querySelector('#rp-plot');

      function parseData(text) {
        if (!text) return [];
        return text
          .split(/[\s,;]+/)
          .map(s => s.trim())
          .filter(s => s.length > 0)
          .map(Number)
          .filter(v => Number.isFinite(v));
      }

      function computeStats(values) {
        const n = values.length;
        if (!n) return null;

        const sorted = [...values].sort((a, b) => a - b);
        const mean = sorted.reduce((a, b) => a + b, 0) / n;

        const mid = Math.floor(n / 2);
        const median = n % 2 ? sorted[mid] : (sorted[mid - 1] + sorted[mid]) / 2;

        const q1 = sorted[Math.floor(n * 0.25)];
        const q3 = sorted[Math.floor(n * 0.75)];
        const iqr = q3 - q1;

        const trimmedSorted = sorted.slice(Math.floor(n * 0.1), Math.ceil(n * 0.9));
        const trimmedMean = trimmedSorted.reduce((a, b) => a + b, 0) / trimmedSorted.length;

        return { n, sorted, mean, median, q1, q3, iqr, trimmedMean };
      }

      function updateMetrics(stats) {
        metricsContainer.innerHTML = '';
        if (!stats) {
          metricsContainer.innerHTML = '<div class="rp-metric"><span class="label">Status</span><span class="value">No valid data</span></div>';
          return;
        }
        const items = [
          ['Count', stats.n.toString(), ''],
          ['Mean', stats.mean.toFixed(2), 'sensitive'],
          ['Median', stats.median.toFixed(2), 'robust'],
          ['Trimmed mean (10%)', stats.trimmedMean.toFixed(2), 'robust'],
          ['Q1 / Q3', `${stats.q1.toFixed(2)} / ${stats.q3.toFixed(2)}`, 'robust'],
          ['IQR', stats.iqr.toFixed(2), 'robust']
        ];

        for (const [label, value, tone] of items) {
          const div = document.createElement('div');
          div.className = 'rp-metric';
          div.innerHTML = `
            <span class="label">${label}</span>
            <span class="value ${tone === 'robust' ? 'robust' : tone === 'sensitive' ? 'sensitive' : ''}">${value}</span>
          `;
          metricsContainer.appendChild(div);
        }
      }

      function drawPlot(stats) {
        while (svg.firstChild) svg.removeChild(svg.firstChild);
        const width = svg.clientWidth || 600;
        const height = svg.clientHeight || 140;
        const padding = 35;

        svg.setAttribute('viewBox', `0 0 ${width} ${height}`);

        if (!stats || stats.sorted.length === 0) return;

        const data = stats.sorted;
        const min = Math.min(...data, stats.mean, stats.median);
        const max = Math.max(...data, stats.mean, stats.median);
        const spread = max - min || 1;

        function xScale(v) {
          return padding + ((v - min) / spread) * (width - 2 * padding);
        }

        const baselineY = height / 2;

        // Baseline
        const baseLine = document.createElementNS('http://www.w3.org/2000/svg', 'line');
        baseLine.setAttribute('x1', padding);
        baseLine.setAttribute('x2', width - padding);
        baseLine.setAttribute('y1', baselineY);
        baseLine.setAttribute('y2', baselineY);
        baseLine.setAttribute('stroke', '#4b5563');
        baseLine.setAttribute('stroke-width', '1');
        svg.appendChild(baseLine);

        // Points
        data.forEach(v => {
          const cx = xScale(v);
          const cy = baselineY + (Math.random() - 0.5) * 14; // small jitter
          const circle = document.createElementNS('http://www.w3.org/2000/svg', 'circle');
          circle.setAttribute('cx', cx);
          circle.setAttribute('cy', cy);
          circle.setAttribute('r', 4);
          circle.setAttribute('fill', '#60a5fa');
          svg.appendChild(circle);
        });

        function drawLine(value, color, label) {
          const x = xScale(value);
          const line = document.createElementNS('http://www.w3.org/2000/svg', 'line');
          line.setAttribute('x1', x);
          line.setAttribute('x2', x);
          line.setAttribute('y1', baselineY - 30);
          line.setAttribute('y2', baselineY + 30);
          line.setAttribute('stroke', color);
          line.setAttribute('stroke-width', '2');
          svg.appendChild(line);

          const text = document.createElementNS('http://www.w3.org/2000/svg', 'text');
          text.setAttribute('x', x + 3);
          text.setAttribute('y', baselineY - 34);
          text.setAttribute('fill', color);
          text.setAttribute('font-size', '10');
          text.textContent = label;
          svg.appendChild(text);
        }

        drawLine(stats.mean, '#f97373', 'mean');
        drawLine(stats.median, '#4ade80', 'median');
      }

      function updateAll() {
        const baseValues = parseData(dataInput.value);
        const outlier = Number(outlierSlider.value || 0);
        const values = baseValues.slice();
        if (outlier !== 0) values.push(outlier);

        const stats = computeStats(values);
        updateMetrics(stats);
        drawPlot(stats);
      }

      exampleBtn.addEventListener('click', () => {
        dataInput.value = '1, 2, 2.5, 3, 3.5, 4, 4.5, 5';
        outlierSlider.value = 0;
        outlierValueSpan.textContent = '0';
        updateAll();
      });

      applyBtn.addEventListener('click', () => {
        updateAll();
      });

      outlierSlider.addEventListener('input', () => {
        outlierValueSpan.textContent = outlierSlider.value;
        updateAll();
      });

      // Initial state
      dataInput.value = '1, 2, 2.5, 3, 3.5, 4, 4.5, 5';
      updateAll();
    })();
  </script>
</section>

  
  
  
  
  
  
  
  <section id="resampling-stability-playground">
  <style>
    #resampling-stability-playground {
      margin: 2rem 0;
      padding: 1.5rem;
      border-radius: 10px;
      background: #0b1120;
      color: #e5e7eb;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      box-shadow: 0 4px 12px rgba(0,0,0,0.45);
      border: 1px solid #1e293b;
    }
    #resampling-stability-playground h2 {
      margin: 0 0 0.4rem;
      font-size: 1.2rem;
    }
    #resampling-stability-playground p.rsp-desc {
      margin: 0 0 1rem;
      font-size: 0.9rem;
      color: #9ca3af;
    }
    #resampling-stability-playground .rsp-layout {
      display: grid;
      grid-template-columns: minmax(0, 1.3fr) minmax(0, 1fr);
      gap: 1.25rem;
      align-items: flex-start;
    }
    @media (max-width: 900px) {
      #resampling-stability-playground .rsp-layout {
        grid-template-columns: 1fr;
      }
    }
    #resampling-stability-playground .rsp-card {
      background: #020617;
      border-radius: 10px;
      padding: 0.9rem 1rem;
      border: 1px solid #1e293b;
    }
    #resampling-stability-playground label {
      display: block;
      font-size: 0.8rem;
      color: #9ca3af;
      margin-bottom: 0.25rem;
    }
    #resampling-stability-playground .rsp-row {
      display: flex;
      flex-wrap: wrap;
      gap: 0.9rem;
      margin-bottom: 0.6rem;
      align-items: center;
    }
    #resampling-stability-playground input[type="range"] {
      width: 160px;
    }
    #resampling-stability-playground .rsp-slider-label {
      font-size: 0.8rem;
      color: #9ca3af;
      display: flex;
      align-items: center;
      gap: 0.35rem;
    }
    #resampling-stability-playground .rsp-slider-value {
      font-weight: 600;
      color: #facc15;
      min-width: 2.6em;
      text-align: right;
    }
    #resampling-stability-playground button {
      border-radius: 6px;
      border: 1px solid #4b5563;
      background: #111827;
      color: #e5e7eb;
      padding: 0.45rem 0.9rem;
      font-size: 0.8rem;
      cursor: pointer;
    }
    #resampling-stability-playground button:hover {
      background: #1f2937;
    }
    #resampling-stability-playground .rsp-note {
      margin-top: 0.4rem;
      font-size: 0.8rem;
      color: #9ca3af;
    }
    #resampling-stability-playground .rsp-metrics {
      margin-top: 0.3rem;
      font-size: 0.78rem;
      border-collapse: collapse;
      width: 100%;
    }
    #resampling-stability-playground .rsp-metrics th,
    #resampling-stability-playground .rsp-metrics td {
      padding: 0.25rem 0.35rem;
      border-bottom: 1px solid #111827;
      text-align: right;
      white-space: nowrap;
    }
    #resampling-stability-playground .rsp-metrics th:first-child,
    #resampling-stability-playground .rsp-metrics td:first-child {
      text-align: left;
    }
    #resampling-stability-playground .rsp-metrics th {
      font-weight: 600;
      color: #9ca3af;
    }
    #resampling-stability-playground .rsp-tag {
      display: inline-block;
      padding: 0.1rem 0.3rem;
      border-radius: 999px;
      font-size: 0.7rem;
      border: 1px solid #4b5563;
      color: #e5e7eb;
    }
    #resampling-stability-playground .rsp-tag-strong {
      border-color: #22c55e;
      color: #22c55e;
    }
    #resampling-stability-playground .rsp-tag-weak {
      border-color: #fb7185;
      color: #fb7185;
    }
    #resampling-stability-playground svg {
      width: 100%;
      max-width: 580px;
      height: 180px;
      background: #020617;
      border-radius: 8px;
      border: 1px solid #111827;
    }
    #resampling-stability-playground .rsp-legend {
      margin-top: 0.4rem;
      font-size: 0.75rem;
      color: #9ca3af;
    }
    #resampling-stability-playground .rsp-legend span.box {
      display: inline-block;
      width: 10px;
      height: 10px;
      margin-right: 0.25rem;
      border-radius: 2px;
      
      
          #resampling-stability-playground .rsp-left-col {
      display: flex;
      flex-direction: column;
      gap: 0.75rem;
    }

    #resampling-stability-playground .rsp-info-card h3 {
      margin: 0 0 0.4rem;
      font-size: 0.95rem;
    }

    #resampling-stability-playground .rsp-info-card h4 {
      margin: 0.4rem 0 0.2rem;
      font-size: 0.85rem;
    }

    #resampling-stability-playground .rsp-info-card p,
    #resampling-stability-playground .rsp-info-card ul {
      font-size: 0.8rem;
      line-height: 1.4;
      color: #9ca3af;
      margin: 0 0 0.4rem;
      padding-left: 0;
    }

    #resampling-stability-playground .rsp-info-card ul {
      list-style: disc;
      padding-left: 1.2rem;
    }

    body:not(.dark) #resampling-stability-playground .rsp-info-card p,
    body:not(.dark) #resampling-stability-playground .rsp-info-card ul {
      color: #4b5563;
    }

      
      
      
      
    }

    /* Light mode override ‚Äì looks like your mean/median section */
    body:not(.dark) #resampling-stability-playground {
      background: #f9fafb;
      color: #111827;
      border-color: #e5e7eb;
      box-shadow: 0 1px 3px rgba(15,23,42,0.08);
    }
    body:not(.dark) #resampling-stability-playground .rsp-card {
      background: #ffffff;
      border-color: #e5e7eb;
    }
    body:not(.dark) #resampling-stability-playground .rsp-desc,
    body:not(.dark) #resampling-stability-playground .rsp-note {
      color: #4b5563;
    }
    body:not(.dark) #resampling-stability-playground .rsp-slider-label {
      color: #4b5563;
    }
    body:not(.dark) #resampling-stability-playground .rsp-metrics th {
      color: #6b7280;
    }
    body:not(.dark) #resampling-stability-playground svg {
      background: #ffffff;
      border-color: #e5e7eb;
    }
    body:not(.dark) #resampling-stability-playground .rsp-legend {
      color: #6b7280;
    }
    body:not(.dark) #resampling-stability-playground .rsp-tag {
      border-color: #9ca3af;
      color: #374151;
    }
    body:not(.dark) #resampling-stability-playground .rsp-tag-strong {
      border-color: #16a34a;
      color: #166534;
    }
    body:not(.dark) #resampling-stability-playground .rsp-tag-weak {
      border-color: #f97373;
      color: #b91c1c;
    }
  </style>

  <h2>Resampling Stability Playground (Bootstrap vs Jackknife)</h2>
  <p class="rsp-desc">
    This playground simulates a simple linear model with one true signal feature and one noise feature.
    Play with sample size, noise, and signal strength, then compare how a single fit, bootstrap, and jackknife
    disagree about the coefficient. Watch how <strong>CI width</strong> and <strong>sign stability</strong> change.
  </p>

    <div class="rsp-layout">
    <!-- LEFT COLUMN: controls + explanation -->
    <div class="rsp-left-col">
      <!-- Controls -->
      <div class="rsp-card">
        <div class="rsp-row">
          <div class="rsp-slider-label">
            Sample size
            <input id="rsp-n" type="range" min="30" max="250" step="10" value="100">
            <span id="rsp-n-val" class="rsp-slider-value">100</span>
          </div>
        </div>

        <div class="rsp-row">
          <div class="rsp-slider-label">
            Noise level (œÉ)
            <input id="rsp-noise" type="range" min="0" max="3" step="0.2" value="1.0">
            <span id="rsp-noise-val" class="rsp-slider-value">1.0</span>
          </div>
        </div>

        <div class="rsp-row">
          <div class="rsp-slider-label">
            Signal strength (Œ≤‚ÇÅ)
            <input id="rsp-signal" type="range" min="0.2" max="3" step="0.2" value="1.5">
            <span id="rsp-signal-val" class="rsp-slider-value">1.5</span>
          </div>
        </div>

        <div class="rsp-row">
          <button type="button" id="rsp-resample-btn">Generate new data + resamples</button>
          <span class="rsp-note">
            Model: <em>y = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑x‚ÇÅ + Œµ</em> with an extra noise feature <em>x‚ÇÇ</em> (true Œ≤‚ÇÇ = 0).
          </span>
        </div>

        <p class="rsp-note">
          Look for: when noise is high or n is small, <strong>single fit</strong> can be very misleading.
          Bootstrap and jackknife show how uncertain the coefficient really is.
        </p>
      </div>

      <!-- Explanation card -->
      <div class="rsp-card rsp-info-card">
        <h3>How to read this playground</h3>

        <p>
          This Resampling Stability Playground compares two resampling methods ‚Äì <strong>Bootstrap</strong>
          and <strong>Jackknife</strong> ‚Äì for a simple linear model. It answers:
          <em>‚ÄúHow stable is my estimated coefficient under resampling?‚Äù</em>
        </p>

        <h4>Purpose</h4>
        <p>
          The model is <em>y = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑x‚ÇÅ + Œµ</em> with one true signal feature (<em>x‚ÇÅ</em>) and one pure
          noise feature (<em>x‚ÇÇ</em>, true Œ≤‚ÇÇ = 0). By changing sample size, noise level, and signal
          strength, you can see when estimates are stable vs. when they are fragile.
        </p>

        <h4>How to use the controls</h4>
        <ul>
          <li><strong>Sample size</strong> ‚Äì more observations usually mean tighter intervals.</li>
          <li><strong>Noise level</strong> ‚Äì higher noise makes estimates wobble more.</li>
          <li><strong>Signal strength</strong> ‚Äì stronger Œ≤‚ÇÅ is easier to detect reliably.</li>
          <li><strong>Generate new data</strong> ‚Äì redraws a fresh dataset and new resamples.</li>
        </ul>

        <h4>Reading the plot</h4>
        <ul>
          <li>The vertical green dashed line shows the <strong>true Œ≤‚ÇÅ</strong>.</li>
          <li>The orange dot is the <strong>single-fit estimate</strong> on the full sample.</li>
          <li>The blue bar is the <strong>Bootstrap 95% CI</strong> for Œ≤‚ÇÅ.</li>
          <li>The purple bar is the <strong>Jackknife 95% CI</strong> for Œ≤‚ÇÅ.</li>
        </ul>

        <h4>Reading the table</h4>
        <ul>
          <li><strong>CI width</strong> ‚Äì how wide the interval is (narrow = more precise).</li>
          <li><strong>Sign stable</strong> ‚Äì fraction of resamples that keep the same sign as the mean.</li>
          <li>
            Green ‚Äúhigh stability‚Äù tags mean the method gives a tight CI and almost never flips sign;
            red ‚Äúlow stability‚Äù tags mean the estimate is fragile.
          </li>
        </ul>

        <p>
          Use this to build intuition for <strong>robustness</strong>: when resampling methods agree
          and intervals are narrow, your inference is much safer than when everything jumps around.
        </p>
      </div>
    </div>

    <!-- RIGHT COLUMN: plot + metrics -->
    <div class="rsp-card">
      <svg id="rsp-plot"></svg>
      <div class="rsp-legend">
        <span class="box" style="background:#38bdf8;"></span> Bootstrap 95% CI &nbsp;
        <span class="box" style="background:#a855f7;"></span> Jackknife 95% CI &nbsp;
        <span class="box" style="background:#f97316;"></span> Single-fit estimate
      </div>

      <table class="rsp-metrics" id="rsp-metrics-table"></table>

      <p class="rsp-note">
        <span class="rsp-tag rsp-tag-strong">High stability</span> = coefficient keeps sign and tight CI.<br>
        <span class="rsp-tag rsp-tag-weak">Low stability</span> = sign flips often / very wide CI.
      </p>
    </div>
  </div>

  <script>
    (function () {
      const root = document.getElementById('resampling-stability-playground');
      if (!root) return;

      const nSlider      = root.querySelector('#rsp-n');
      const nVal         = root.querySelector('#rsp-n-val');
      const noiseSlider  = root.querySelector('#rsp-noise');
      const noiseVal     = root.querySelector('#rsp-noise-val');
      const signalSlider = root.querySelector('#rsp-signal');
      const signalVal    = root.querySelector('#rsp-signal-val');
      const resampleBtn  = root.querySelector('#rsp-resample-btn');
      const svg          = root.querySelector('#rsp-plot');
      const metricsTable = root.querySelector('#rsp-metrics-table');

      function randn() {
        // Box‚ÄìMuller
        let u = 0, v = 0;
        while (u === 0) u = Math.random();
        while (v === 0) v = Math.random();
        return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2.0 * Math.PI * v);
      }

      function generateData(n, noise, signal) {
        const x1 = new Array(n);
        const x2 = new Array(n);
        const y  = new Array(n);
        for (let i = 0; i < n; i++) {
          const a = randn();
          const b = randn();
          x1[i] = a;
          x2[i] = b;
          y[i]  = signal * a + noise * randn(); // Œ≤0 = 0, Œ≤2 = 0
        }
        return { x1, x2, y };
      }

      function fitOLS(x1, x2, y) {
        const n = y.length;
        let sx1 = 0, sx2 = 0, sy = 0;
        let sx1x1 = 0, sx2x2 = 0, sx1x2 = 0;
        let sx1y = 0, sx2y = 0;

        for (let i = 0; i < n; i++) {
          const xi1 = x1[i], xi2 = x2[i], yi = y[i];
          sx1 += xi1;
          sx2 += xi2;
          sy  += yi;
          sx1x1 += xi1 * xi1;
          sx2x2 += xi2 * xi2;
          sx1x2 += xi1 * xi2;
          sx1y  += xi1 * yi;
          sx2y  += xi2 * yi;
        }

        const A = [
          [n,     sx1,    sx2],
          [sx1,   sx1x1,  sx1x2],
          [sx2,   sx1x2,  sx2x2]
        ];
        const b = [sy, sx1y, sx2y];

        const detA =
          A[0][0]*(A[1][1]*A[2][2]-A[1][2]*A[2][1]) -
          A[0][1]*(A[1][0]*A[2][2]-A[1][2]*A[2][0]) +
          A[0][2]*(A[1][0]*A[2][1]-A[1][1]*A[2][0]);

        if (Math.abs(detA) < 1e-10) return { b0: NaN, b1: NaN, b2: NaN };

        const inv = [[0,0,0],[0,0,0],[0,0,0]];
        inv[0][0] = (A[1][1]*A[2][2]-A[1][2]*A[2][1]) / detA;
        inv[0][1] = (A[0][2]*A[2][1]-A[0][1]*A[2][2]) / detA;
        inv[0][2] = (A[0][1]*A[1][2]-A[0][2]*A[1][1]) / detA;
        inv[1][0] = (A[1][2]*A[2][0]-A[1][0]*A[2][2]) / detA;
        inv[1][1] = (A[0][0]*A[2][2]-A[0][2]*A[2][0]) / detA;
        inv[1][2] = (A[0][2]*A[1][0]-A[0][0]*A[1][2]) / detA;
        inv[2][0] = (A[1][0]*A[2][1]-A[1][1]*A[2][0]) / detA;
        inv[2][1] = (A[0][1]*A[2][0]-A[0][0]*A[2][1]) / detA;
        inv[2][2] = (A[0][0]*A[1][1]-A[0][1]*A[1][0]) / detA;

        const beta = [0,0,0];
        for (let i = 0; i < 3; i++) {
          beta[i] = inv[i][0]*b[0] + inv[i][1]*b[1] + inv[i][2]*b[2];
        }
        return { b0: beta[0], b1: beta[1], b2: beta[2] };
      }

      function bootstrapCoefs(data, B) {
        const { x1, x2, y } = data;
        const n = y.length;
        const coefs = [];
        for (let b = 0; b < B; b++) {
          const bx1 = new Array(n);
          const bx2 = new Array(n);
          const by  = new Array(n);
          for (let i = 0; i < n; i++) {
            const j = Math.floor(Math.random() * n);
            bx1[i] = x1[j];
            bx2[i] = x2[j];
            by[i]  = y[j];
          }
          coefs.push(fitOLS(bx1, bx2, by));
        }
        return coefs;
      }

      function jackknifeCoefs(data) {
        const { x1, x2, y } = data;
        const n = y.length;
        const coefs = [];
        for (let leave = 0; leave < n; leave++) {
          const jx1 = [];
          const jx2 = [];
          const jy  = [];
          for (let i = 0; i < n; i++) {
            if (i === leave) continue;
            jx1.push(x1[i]);
            jx2.push(x2[i]);
            jy.push(y[i]);
          }
          coefs.push(fitOLS(jx1, jx2, jy));
        }
        return coefs;
      }

      function summarise(coefArray, key) {
        const vals = coefArray.map(c => c[key]).filter(v => Number.isFinite(v));
        if (!vals.length) return null;
        vals.sort((a,b) => a-b);
        const n = vals.length;
        const mean = vals.reduce((a,b) => a+b, 0) / n;
        const p = q => {
          if (n === 1) return vals[0];
          const idx = (n-1)*q;
          const lo = Math.floor(idx), hi = Math.ceil(idx);
          if (lo === hi) return vals[lo];
          const w = idx - lo;
          return vals[lo]*(1-w) + vals[hi]*w;
        };
        const lo = p(0.025);
        const hi = p(0.975);
        const width = hi - lo;
        const signStable = vals.filter(v => Math.sign(v) === Math.sign(mean) && Math.sign(v) !== 0).length / n;
        return { mean, lo, hi, width, signStable };
      }

      function clearSVG(s) {
        while (s.firstChild) s.removeChild(s.firstChild);
      }

      function drawPlot(single, bootSum, jackSum, trueBeta) {
        clearSVG(svg);
        const width  = svg.clientWidth  || 560;
        const height = svg.clientHeight || 180;
        svg.setAttribute('viewBox', `0 0 ${width} ${height}`);

        const leftPad  = 130;  // room for labels
        const rightPad = 40;

        const midYsingle = 40;
        const midYboot   = 95;
        const midYjack   = 150;

        const values = [trueBeta];
        if (bootSum) { values.push(bootSum.lo, bootSum.hi); }
        if (jackSum) { values.push(jackSum.lo, jackSum.hi); }
        if (single)  { values.push(single.b1); }

        const min  = Math.min.apply(null, values) - 0.5;
        const max  = Math.max.apply(null, values) + 0.5;
        const span = (max - min) || 1;

        const xScale = v =>
          leftPad + ((v - min) / span) * (width - leftPad - rightPad);

        function addLabel(y, text) {
          const t = document.createElementNS('http://www.w3.org/2000/svg','text');
          t.setAttribute('x', leftPad - 10);
          t.setAttribute('y', y + 4);
          t.setAttribute('fill', '#9ca3af');
          t.setAttribute('font-size','11');
          t.setAttribute('text-anchor','end');
          t.textContent = text;
          svg.appendChild(t);
        }
        function drawCI(summary, y, color) {
          if (!summary) return;
          const x1 = xScale(summary.lo);
          const x2 = xScale(summary.hi);

          const line = document.createElementNS('http://www.w3.org/2000/svg','line');
          line.setAttribute('x1', x1);
          line.setAttribute('x2', x2);
          line.setAttribute('y1', y);
          line.setAttribute('y2', y);
          line.setAttribute('stroke', color);
          line.setAttribute('stroke-width', '6');
          line.setAttribute('stroke-linecap', 'round');
          svg.appendChild(line);

          const dot = document.createElementNS('http://www.w3.org/2000/svg','circle');
          dot.setAttribute('cx', xScale(summary.mean));
          dot.setAttribute('cy', y);
          dot.setAttribute('r', 4);
          dot.setAttribute('fill', '#e5e7eb');
          dot.setAttribute('stroke', '#0f172a');
          dot.setAttribute('stroke-width', '1');
          svg.appendChild(dot);
        }

        // Axis
        const axis = document.createElementNS('http://www.w3.org/2000/svg','line');
        axis.setAttribute('x1', leftPad);
        axis.setAttribute('x2', width - rightPad);
        axis.setAttribute('y1', midYboot);
        axis.setAttribute('y2', midYboot);
        axis.setAttribute('stroke', '#4b5563');
        axis.setAttribute('stroke-width', '1');
        svg.appendChild(axis);

        // True beta line across CI region only
        const tbx = xScale(trueBeta);
        const tb  = document.createElementNS('http://www.w3.org/2000/svg','line');
        tb.setAttribute('x1', tbx);
        tb.setAttribute('x2', tbx);
        tb.setAttribute('y1', midYsingle - 10);
        tb.setAttribute('y2', midYjack + 10);
        tb.setAttribute('stroke', '#22c55e');
        tb.setAttribute('stroke-dasharray', '3 3');
        tb.setAttribute('stroke-width', '1');
        tb.setAttribute('opacity', '0.8');
        svg.appendChild(tb);

        // Bands
        drawCI({ mean: single.b1, lo: single.b1, hi: single.b1 }, midYsingle, '#f97316');
        drawCI(bootSum, midYboot, '#38bdf8');
        drawCI(jackSum, midYjack, '#a855f7');

        // Labels
        addLabel(midYsingle, 'Single fit (full sample)');
        addLabel(midYboot,   'Bootstrap (B=80)');
        addLabel(midYjack,   'Jackknife (leave-one-out)');

        // Min / max ticks
        const tMin = document.createElementNS('http://www.w3.org/2000/svg','text');
        tMin.setAttribute('x', leftPad);
        tMin.setAttribute('y', height - 8);
        tMin.setAttribute('fill', '#6b7280');
        tMin.setAttribute('font-size','10');
        tMin.textContent = min.toFixed(1);
        svg.appendChild(tMin);

        const tMax = document.createElementNS('http://www.w3.org/2000/svg','text');
        tMax.setAttribute('x', width - rightPad - 4);
        tMax.setAttribute('y', height - 8);
        tMax.setAttribute('fill', '#6b7280');
        tMax.setAttribute('font-size','10');
        tMax.setAttribute('text-anchor','end');
        tMax.textContent = max.toFixed(1);
        svg.appendChild(tMax);
      }

      function update() {
        const n      = parseInt(nSlider.value, 10);
        const noise  = parseFloat(noiseSlider.value);
        const signal = parseFloat(signalSlider.value);

        nVal.textContent      = n;
        noiseVal.textContent  = noise.toFixed(1);
        signalVal.textContent = signal.toFixed(1);

        const data      = generateData(n, noise, signal);
        const singleFit = fitOLS(data.x1, data.x2, data.y);
        const boot      = bootstrapCoefs(data, 80);
        const jack      = jackknifeCoefs(data);

        const bootB1 = summarise(boot, 'b1');
        const jackB1 = summarise(jack, 'b1');
        const bootB2 = summarise(boot, 'b2');
        const jackB2 = summarise(jack, 'b2');

        drawPlot(singleFit, bootB1, jackB1, signal);

        // ---- Metrics table ----
        metricsTable.innerHTML = `
          <tr>
            <th>Coefficient</th>
            <th>Single fit Œ≤ÃÇ</th>
            <th colspan="2">Bootstrap (CI width, sign stability)</th>
            <th colspan="2">Jackknife (CI width, sign stability)</th>
          </tr>
          <tr>
            <th></th>
            <th></th>
            <th>Width</th>
            <th>Sign stable</th>
            <th>Width</th>
            <th>Sign stable</th>
          </tr>
        `;

        function addRow(label, singleVal, bootSum, jackSum, trueVal) {
          const fmt = v => (v === null || v === undefined || Number.isNaN(v)) ? '‚Äì' : v.toFixed(2);
          const fmtPct = v => (v === null || v === undefined) ? '‚Äì' : Math.round(v*100) + '%';

          const bootWidth  = bootSum ? bootSum.width      : null;
          const jackWidth  = jackSum ? jackSum.width      : null;
          const bootStable = bootSum ? bootSum.signStable : null;
          const jackStable = jackSum ? jackSum.signStable : null;

          const scale = Math.abs(trueVal) > 1e-6 ? Math.abs(trueVal) : 0.5;

          const bootTag = (bootStable !== null && bootWidth !== null)
            ? (bootStable > 0.9 && bootWidth < scale * 0.7 ? 'strong' : 'weak')
            : null;

          const jackTag = (jackStable !== null && jackWidth !== null)
            ? (jackStable > 0.9 && jackWidth < scale * 0.7 ? 'strong' : 'weak')
            : null;

          metricsTable.innerHTML += `
            <tr>
              <td>${label}</td>
              <td>${fmt(singleVal)}</td>
              <td>${fmt(bootWidth)} ${
                bootTag ? `<span class="rsp-tag rsp-tag-${bootTag}">${bootTag === 'strong' ? 'stable' : 'unstable'}</span>` : ''
              }</td>
              <td>${fmtPct(bootStable)}</td>
              <td>${fmt(jackWidth)} ${
                jackTag ? `<span class="rsp-tag rsp-tag-${jackTag}">${jackTag === 'strong' ? 'stable' : 'unstable'}</span>` : ''
              }</td>
              <td>${fmtPct(jackStable)}</td>
            </tr>
          `;
        }

        addRow(
          'Signal feature Œ≤‚ÇÅ (true = ' + signal.toFixed(1) + ')',
          singleFit.b1, bootB1, jackB1, signal
        );
        addRow(
          'Noise feature Œ≤‚ÇÇ (true = 0)',
          singleFit.b2, bootB2, jackB2, 0
        );
      }

      resampleBtn.addEventListener('click', update);

      // just keep sliders updating labels; recompute on button click
      nSlider.addEventListener('input', () => { nVal.textContent = nSlider.value; });
      noiseSlider.addEventListener('input', () => { noiseVal.textContent = parseFloat(noiseSlider.value).toFixed(1); });
      signalSlider.addEventListener('input', () => { signalVal.textContent = parseFloat(signalSlider.value).toFixed(1); });

      // initial draw
      update();
    })();
  </script>
</section>


  
  
  
  
  
  
  
  
  
  
<table>
<thead>
<tr>
  <th>Metric / Method</th>
  <th>Decision Criterion</th>
  <th>Purpose</th>
  <th>Description</th>
  <th>Working Mechanism</th>
  <th>Example</th>
  <th>Limitations</th>
</tr>
</thead>
<tbody>

<tr>
<td><strong>Cross-Validation Mean &amp; Std</strong> (k-fold CV)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  <span class="good">Higher mean score</span> = better average performance.<br><br>

  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-robustness"></div>
    <div class="colorbar-scale">
      <span class="tick-robust-low">high var</span>
      <span class="tick-robust-mid">medium</span>
      <span class="tick-robust-high">low var</span>
    </div>
  </div>

  <span class="good">Low std across folds</span> = stable / robust model.<br><br>
  <span class="poor">High std across folds</span> = performance sensitive to data split.
</td>
<td>Estimate out‚Äëof‚Äësample performance and stability.</td>
<td>Repeatedly train/test on different folds of the data.</td>
<td>Mean = expected performance; std = sensitivity to sample.</td>
<td>0.88 ¬± 0.01 across folds is strong &amp; stable; 0.90 ¬± 0.10 is unstable.</td>
<td>More expensive than simple train/test; must respect temporal / grouped
structure.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Randomly shuffling time‚Äëseries data or grouped data, breaking dependencies.</li>
  <li>Choosing k so large that folds are too small (high variance) or so small that variance estimates are unreliable.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Bias‚ÄìVariance Tradeoff</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-biasvar"></div>
    <div class="colorbar-scale">
      <span class="tick-0">underfit</span>
      <span class="tick-05">balanced</span>
      <span class="tick-10">overfit</span>
    </div>
  </div>

  <span class="poor">High bias (underfit)</span> ‚Üí high error on train &amp; test.<br><br>
  <span class="good">Balanced bias‚Äìvariance</span> ‚Üí low and similar train/test error.<br><br>
  <span class="poor">High variance (overfit)</span> ‚Üí very low train error, high test error.
</td>
<td>Conceptual tool for selecting model complexity.</td>
<td>Simple models: high bias, low variance; complex models: low bias, high
variance.</td>
<td>Analyse learning curves vs model capacity to find sweet spot.</td>
<td>Deep tree that fits training perfectly but fails on test is high‚Äëvariance.</td>
<td>Not a single numeric statistic; patterns can be subtle for deep models.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Assuming that increasing model capacity always improves performance without monitoring overfitting.</li>
  <li>Using training error alone as proxy for generalisation error.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Jackknife Variability</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-robustness"></div>
    <div class="colorbar-scale">
      <span class="tick-robust-low">high var</span>
      <span class="tick-robust-mid">medium</span>
      <span class="tick-robust-high">low var</span>
    </div>
  </div>

  <span class="good">Low jackknife SE</span> ‚Üí estimator stable to leaving out single observations.<br><br>
  <span class="moderate">Moderate SE</span> ‚Üí some sensitivity.<br><br>
  <span class="poor">High SE</span> ‚Üí highly sensitive to specific cases.
</td>
<td>Assess estimator stability and approximate standard errors by
leave‚Äëone‚Äëout recomputation.</td>
<td>Compute estimates on N datasets, each missing one observation; inspect
spread.</td>
<td>Big changes when omitting particular points indicate influence.</td>
<td>If dropping any one observation barely changes a coefficient, model is
stable.</td>
<td>Less flexible than bootstrap for complex estimators; can be noisy for small
N.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Confusing jackknife variability (influence of individual points) with overall sampling variability.</li>
  <li>Applying jackknife to highly non‚Äësmooth estimators where leave‚Äëone‚Äëout changes structure dramatically.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Bootstrap CI Width</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-ciwidth"></div>
    <div class="colorbar-scale">
      <span class="tick-ci-narrow">narrow</span>
      <span class="tick-ci-mid">medium</span>
      <span class="tick-ci-wide">wide</span>
    </div>
  </div>

  <span class="good">Narrow bootstrap CI</span> = precise estimate.<br><br>
  <span class="moderate">Medium width</span> = acceptable uncertainty.<br><br>
  <span class="poor">Very wide or irregular CI</span> = high uncertainty / instability.
</td>
<td>Non‚Äëparametric uncertainty quantification for statistics and model
parameters.</td>
<td>Resample with replacement, recompute estimator, and use empirical
distribution.</td>
<td>Percentile or BCa intervals reflect sampling variability without assuming
normality.</td>
<td>Narrow [4.1, 4.2] CI is very precise; [0, 20] shows extreme uncertainty.</td>
<td>Expensive for large models; assumes sample is representative.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Bootstrapping data with temporal or grouped dependence without respecting structure.</li>
  <li>Using too few bootstrap samples, resulting in noisy interval estimates.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Feature Stability Across Resamples</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-standard"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-05">0.5</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  <span class="poor">&lt; 0.5</span> = feature selected or important in &lt;50% of resamples (unstable).<br><br>
  <span class="moderate">0.5‚Äì0.8</span> = moderate stability.<br><br>
  <span class="good">&gt; 0.8</span> = highly stable across resamples.
</td>
<td>Check whether discovered ‚Äúimportant features‚Äù are robust to sampling
variation.</td>
<td>Repeat feature selection / importance computation over many resamples and
count how often each feature is selected as important.</td>
<td>Helps distinguish real signal from noise‚Äëdriven feature choices.</td>
<td>Stable features (e.g. 95/100 bootstrap samples) are more trustworthy than
features selected rarely.</td>
<td>Correlated predictors may appear unstable individually; requires careful
interpretation.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Judging features purely by single‚Äëfit importance scores without looking at stability.</li>
  <li>Ignoring that correlated feature groups may swap roles across resamples while jointly representing the same signal.</li>
</ul>
</td>
</tr>

</tbody>
</table>
</div>
</details>

<!-- ===================================== -->
<!-- 4. REGRESSION & CORRELATION           -->
<!-- ===================================== -->
<details open>
<summary>Regression &amp; Correlation</summary>
<div class="table-responsive">
<table>
<thead>
<tr>
  <th>Metric</th>
  <th>Decision Criterion</th>
  <th>Purpose</th>
  <th>Description</th>
  <th>Working Mechanism</th>
  <th>Example</th>
  <th>Limitations</th>
</tr>
</thead>
<tbody>

<tr>
<td><strong>R<sup>2</sup> (Coefficient of Determination)</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-r2"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-025">0.25</span>
      <span class="tick-05">0.5</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  <span class="poor">&lt; 0.25</span> = weak fit.<br><br>
  <span class="moderate">0.25‚Äì0.5</span> = moderate fit.<br><br>
  <span class="good">&gt; 0.5</span> = strong fit.
</td>
<td>Proportion of variance in response explained by the model.</td>
<td>
\[
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}.
\]</td>
<td>Compares residual variance to variance around the mean.</td>
<td>R¬≤ = 0.85 ‚Üí model explains 85% of variability in outcome.</td>
<td>Can be inflated by overfitting; does not imply causality or good
out‚Äëof‚Äësample performance.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Equating high R¬≤ with causal explanation rather than predictive association.</li>
  <li>Comparing R¬≤ across models fitted to different datasets or with different outcome variances.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Adjusted R<sup>2</sup></strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-r2"></div>
    <div class="colorbar-scale">
      <span class="tick-0">0</span>
      <span class="tick-025">0.25</span>
      <span class="tick-05">0.5</span>
      <span class="tick-10">1.0</span>
    </div>
  </div>

  <span class="good">Higher adjusted R¬≤</span> = better fit after penalising extra predictors.<br><br>
  <span class="moderate">Small gains</span> = marginal benefit of added predictors.<br><br>
  <span class="poor">Decrease when adding predictors</span> = likely overfitting / noise variables.
</td>
<td>Compare models with different numbers of predictors while penalising
complexity.</td>
<td>Adjusts R¬≤ downward for each extra degree of freedom.</td>
<td>Only increases when added variables meaningfully reduce residual variance.</td>
<td>If R¬≤ rises but adjusted R¬≤ falls when adding variables, they‚Äôre probably
not helpful.</td>
<td>Can‚Äôt compare across different datasets; still doesn‚Äôt guarantee predictive
performance.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using small differences in adjusted R¬≤ as decisive evidence between models.</li>
  <li>Ignoring other diagnostics (residual plots, multicollinearity) when adjusted R¬≤ looks acceptable.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>RMSE</strong> (Root Mean Square Error)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-error"></div>
    <div class="colorbar-scale">
      <span class="tick-err0">0</span>
      <span class="tick-errmid">moderate</span>
      <span class="tick-errhi">large</span>
    </div>
  </div>

  <span class="good">Lower RMSE</span> = better (small typical errors).<br><br>
  <span class="moderate">Similar to target SD</span> = modest improvement over baseline.<br><br>
  <span class="poor">Close to or above target SD/range</span> = weak predictive value.
</td>
<td>Average magnitude of squared prediction errors, back in original units.</td>
<td>
\[
\text{RMSE} = \sqrt{\frac{1}{n}\sum (y_i - \hat{y}_i)^2 }.
\]</td>
<td>Squares errors (emphasising large ones), then square‚Äëroots.</td>
<td>RMSE \$20k on houses with SD \$60k is good; \$55k is poor.</td>
<td>Highly sensitive to outliers; needs baseline to interpret magnitude.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Comparing RMSE across targets with different scales instead of using relative metrics.</li>
  <li>Optimising RMSE when extreme outliers are less important than typical error magnitude (where MAE may be better).</li>
</ul>
</td>
</tr>

<tr>
<td><strong>MAE</strong> (Mean Absolute Error)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-error"></div>
    <div class="colorbar-scale">
      <span class="tick-err0">0</span>
      <span class="tick-errmid">moderate</span>
      <span class="tick-errhi">large</span>
    </div>
  </div>

  <span class="good">Lower MAE</span> = predictions on average close to truth.<br><br>
  <span class="moderate">Moderate fraction of target scale</span> = acceptable.<br><br>
  <span class="poor">Large fraction of target scale</span> = poor accuracy.
</td>
<td>Average absolute prediction error; more robust than RMSE.</td>
<td>
\[
\text{MAE} = \frac{1}{n}\sum |y_i - \hat{y}_i|.
\]</td>
<td>Each error contributes linearly.</td>
<td>MAE of 1.5k on 20k prices ‚âà 7.5% typical error (good).</td>
<td>Doesn‚Äôt strongly penalise rare huge errors.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using MAE when large outliers are mission‚Äëcritical, under‚Äëpenalising them.</li>
  <li>Interpreting MAE without relating it to the typical value or variance of the target.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>MSE / RMSLE</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-error"></div>
    <div class="colorbar-scale">
      <span class="tick-err0">0</span>
      <span class="tick-errmid">moderate</span>
      <span class="tick-errhi">large</span>
    </div>
  </div>

  <span class="good">Lower MSE/RMSLE</span> = smaller errors overall.<br><br>
  <span class="moderate">Intermediate values</span> = moderate performance.<br><br>
  <span class="poor">High values</span> = many large errors.
</td>
<td>Alternative regression loss functions: MSE for squared errors, RMSLE for
relative/log errors.</td>
<td>RMSLE uses log1p transform, emphasising multiplicative errors.</td>
<td>Useful when underestimation of large values is particularly problematic or
targets are highly skewed.</td>
<td>Popular in Kaggle competitions for positive‚Äëonly targets.</td>
<td>RMSLE can‚Äôt straightforwardly handle zeros/negatives; MSE sensitive to
outliers.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Applying RMSLE when the target has zeros/negatives without appropriate transformations.</li>
  <li>Ignoring that MSE/RMSE heavily weight a small number of very large residuals.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Pearson Correlation</strong> (r)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-correlation"></div>
    <div class="colorbar-scale">
      <span class="tick-m1">-1</span>
      <span class="tick-m05">-0.5</span>
      <span class="tick-0c">0</span>
      <span class="tick-05c">0.5</span>
      <span class="tick-1c">1</span>
    </div>
  </div>

  Interpreting |r|:<br><br>
  <span class="poor">&lt; 0.3</span> = weak.<br><br>
  <span class="moderate">0.3‚Äì0.5</span> = moderate.<br><br>
  <span class="good">&gt; 0.7</span> = strong linear association.
</td>
<td>Strength and direction of linear association between two numeric
variables.</td>
<td>Standardised covariance:
\[
r = \frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y}.
\]</td>
<td>Ranges ‚àí1..1; sign gives direction, magnitude gives strength.</td>
<td>Height vs weight often r‚âà0.7‚Äì0.8.</td>
<td>Very sensitive to outliers; misses non‚Äëlinear relationships.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Interpreting correlation as causation or assuming no hidden confounders.</li>
  <li>Quoting r without visualising scatter plots to check linearity and outliers.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Spearman Correlation</strong> (œÅ)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-correlation"></div>
    <div class="colorbar-scale">
      <span class="tick-m1">-1</span>
      <span class="tick-m05">-0.5</span>
      <span class="tick-0c">0</span>
      <span class="tick-05c">0.5</span>
      <span class="tick-1c">1</span>
    </div>
  </div>

  Interpreting |œÅ|:<br><br>
  <span class="poor">&lt; 0.3</span> = weak monotonic association.<br><br>
  <span class="moderate">0.3‚Äì0.5</span> = moderate.<br><br>
  <span class="good">&gt; 0.7</span> = strong monotonic relationship.
</td>
<td>Correlation on ranks; robust to outliers and non‚Äëlinear but monotonic
trends.</td>
<td>Compute ranks of X and Y, then Pearson r on ranks.</td>
<td>Captures relationships where one variable consistently increases/decreases
with the other.</td>
<td>Useful for ordinal data or non‚Äëlinear monotonic relationships.</td>
<td>Near zero for non‚Äëmonotonic relationships (e.g. U‚Äëshaped).<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Assuming Spearman detects arbitrary non‚Äëlinear patterns; it only captures monotonic tendencies.</li>
  <li>Not accounting for many ties in ranked data, which can affect estimates.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Partial Correlation</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-correlation"></div>
    <div class="colorbar-scale">
      <span class="tick-m1">-1</span>
      <span class="tick-m05">-0.5</span>
      <span class="tick-0c">0</span>
      <span class="tick-05c">0.5</span>
      <span class="tick-1c">1</span>
    </div>
  </div>

  Interpreting |r<sub>partial</sub>|:<br><br>
  <span class="poor">‚âà 0</span> = little remaining association beyond controls.<br><br>
  <span class="moderate">‚âà 0.3‚Äì0.5</span> = moderate residual link.<br><br>
  <span class="good">&gt; 0.5</span> = strong association beyond controlled factors.
</td>
<td>Measure association between two variables while controlling for others.</td>
<td>Regress each variable on controls, then correlate residuals.</td>
<td>Helps separate direct from indirect effects.</td>
<td>Correlation between exercise and blood pressure may shrink after
controlling for age.</td>
<td>Only removes linear effects; can be unstable with many correlated
controls.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Interpreting partial correlation as proof of direct causal influence.</li>
  <li>Including too many collinear controls, leading to noisy or unstable estimates.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Regression Coefficients + CI</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-ciwidth"></div>
    <div class="colorbar-scale">
      <span class="tick-ci-narrow">narrow</span>
      <span class="tick-ci-mid">medium</span>
      <span class="tick-ci-wide">wide</span>
    </div>
  </div>

  <span class="good">CI not containing 0</span> ‚Üí coefficient significantly different from 0.<br><br>
  <span class="good">Narrow CI</span> ‚Üí precise effect estimate.<br><br>
  <span class="poor">Wide CI including 0</span> ‚Üí weak or uncertain effect.
</td>
<td>Interpret predictor effects and uncertainty in regression models.</td>
<td>Coefficients describe expected change in response for unit change in
predictor, holding others fixed; CI shows uncertainty.</td>
<td>Based on estimated SEs and t / normal critical values.</td>
<td>‚ÄúEach extra year of experience adds \$2k (95% CI \$1.5k‚Äì\$2.5k)‚Äù is clear and
interpretable.</td>
<td>Interpretation assumes correct model form and no severe multicollinearity.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Interpreting coefficients from poorly specified models (e.g. missing confounders) as causal.</li>
  <li>Ignoring the width of CIs and focusing only on significance.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Durbin‚ÄìWatson Test</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-dw"></div>
    <div class="colorbar-scale">
      <span class="tick-dw0">0</span>
      <span class="tick-dw2">2</span>
      <span class="tick-dw4">4</span>
    </div>
  </div>

  <span class="poor">&lt; 1.5</span> = likely positive autocorrelation (bad for OLS SEs).<br><br>
  <span class="good">1.5‚Äì2.5</span> = little evidence of serious autocorrelation.<br><br>
  <span class="poor">&gt; 2.5</span> = possible negative autocorrelation.
</td>
<td>Detect first‚Äëorder serial correlation in regression residuals.</td>
<td>DW ‚âà 2(1 ‚àí œÅ<sub>1</sub>), where œÅ<sub>1</sub> is lag‚Äë1 autocorrelation.</td>
<td>Values far from 2 suggest residual dependence.</td>
<td>DW = 0.9 suggests strong positive autocorrelation; consider time‚Äëseries
models or GLS.</td>
<td>Primarily detects first‚Äëorder correlation; interpretation uses critical
value tables or approximations.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Applying Durbin‚ÄìWatson to models with lagged dependent variables where its distribution changes.</li>
  <li>Ignoring serial correlation even when DW indicates strong dependence, leading to underestimated SEs.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Breusch‚ÄìPagan Test</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-pvalue-assump"></div>
    <div class="colorbar-scale">
      <span class="tick-p0">0</span>
      <span class="tick-p005">0.05</span>
      <span class="tick-p1">1.0</span>
    </div>
  </div>

  <span class="poor">p &lt; 0.05</span> ‚Üí reject homoscedasticity; heteroscedastic errors (bad for standard OLS SEs).<br><br>
  <span class="good">p ‚â• 0.05</span> ‚Üí no strong evidence of non‚Äëconstant variance.
</td>
<td>Test for heteroscedasticity (non‚Äëconstant variance) in regression.</td>
<td>Regress squared residuals on predictors; statistic ~œá¬≤ under constant
variance.</td>
<td>Significant p suggests need for robust SEs, transforms, or alternate
models.</td>
<td>Often used in linear regression diagnostics.</td>
<td>Power depends on auxiliary regression specification; may miss complex
patterns.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Assuming homoscedasticity solely because the Breusch‚ÄìPagan p‚Äëvalue is slightly above 0.05.</li>
  <li>Using standard OLS SEs despite strong evidence of heteroscedasticity.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Shapiro‚ÄìWilk Normality Test</strong> (on residuals)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-pvalue-assump"></div>
    <div class="colorbar-scale">
      <span class="tick-p0">0</span>
      <span class="tick-p005">0.05</span>
      <span class="tick-p1">1.0</span>
    </div>
  </div>

  Null: residuals are normal.<br><br>
  <span class="poor">p &lt; 0.05</span> ‚Üí reject normality (assumption violation).<br><br>
  <span class="good">p ‚â• 0.05</span> ‚Üí no strong evidence against normality.
</td>
<td>Check normality assumption for regression residuals.</td>
<td>Statistic W measures agreement between ordered residuals and expected
normal order statistics.</td>
<td>Used with residual plots to assess normality assumption for t‚Äëbased
inference.</td>
<td>p = 0.4 suggests residual normality is acceptable; p = 0.001 suggests
heavy tails / skew.</td>
<td>Very powerful for large n (tiny deviations flagged); doesn‚Äôt tell how
residuals deviate.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Overreacting to tiny deviations from normality in large samples where CLT‚Äëbased inference is still robust.</li>
  <li>Relying solely on the test without inspecting Q‚ÄìQ plots.</li>
</ul>
</td>
</tr>

</tbody>
</table>
</div>

<h2>Comparative Table ‚Äì When to Use MAE vs RMSE</h2>
<div class="table-responsive">
  <table>
    <thead>
      <tr>
        <th>Scenario</th>
        <th>Prefer MAE</th>
        <th>Prefer RMSE</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Robustness to outliers</td>
        <td>Yes ‚Äì if occasional extreme errors are not critical.</td>
        <td>No ‚Äì RMSE will be dominated by a few large residuals.</td>
      </tr>
      <tr>
        <td>Penalising large errors heavily</td>
        <td>No ‚Äì treats all deviations linearly.</td>
        <td>Yes ‚Äì squared errors heavily punish large mistakes.</td>
      </tr>
      <tr>
        <td>Interpretability</td>
        <td>‚ÄúOn average, we are off by ‚Ä¶‚Äù is intuitive.</td>
        <td>Less intuitive, but mathematically convenient for optimisation.</td>
      </tr>
      <tr>
        <td>Gradient‚Äëbased optimisation</td>
        <td>Non‚Äëdifferentiable at 0 but workable.</td>
        <td>Smooth and strongly convex; widely used loss.</td>
      </tr>
      <tr>
        <td>Highly skewed targets</td>
        <td>Sometimes combined with median‚Äëbased models.</td>
        <td>May require log‚Äëtransform or RMSLE for stability.</td>
      </tr>
    </tbody>
  </table>
</div>
</details>

<!-- ===================================== -->
<!-- 5. MULTICOLLINEARITY DIAGNOSTICS      -->
<!-- ===================================== -->
<details open>
<summary>Multicollinearity Diagnostics</summary>
<div class="table-responsive">
<table>
<thead>
<tr>
  <th>Metric</th>
  <th>Decision Criterion</th>
  <th>Purpose</th>
  <th>Description</th>
  <th>Working Mechanism</th>
  <th>Example</th>
  <th>Limitations</th>
</tr>
</thead>
<tbody>

<tr>
<td><strong>Variance Inflation Factor (VIF)</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-vif"></div>
    <div class="colorbar-scale">
      <span class="tick-vif1">1</span>
      <span class="tick-vif5">5</span>
      <span class="tick-vif10">10</span>
      <span class="tick-vif20">20+</span>
    </div>
  </div>

  VIF ‚âà 1 ‚Üí no multicollinearity.<br><br>
  <span class="moderate">5‚Äì10</span> = moderate concern.<br><br>
  <span class="poor">&gt; 10</span> = serious multicollinearity.
</td>
<td>Quantify how much variance of a coefficient is inflated by linear
dependence with other predictors.</td>
<td>
\[
\text{VIF}_j = \frac{1}{1-R_j^2}
\]
where R<sub>j</sub>¬≤ from regressing predictor j on others.</td>
<td>Large R<sub>j</sub>¬≤ ‚Üí large VIF ‚Üí unstable coefficient for that predictor.</td>
<td>VIF = 12 suggests coefficient may be poorly estimated and highly
sensitive to small data changes.</td>
<td>Doesn‚Äôt indicate which predictors are collinear with each other; only that
some redundancy exists.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Dropping variables solely because VIF is above a rule‚Äëof‚Äëthumb threshold without considering domain meaning.</li>
  <li>Ignoring that standardising predictors can change VIF interpretation.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Condition Number</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-cond"></div>
    <div class="colorbar-scale">
      <span class="tick-cond1">1</span>
      <span class="tick-cond10">10</span>
      <span class="tick-cond30">30</span>
      <span class="tick-cond60">60+</span>
    </div>
  </div>

  <span class="good">&lt; 10</span> = low multicollinearity.<br><br>
  <span class="moderate">10‚Äì30</span> = moderate.<br><br>
  <span class="poor">&gt; 30</span> = severe (near singular matrix).
</td>
<td>Measure overall multicollinearity of predictor matrix.</td>
<td>Ratio of largest to smallest singular value (or sqrt of eigenvalue
ratio).</td>
<td>Large condition number means X·µÄX is ill‚Äëconditioned; coefficient estimates
can be unstable.</td>
<td>Condition number ‚âà 50 suggests strong collinearity somewhere among
predictors.</td>
<td>Scaling affects value; interpret with standardised predictors. Does not
pinpoint which variables are problematic.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Ignoring collinearity when condition number is large but VIFs seem moderate.</li>
  <li>Comparing condition numbers across models where predictors are scaled differently.</li>
</ul>
</td>
</tr>

</tbody>
</table>
</div>
</details>

<!-- ===================================== -->
<!-- 6. OUTLIER & DISTRIBUTION METRICS     -->
<!-- ===================================== -->
<details open>
<summary>Outlier &amp; Distribution Metrics</summary>
<div class="table-responsive">
<table>
<thead>
<tr>
  <th>Metric/Test</th>
  <th>Decision Criterion</th>
  <th>Purpose</th>
  <th>Description</th>
  <th>Working Mechanism</th>
  <th>Example</th>
  <th>Limitations</th>
</tr>
</thead>
<tbody>

<tr>
<td><strong>Skewness</strong> (Distribution Asymmetry)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-skewness"></div>
    <div class="colorbar-scale">
      <span class="tick-skew-m3">-3</span>
      <span class="tick-skew-m1">-1</span>
      <span class="tick-skew-0">0</span>
      <span class="tick-skew-1">1</span>
      <span class="tick-skew-3">3</span>
    </div>
  </div>

  <span class="good">Between -1 and +1</span> = modest skew (often acceptable).<br><br>
  <span class="moderate">Between -2 and -1 or 1 and 2</span> = moderate skew.<br><br>
  <span class="poor">&lt; -2 or &gt; 2</span> = strong skew; consider transform or robust methods.
</td>
<td>Quantify asymmetry of a distribution (left vs right tail).</td>
<td>Third standardized moment; sign indicates direction of long tail.</td>
<td>Positive skew ‚Üí long right tail; negative ‚Üí long left tail.</td>
<td>Incomes are typically right‚Äëskewed; log‚Äëincomes are closer to symmetric.</td>
<td>Unstable in small samples; easily influenced by a few extreme points.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using skewness alone to justify transformations without visual inspection.</li>
  <li>Interpreting minor non‚Äëzero skewness in large samples as serious model violation.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Kurtosis</strong> (Tailedness)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-kurtosis"></div>
    <div class="colorbar-scale">
      <span class="tick-kurt-m2">-2</span>
      <span class="tick-kurt-0">0</span>
      <span class="tick-kurt-2">2</span>
    </div>
  </div>

  (Using excess kurtosis, normal ‚âà 0.)<br><br>
  <span class="good">‚âà 0</span> = tails similar to normal.<br><br>
  <span class="moderate">Between -2 and -0.5</span> = somewhat light‚Äëtailed.<br><br>
  <span class="poor">&gt; 2</span> = heavy tails / many extreme values.
</td>
<td>Describe how heavy the tails are compared to normal.</td>
<td>Fourth standardized moment minus 3.</td>
<td>High kurtosis indicates variance dominated by rare large deviations.</td>
<td>Financial returns often show high positive kurtosis.</td>
<td>Very sensitive to outliers; hard to interpret without skewness and plots.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Attributing all high kurtosis to ‚Äúfat tails‚Äù instead of checking for data quality or structural breaks.</li>
  <li>Using kurtosis for tiny samples where estimates are extremely noisy.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Shapiro-Wilk Test</strong> (Normality test)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-pvalue-assump"></div>
    <div class="colorbar-scale">
      <span class="tick-p0">0</span>
      <span class="tick-p005">0.05</span>
      <span class="tick-p1">1.0</span>
    </div>
  </div>

  Null: data are normal.<br><br>
  <span class="poor">p &lt; 0.05</span> ‚Üí reject normality.<br><br>
  <span class="good">p ‚â• 0.05</span> ‚Üí no strong evidence against normality.
</td>
<td>Formal test of normality for small‚Äìmoderate samples.</td>
<td>Statistic W measures agreement between ordered data and expected normal
quantiles.</td>
<td>p-value derived from W; small p indicates deviation from normality.</td>
<td>p=0.08 ‚Üí normality acceptable; p=0.001 ‚Üí strong deviation.</td>
<td>Very sensitive with large n; use with plots and domain context.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Automatically transforming data because p &lt; 0.05 even when deviations are minor and models are robust.</li>
  <li>Using the test on discrete or heavily censored data where normality is impossible.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Z-score Outlier Detection</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-zscore"></div>
    <div class="colorbar-scale">
      <span class="tick-z-m3">-3</span>
      <span class="tick-z-m2">-2</span>
      <span class="tick-z-0">0</span>
      <span class="tick-z-2">2</span>
      <span class="tick-z-3">3</span>
    </div>
  </div>

  Under normality:<br><br>
  <span class="good">|z| &lt; 2</span> = typical.<br><br>
  <span class="moderate">2 ‚â§ |z| ‚â§ 3</span> = borderline outlier.<br><br>
  <span class="poor">|z| &gt; 3</span> = potential outlier.
</td>
<td>Flag univariate outliers relative to mean and SD.</td>
<td>
\[
z = \frac{x - \mu}{\sigma}.
\]</td>
<td>Extremely large |z| values are unlikely under a normal model.</td>
<td>z = 5 is extremely unusual (probability &lt; 10‚Åª‚Å∂ under normal).</td>
<td>Assumes normality; heavy‚Äëtailed data produce many |z|>3 that are not
truly abnormal. Mean/SD can be distorted by the outliers.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using z‚Äëscore thresholds on clearly non‚Äënormal data (e.g. Pareto‚Äëlike heavy tails).</li>
  <li>Removing points based only on z‚Äëscores without investigating data quality or domain context.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>IQR Method</strong> (Tukey‚Äôs Fences)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-iqr"></div>
    <div class="colorbar-scale">
      <span class="tick-iqr-lf">LF</span>
      <span class="tick-iqr-q1">Q1</span>
      <span class="tick-iqr-med">Med</span>
      <span class="tick-iqr-q3">Q3</span>
      <span class="tick-iqr-uf">UF</span>
    </div>
  </div>

  <span class="good">Within [Q1‚àí1.5¬∑IQR, Q3+1.5¬∑IQR]</span> = typical range.<br><br>
  <span class="moderate">Outside 1.5¬∑IQR but within 3¬∑IQR</span> = moderate outlier.<br><br>
  <span class="poor">Beyond 3¬∑IQR fences</span> = extreme outlier.
</td>
<td>Non‚Äëparametric, robust rule of thumb for univariate outliers.</td>
<td>Uses quartiles and interquartile range (IQR = Q3‚ÄìQ1).</td>
<td>Points outside whiskers in a boxplot correspond to Tukey outliers.</td>
<td>Common default rule in statistical software boxplots.</td>
<td>Skewed distributions may produce many flagged points; rule is heuristic
and dimension‚Äëwise only.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Applying standard 1.5√óIQR rule to strongly skewed data without adjustment.</li>
  <li>Dropping all outliers automatically instead of investigating their cause.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Robust Z-score (MAD)</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-zscore"></div>
    <div class="colorbar-scale">
      <span class="tick-z-m3">-3</span>
      <span class="tick-z-m2">-2</span>
      <span class="tick-z-0">0</span>
      <span class="tick-z-2">2</span>
      <span class="tick-z-3">3</span>
    </div>
  </div>

  For robust z<sub>MAD</sub>:<br><br>
  <span class="good">|z<sub>MAD</sub>| &lt; 2.5</span> = typical.<br><br>
  <span class="moderate">2.5‚Äì3.5</span> = borderline.<br><br>
  <span class="poor">&gt; 3.5</span> = strong outlier candidate.
</td>
<td>Detect outliers robustly using median and median absolute deviation.</td>
<td>Robust Z = (x ‚àí median) / (1.4826 ¬∑ MAD), where MAD is median(|x ‚àí
median|).</td>
<td>More stable than classical z‚Äëscore in presence of outliers.</td>
<td>Good for heavy‚Äëtailed or skewed distributions where mean/SD are distorted.</td>
<td>Still assumes a roughly unimodal distribution; threshold choices are
heuristic.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using robust z‚Äëscores but still computing MAD on a mixture of very different populations.</li>
  <li>Believing robust methods remove the need for visual inspection or domain knowledge.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Cook‚Äôs Distance</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-cookd"></div>
    <div class="colorbar-scale">
      <span class="tick-cook-0">0</span>
      <span class="tick-cook-05">0.5</span>
      <span class="tick-cook-1">1</span>
      <span class="tick-cook-2">2+</span>
    </div>
  </div>

  <span class="good">&lt; 0.5</span> = typically low influence.<br><br>
  <span class="moderate">0.5‚Äì1</span> = potentially influential, inspect.<br><br>
  <span class="poor">&gt; 1</span> (especially much &gt;1) = highly influential point.
</td>
<td>Measure influence of each observation on regression fit.</td>
<td>Combines leverage and residual size to approximate change in all fitted
values when a point is removed.</td>
<td>Large Cook‚Äôs D means the observation strongly affects estimates.</td>
<td>One point with D=1.5 while others &lt; 0.1 indicates a dominating data
point.</td>
<td>Thresholds are rules of thumb; influential points may be valid data, not
necessarily errors.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Automatically deleting points with Cook‚Äôs D above a threshold without checking if they are legitimate.</li>
  <li>Ignoring the leverage‚Äìresidual decomposition that explains why points are influential.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Mahalanobis Distance</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-mahal"></div>
    <div class="colorbar-scale">
      <span class="tick-mah-0">0</span>
      <span class="tick-mah-975">œá¬≤<sub>p,0.975</sub></span>
      <span class="tick-mah-99">œá¬≤<sub>p,0.99</sub></span>
      <span class="tick-mah-ext">extreme</span>
    </div>
  </div>

  For p dimensions:<br><br>
  <span class="good">MD¬≤ ‚â§ œá¬≤<sub>p,0.975</sub></span> = inside main cloud.<br><br>
  <span class="moderate">Between œá¬≤<sub>p,0.975</sub> and œá¬≤<sub>p,0.99</sub></span> = potential multivariate outlier.<br><br>
  <span class="poor">MD¬≤ &gt; œá¬≤<sub>p,0.99</sub></span> = strong multivariate outlier.
</td>
<td>Detect multivariate outliers accounting for correlations between
variables.</td>
<td>
\[
MD(x) = \sqrt{(x-\mu)^\top \Sigma^{-1} (x-\mu)}.
\]
Squaring MD gives a œá¬≤ statistic under multivariate normality.</td>
<td>Points with large MD¬≤ lie far from the multivariate mean in whitened
space.</td>
<td>Useful for anomaly detection in multi‚Äëfeature settings.</td>
<td>Requires good estimates of Œº and Œ£; classical covariance is itself
distorted by outliers; robust covariance estimators may be needed.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Computing Mahalanobis distance with non‚Äëinvertible or poorly conditioned covariance matrices.</li>
  <li>Applying œá¬≤ cutoffs to data that are far from multivariate normal.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Kolmogorov‚ÄìSmirnov Test</strong> (KS)</td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-pvalue-assump"></div>
    <div class="colorbar-scale">
      <span class="tick-p0">0</span>
      <span class="tick-p005">0.05</span>
      <span class="tick-p1">1.0</span>
    </div>
  </div>

  Two‚Äësample KS for distribution shift:<br><br>
  <span class="poor">p &lt; 0.05</span> ‚Üí distributions differ significantly (potential shift / mismatch).<br><br>
  <span class="good">p ‚â• 0.05</span> ‚Üí no strong evidence of difference.<br><br>
  Larger D (0‚Äì1) = stronger discrepancy.
</td>
<td>Compare empirical distributions (e.g. train vs production) or sample
vs theoretical distribution.</td>
<td>KS statistic D = sup |F<sub>1</sub>(x) ‚àí F<sub>2</sub>(x)| over x.</td>
<td>p‚Äëvalue derived from D; sensitive to location and shape differences.</td>
<td>Useful in monitoring feature distribution drift over time.</td>
<td>More sensitive near median than in tails; assumes continuous data and
independent samples.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using KS on discrete or heavily binned data without an appropriate variant.</li>
  <li>Interpreting a non‚Äësignificant KS as proof that distributions are identical (rather than ‚Äúno strong evidence of difference‚Äù).</li>
</ul>
</td>
</tr>

</tbody>
</table>
</div>
</details>







<!-- =========================================
     Influence & Robustness Lab (Interactive)
     Place this AFTER "Outlier & Distribution Metrics"
     ========================================== -->
<section id="influence-robustness-lab">
  <style>
    /* Shared lab layout / theming */
    #influence-robustness-lab {
      margin-top: 2rem;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
    }

    #influence-robustness-lab h3 {
      margin: 0 0 0.4rem;
      font-size: 1.05rem;
    }

    #influence-robustness-lab h4 {
      margin: 0.4rem 0 0.25rem;
      font-size: 0.95rem;
    }

    #influence-robustness-lab p {
      margin: 0 0 0.4rem;
      font-size: 0.85rem;
      line-height: 1.4;
    }

    #influence-robustness-lab details.lab-block {
      border-radius: 0.5rem;
      border: 1px solid rgba(148, 163, 184, 0.4);
      margin-top: 0.75rem;
      overflow: hidden;
    }

    #influence-robustness-lab summary.lab-summary {
      padding: 0.75rem 1rem;
      cursor: pointer;
      font-weight: 600;
      background: rgba(15, 23, 42, 0.96);
      color: #e5e7eb;
      list-style: none;
    }

    #influence-robustness-lab summary.lab-summary::-webkit-details-marker {
      display: none;
    }

    #influence-robustness-lab summary.lab-summary::before {
      content: "‚ñ∏ ";
      margin-right: 0.25rem;
      display: inline-block;
      transition: transform 0.15s ease;
    }

    #influence-robustness-lab details[open] > summary.lab-summary::before {
      transform: rotate(90deg);
    }

    body:not(.dark) #influence-robustness-lab summary.lab-summary {
      background: #f3f4f6;
      color: #111827;
      border-bottom: 1px solid rgba(148, 163, 184, 0.4);
    }

    #influence-robustness-lab .lab-inner {
      padding: 0.9rem 1rem 1.1rem;
      background: rgba(15, 23, 42, 0.98);
    }

    body:not(.dark) #influence-robustness-lab .lab-inner {
      background: #ffffff;
    }

    .ir-note {
      font-size: 0.8rem;
      color: #9ca3af;
    }

    body:not(.dark) .ir-note {
      color: #4b5563;
    }

    .ir-layout {
      display: grid;
      grid-template-columns: minmax(260px, 420px) minmax(320px, 520px);
      gap: 1rem;
    }

    @media (max-width: 980px) {
      .ir-layout {
        grid-template-columns: 1fr;
      }
    }

    .ir-card {
      border-radius: 0.5rem;
      padding: 0.75rem 0.9rem;
      background: #020617;
      border: 1px solid rgba(148, 163, 184, 0.35);
    }

    body:not(.dark) .ir-card {
      background: #f9fafb;
      border-color: #e5e7eb;
    }

    .ir-controls-row {
      margin-bottom: 0.5rem;
    }

    .ir-slider-label {
      font-size: 0.8rem;
      margin-bottom: 0.15rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .ir-slider-label span.ir-slider-value {
      font-variant-numeric: tabular-nums;
      font-weight: 600;
      margin-left: 0.35rem;
      color: #e5e7eb;
    }

    body:not(.dark) .ir-slider-label span.ir-slider-value {
      color: #111827;
    }

    .ir-controls-row input[type="range"] {
      width: 100%;
    }

    .ir-btn-row {
      margin-top: 0.4rem;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      flex-wrap: wrap;
    }

    .ir-btn-row button {
      border-radius: 999px;
      border: none;
      padding: 0.3rem 0.8rem;
      font-size: 0.8rem;
      cursor: pointer;
      background: #0ea5e9;
      color: white;
      font-weight: 500;
    }

    .ir-btn-row button:hover {
      background: #0284c7;
    }

    .ir-legend {
      font-size: 0.78rem;
      margin-top: 0.35rem;
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      align-items: center;
    }

    .ir-legend span.swatch {
      display: inline-block;
      width: 10px;
      height: 10px;
      border-radius: 999px;
      margin-right: 0.25rem;
    }

    .ir-metric-table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 0.5rem;
      font-size: 0.8rem;
    }

    .ir-metric-table th,
    .ir-metric-table td {
      padding: 0.25rem 0.4rem;
      border-bottom: 1px solid rgba(148, 163, 184, 0.3);
      text-align: right;
      font-variant-numeric: tabular-nums;
    }

    .ir-metric-table th:first-child,
    .ir-metric-table td:first-child {
      text-align: left;
    }

    .ir-metric-table thead th {
      font-weight: 600;
      color: #e5e7eb;
    }

    body:not(.dark) .ir-metric-table thead th {
      color: #111827;
    }

    .ir-tag {
      display: inline-block;
      padding: 0.05rem 0.45rem;
      border-radius: 999px;
      border: 1px solid;
      font-size: 0.7rem;
      white-space: nowrap;
    }

    .ir-tag.bad {
      border-color: #f97316;
      color: #f97316;
      background: rgba(248, 113, 113, 0.05);
    }

    .ir-tag.good {
      border-color: #22c55e;
      color: #22c55e;
      background: rgba(34, 197, 94, 0.06);
    }

    .ir-tag.neutral {
      border-color: #6b7280;
      color: #e5e7eb;
      background: rgba(107, 114, 128, 0.08);
    }

    body:not(.dark) .ir-tag.neutral {
      color: #111827;
    }

    /* SVG plots */
    .ir-plot {
      width: 100%;
      height: 260px;
    }

    .ir-plot svg {
      width: 100%;
      height: 100%;
      overflow: visible;
    }

    .ir-explain p,
    .ir-explain ul {
      font-size: 0.8rem;
      line-height: 1.4;
      color: #9ca3af;
    }

    body:not(.dark) .ir-explain p,
    body:not(.dark) .ir-explain ul {
      color: #4b5563;
    }

    .ir-explain ul {
      padding-left: 1.1rem;
      margin-top: 0.2rem;
      margin-bottom: 0.3rem;
    }

    .ir-grid-2 {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 0.75rem;
    }

    .ir-kpi {
      font-size: 0.85rem;
      margin-bottom: 0.25rem;
    }

    .ir-kpi span.val {
      font-variant-numeric: tabular-nums;
      font-weight: 600;
      margin-left: 0.3rem;
    }
  </style>

  <!-- Main wrapper collapsible for the whole lab (optional; you can remove if you want it always open) -->
  <details class="lab-block" open>
    <summary class="lab-summary">Influence & Robustness Lab (Interactive Playgrounds) <p class="ir-note">
      &nbsp;&nbsp;&nbsp;&nbsp;These playgrounds show how single points, noise and multiple comparisons can quietly break your models,
    even when headline metrics still look good.
  </p></summary>
    <div class="lab-inner">

      <!-- =========================
           Playground 1: Outlier Impact 2.0
           ========================== -->
      <details class="lab-block" open>
        <summary class="lab-summary">Outlier Impact 2.0 ‚Äì How one point can twist a regression line</summary>
        <div class="lab-inner">
          <div class="ir-layout">
            <!-- LEFT: controls + explanation -->
            <div>
              <div class="ir-card">
                <h4>Controls</h4>

                <div class="ir-controls-row">
                  <div class="ir-slider-label">
                    <span>Sample size (base points)</span>
                    <span class="ir-slider-value" id="oi-n-val">30</span>
                  </div>
                  <input id="oi-n" type="range" min="10" max="80" step="5" value="30" />
                </div>

                <div class="ir-controls-row">
                  <div class="ir-slider-label">
                    <span>Noise level (œÉ)</span>
                    <span class="ir-slider-value" id="oi-noise-val">1.0</span>
                  </div>
                  <input id="oi-noise" type="range" min="0" max="2.5" step="0.1" value="1.0" />
                </div>

                <div class="ir-controls-row">
                  <div class="ir-slider-label">
                    <span>Outlier height</span>
                    <span class="ir-slider-value" id="oi-outlier-y-val">6.0</span>
                  </div>
                  <input id="oi-outlier-y" type="range" min="-6" max="8" step="0.2" value="6" />
                </div>

                <div class="ir-btn-row">
                  <button type="button" id="oi-resample-btn">Regenerate base sample</button>
                  <span class="ir-note">
                    True model: <em>y = 1 + 1.2¬∑x</em>. Outlier shares the same x-range, but you can move it
                    far up or down.
                  </span>
                </div>
              </div>

              <div class="ir-card ir-explain">
                <h4>How to read this playground</h4>
                <p>
                  This playground shows how a <strong>single outlier</strong> can dramatically change an
                  ordinary least-squares regression line.
                </p>
                <p><strong>Purpose</strong></p>
                <p>
                  We simulate a simple linear model and compare two fits: one
                  <strong>without</strong> the outlier (baseline) and one <strong>with</strong> the outlier.
                  If a single point can flip the slope or change it a lot, your model is fragile.
                </p>
                <p><strong>How to use the controls</strong></p>
                <ul>
                  <li><strong>Sample size</strong> ‚Äì more points ‚üπ harder to twist the line.</li>
                  <li><strong>Noise level</strong> ‚Äì more noise hides the clean relationship.</li>
                  <li><strong>Outlier height</strong> ‚Äì drag far up/down and watch the orange line tilt.</li>
                  <li><strong>Regenerate</strong> ‚Äì new random base cloud with same settings.</li>
                </ul>
                <p><strong>Interpretation</strong></p>
                <ul>
                  <li>Blue dots: base data. Red dot: outlier. Blue line: fit without outlier.</li>
                  <li>Orange line: fit including outlier.</li>
                  <li>
                    In the table, <strong>Œî slope</strong> and the tag
                    <span class="ir-tag bad">fragile</span>/<span class="ir-tag good">stable</span> tell you
                    how influential the point is.
                  </li>
                </ul>
              </div>
            </div>

            <!-- RIGHT: plot + table -->
            <div class="ir-card">
              <div class="ir-plot">
                <svg id="oi-svg"></svg>
              </div>
              <div class="ir-legend">
                <span><span class="swatch" style="background:#38bdf8;"></span> Base points</span>
                <span><span class="swatch" style="background:#ef4444;"></span> Outlier</span>
                <span><span class="swatch" style="background:#0ea5e9;"></span> Fit without outlier</span>
                <span><span class="swatch" style="background:#f97316;"></span> Fit with outlier</span>
              </div>

              <table class="ir-metric-table" id="oi-table">
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>Slope</th>
                    <th>Intercept</th>
                    <th>R¬≤</th>
                    <th>Œî slope</th>
                    <th>Stability</th>
                  </tr>
                </thead>
                <tbody></tbody>
              </table>

              <p class="ir-note" style="margin-top:0.35rem;">
                If one point can flip the conclusion, you don‚Äôt have a stable finding ‚Äì you have an
                <strong>anecdote</strong> dressed up as a model.
              </p>
            </div>
          </div>
        </div>
      </details>

      <!-- =========================
           Playground 2: Cook's Distance & Leverage
           ========================== -->
      <details class="lab-block">
        <summary class="lab-summary">Cook‚Äôs Distance & Leverage ‚Äì Influence of a single high-leverage point</summary>
        <div class="lab-inner">
          <div class="ir-layout">
            <div>
              <div class="ir-card">
                <h4>Controls</h4>
                <div class="ir-controls-row">
                  <div class="ir-slider-label">
                    <span>Leverage (x position)</span>
                    <span class="ir-slider-value" id="cd-x-val">2.5</span>
                  </div>
                  <input id="cd-x" type="range" min="0" max="4" step="0.1" value="2.5" />
                </div>

                <div class="ir-controls-row">
                  <div class="ir-slider-label">
                    <span>Residual size (vertical offset)</span>
                    <span class="ir-slider-value" id="cd-resid-val">2.0</span>
                  </div>
                  <input id="cd-resid" type="range" min="0" max="4" step="0.1" value="2.0" />
                </div>

                <div class="ir-btn-row">
                  <button type="button" id="cd-resample-btn">Regenerate base sample</button>
                  <span class="ir-note">
                    Base model: same <em>y = 1 + 1.2¬∑x</em> with noise. The purple point is the
                    high-leverage candidate.
                  </span>
                </div>
              </div>

              <div class="ir-card ir-explain">
                <h4>How to read this playground</h4>
                <p>
                  Cook‚Äôs Distance combines two ideas:
                  <strong>leverage</strong> (how unusual x is) and <strong>residual</strong> (how badly the
                  point is fit). A point with both high leverage and large residual is highly influential.
                </p>
                <p><strong>What you see</strong></p>
                <ul>
                  <li>Blue dots: regular data used to fit the baseline regression line.</li>
                  <li>Purple dot: candidate point at the chosen x-position and residual.</li>
                  <li>
                    The table shows its Cook‚Äôs Distance and a qualitative flag
                    (<span class="ir-tag good">OK</span> vs
                    <span class="ir-tag bad">influential</span>).
                  </li>
                </ul>
                <p><strong>Heuristics</strong></p>
                <ul>
                  <li>Rough rule: Cook‚Äôs D &gt; 1 is clearly influential; D &gt; 0.5 is worth attention.</li>
                  <li>
                    High leverage with tiny residual can still be dangerous if a future small mistake there
                    would flip the slope.
                  </li>
                </ul>
              </div>
            </div>

            <div class="ir-card">
              <div class="ir-plot">
                <svg id="cd-svg"></svg>
              </div>
              <div class="ir-legend">
                <span><span class="swatch" style="background:#38bdf8;"></span> Base points</span>
                <span><span class="swatch" style="background:#a855f7;"></span> Candidate point</span>
                <span><span class="swatch" style="background:#0ea5e9;"></span> Baseline fit</span>
                <span><span class="swatch" style="background:#f97316;"></span> Fit with candidate</span>
              </div>

              <table class="ir-metric-table">
                <thead>
                  <tr>
                    <th>Quantity</th>
                    <th>Value</th>
                    <th>Interpretation</th>
                  </tr>
                </thead>
                <tbody id="cd-table-body">
                  <!-- Filled by JS -->
                </tbody>
              </table>

              <p class="ir-note" style="margin-top:0.35rem;">
                Use this playground to feel that leverage (far-out x) without residual is mild, residual
                without leverage is local ‚Äì but the combination gives large Cook‚Äôs Distance.
              </p>
            </div>
          </div>
        </div>
      </details>

      <!-- =========================
           Playground 3: Noise Injection
           ========================== -->
      <details class="lab-block">
        <summary class="lab-summary">Noise Injection Playground ‚Äì How noise quietly kills stability</summary>
        <div class="lab-inner">
          <div class="ir-layout">
            <div class="ir-card">
              <h4>Noise controls</h4>

              <div class="ir-controls-row">
                <div class="ir-slider-label">
                  <span>Gaussian feature noise</span>
                  <span class="ir-slider-value" id="ni-gauss-val">0.3</span>
                </div>
                <input id="ni-gauss" type="range" min="0" max="1" step="0.05" value="0.3" />
              </div>

              <div class="ir-controls-row">
                <div class="ir-slider-label">
                  <span>Random label flips</span>
                  <span class="ir-slider-value" id="ni-label-val">0.05</span>
                </div>
                <input id="ni-label" type="range" min="0" max="0.4" step="0.01" value="0.05" />
              </div>

              <div class="ir-controls-row">
                <div class="ir-slider-label">
                  <span>Feature noise (jitter)</span>
                  <span class="ir-slider-value" id="ni-feature-val">0.2</span>
                </div>
                <input id="ni-feature" type="range" min="0" max="1" step="0.05" value="0.2" />
              </div>

              <div class="ir-controls-row">
                <div class="ir-slider-label">
                  <span># Irrelevant features</span>
                  <span class="ir-slider-value" id="ni-irrelevant-val">5</span>
                </div>
                <input id="ni-irrelevant" type="range" min="0" max="30" step="1" value="5" />
              </div>

              <p class="ir-note">
                This is a pedagogical model: we don‚Äôt fit an actual classifier, but apply a simple response
                surface that mimics how noise hurts cross-validated performance.
              </p>
            </div>

            <div class="ir-card">
              <h4>Effect on performance & stability</h4>
              <div class="ir-grid-2">
                <div>
                  <div class="ir-kpi">
                    Baseline accuracy (clean data)
                    <span class="val" id="ni-acc-base">0.90</span>
                  </div>
                  <div class="ir-kpi">
                    Expected train accuracy
                    <span class="val" id="ni-acc-train">0.90</span>
                  </div>
                  <div class="ir-kpi">
                    Expected CV accuracy
                    <span class="val" id="ni-acc-cv">0.86</span>
                  </div>
                </div>
                <div>
                  <div class="ir-kpi">
                    Variance of CV scores
                    <span class="val" id="ni-var">0.01</span>
                  </div>
                  <div class="ir-kpi">
                    Stability index (0‚Äì1)
                    <span class="val" id="ni-stab">0.80</span>
                  </div>
                  <div class="ir-kpi">
                    Verdict
                    <span id="ni-verdict" class="ir-tag neutral">moderate robustness</span>
                  </div>
                </div>
              </div>

              <div class="ir-explain" style="margin-top:0.6rem;">
                <p><strong>How to read this playground</strong></p>
                <ul>
                  <li>
                    Increase noise and watch <strong>train accuracy stay high</strong> while
                    <strong>CV accuracy drops</strong> and variance inflates.
                  </li>
                  <li>
                    Many irrelevant features increase overfitting pressure even if the core signal is
                    unchanged.
                  </li>
                  <li>
                    The stability index is a compact score combining CV accuracy and variability: low values
                    mean your model is too dependent on random quirks.
                  </li>
                </ul>
                <p class="ir-note">
                  Moral: always think in terms of ‚Äúsignal vs noise‚Äù. Robust models maintain high CV accuracy
                  and low instability even when noise rises.

                </p>
              </div>
            </div>
          </div>
        </div>
      </details>











<!-- =======================
     Bonferroni & VIF Explainer
======================== -->
<section id="bonferroni-vif-explainer" class="lab-explainer">
  <h3>Bonferroni & VIF ‚Äì Why many tests and correlated features can fool you</h3>

  <p>
    In real projects we almost never test just one thing. We try many features, model variants,
    time points, segments, and outcomes. Every extra test is another chance to see a
    ‚Äúsignificant‚Äù result that is actually just <strong>noise</strong>.
  </p>

  <div class="lab-explainer-grid">
    <!-- Left: Multiple testing / Bonferroni -->
    <div class="lab-column">
      <h4>1. Multiple testing &amp; Bonferroni</h4>

      <p>
        If you test one hypothesis at <code>Œ± = 0.05</code>, there is a 5% chance of a
        <strong>false positive</strong> (a Type I error). If you test many hypotheses at the
        same threshold, the chance that <em>at least one</em> of them is a false positive
        grows very quickly.
      </p>

      <ul>
        <li>1 test at Œ± = 0.05 ‚Üí about <strong>5%</strong> chance of a false positive.</li>
        <li>100 independent tests at Œ± = 0.05 ‚Üí about <strong>99%</strong> chance that at least one is ‚Äúsignificant‚Äù just by luck.</li>
      </ul>

      <h5>What is family-wise error (FWE / FWER)?</h5>
      <p>
        <strong>Family-wise error (FWE)</strong>, often called the
        <strong>family-wise error rate (FWER)</strong>, is the probability of making at least
        one Type&nbsp;I error (false positive) when performing a ‚Äúfamily‚Äù of statistical tests.
        When you run many tests, this probability increases. FWE / FWER is used to
        <strong>quantify and control</strong> this risk, usually by adjusting the significance
        level or using corrections such as <strong>Bonferroni</strong> or <strong>Holm</strong>.
      </p>

      <h5>What Bonferroni does</h5>
      <p>
        Bonferroni is a <strong>conservative safety brake</strong>:
      </p>
      <p class="lab-callout">
        New per-test Œ± = original Œ± √∑ number of tests.
      </p>
      <p>
        Example: with Œ± = 0.05 and 100 tests, the Bonferroni-corrected threshold becomes
        0.05 √∑ 100 = <strong>0.0005</strong> for each test. This keeps the FWER near 5%, but
        makes it harder to detect real effects.
      </p>

      <ul>
        <li><strong>Pros:</strong> very safe; strong control of FWER (false discoveries are rare).</li>
        <li><strong>Cons:</strong> conservative; with many tests it can hide real signals.</li>
      </ul>

      <p class="lab-note">
        The playground shows how FWER explodes with many tests when you <em>don‚Äôt</em>
        correct, and how Bonferroni pulls it back under control.
      </p>
    </div>

    <!-- Right: VIF / collinearity -->
    <div class="lab-column">
      <h4>2. VIF ‚Äì When features tell the same story</h4>

      <p>
        In the right-hand panel, we are not testing many hypotheses, but we are using many
        <strong>correlated features</strong> in a regression. VIF explains how this hurts the
        stability of your coefficients.
      </p>

      <p>
        Intuition:
      </p>
      <ul>
        <li>The slider <strong>"Correlation between similar features"</strong> (œÅ) says <em>how strongly</em> a group of features move together.</li>
        <li>The slider <strong>"# of similarly correlated features"</strong> (k) says <em>how many</em> features sit in that correlated pack.</li>
      </ul>

      <p>
        VIF is defined as <code>VIF = 1 / (1 ‚àí R¬≤)</code> when regressing one feature on the
        others. It tells you how much the <strong>variance of a coefficient</strong> is
        inflated because of collinearity.
      </p>

      <ul>
        <li><strong>VIF ‚âà 1</strong> ‚Äì almost no collinearity.</li>
        <li><strong>VIF 5‚Äì10</strong> ‚Äì coefficients are unstable and hard to interpret.</li>
        <li><strong>VIF &gt; 10</strong> ‚Äì strong collinearity; rethink features or model design.</li>
      </ul>

      <p class="lab-note">
        If œÅ and k are high, the model cannot cleanly separate which feature carries the
        signal. Coefficients may swing wildly, flip sign, or become impossible to interpret,
        even though the underlying relationship hasn‚Äôt changed.
      </p>
    </div>
  </div>

  <hr class="lab-divider"/>

  <h4>3. Big picture</h4>
  <p>
    Both panels demonstrate the same core idea in different ways:
  </p>
  <ul>
    <li><strong>Multiple testing</strong> ‚Äì too many chances to ‚Äúwin‚Äù by noise ‚Üí false discoveries.</li>
    <li><strong>Collinearity</strong> ‚Äì too many overlapping features ‚Üí unstable estimates.</li>
  </ul>
  <p class="lab-summary">
    In short: <strong>noise + many decisions = unreliable statistics</strong>.  
    The corrections and diagnostics you see here (Bonferroni, FWER, VIF) are tools to keep
    that under control.
  </p>
</section>

<style>
  /* --- Bonferroni & VIF explainer styling --- */
  .lab-explainer {
    margin: 1.25rem 0 0.5rem;
    padding: 1.25rem 1.5rem;
    border-radius: 10px;
    border: 1px solid rgba(148, 163, 184, 0.5); /* slate-400 */
    background: rgba(15, 23, 42, 0.04);         /* subtle on light, barely visible on dark */
    font-size: 0.9rem;
    line-height: 1.5;
  }

  .lab-explainer h3 {
    margin-top: 0;
    margin-bottom: 0.75rem;
    font-size: 1.05rem;
    font-weight: 700;
  }

  .lab-explainer h4 {
    margin-top: 0.5rem;
    margin-bottom: 0.5rem;
    font-size: 0.98rem;
    font-weight: 600;
  }

  .lab-explainer h5 {
    margin-top: 0.6rem;
    margin-bottom: 0.3rem;
    font-size: 0.9rem;
    font-weight: 600;
  }

  .lab-explainer p {
    margin: 0.3rem 0 0.5rem;
  }

  .lab-explainer code {
    font-size: 0.85em;
    padding: 0.05em 0.25em;
    border-radius: 4px;
    background: rgba(148, 163, 184, 0.15);
  }

  .lab-explainer-grid {
    display: grid;
    grid-template-columns: minmax(0, 1fr);
    gap: 1rem;
    margin-top: 0.5rem;
  }

  .lab-column ul {
    margin: 0.3rem 0 0.5rem 1.2rem;
    padding-left: 0;
  }

  .lab-callout {
    padding: 0.4rem 0.6rem;
    border-radius: 6px;
    background: rgba(59, 130, 246, 0.08); /* blue-ish */
    border: 1px solid rgba(59, 130, 246, 0.35);
    font-size: 0.85rem;
  }

  .lab-note {
    font-size: 0.85rem;
    opacity: 0.9;
  }

  .lab-divider {
    margin: 0.85rem 0 0.75rem;
    border: 0;
    border-top: 1px dashed rgba(148, 163, 184, 0.6);
  }

  .lab-summary {
    font-weight: 500;
  }

  /* Wider layout on large screens */
  @media (min-width: 1100px) {
    .lab-explainer-grid {
      grid-template-columns: minmax(0, 1.1fr) minmax(0, 1.1fr);
      gap: 1.5rem;
    }
  }

  /* Slightly brighter background in light mode */
  @media (prefers-color-scheme: light) {
    .lab-explainer {
      background: #f9fafb;
      border-color: #e5e7eb;
    }
  }
</style>









      <!-- =========================
           Playground 4: Bonferroni + VIF Intuition
           ========================== -->
      <details class="lab-block">
        <summary class="lab-summary">Bonferroni & VIF Intuition ‚Äì Multiple tests and collinearity</summary>
        <div class="lab-inner">
          <div class="ir-grid-2">
            <!-- Bonferroni -->
            <div class="ir-card">
              <h4>Bonferroni Correction ‚Äì Family-wise error</h4>
              <div class="ir-controls-row">
                <div class="ir-slider-label">
                  <span># of independent tests (m)</span>
                  <span class="ir-slider-value" id="bf-m-val">20</span>
                </div>
                <input id="bf-m" type="range" min="1" max="200" step="1" value="20" />
              </div>

              <div class="ir-controls-row">
                <div class="ir-slider-label">
                  <span>Per-test Œ± (uncorrected)</span>
                  <span class="ir-slider-value" id="bf-alpha-val">0.05</span>
                </div>
                <input id="bf-alpha" type="range" min="0.001" max="0.10" step="0.001" value="0.05" />
              </div>

              <div class="ir-kpi">
                FWER before correction
                <span class="val" id="bf-fwer-raw">0.64</span>
              </div>
              <div class="ir-kpi">
                Bonferroni Œ±<sub>corr</sub> = Œ± / m
                <span class="val" id="bf-alpha-corr">0.0025</span>
              </div>
              <div class="ir-kpi">
                ‚âà FWER after correction
                <span class="val" id="bf-fwer-corr">0.05</span>
              </div>
              <div class="ir-kpi">
                Verdict
                <span id="bf-verdict" class="ir-tag neutral">reasonable</span>
              </div>

              <div class="ir-explain" style="margin-top:0.5rem;">
                <p>
                  Every extra test is another chance to see ‚Äúsignificance‚Äù by luck. Bonferroni shrinks the
                  per-test Œ± so the family-wise error rate stays near your target (e.g. 5%).
                </p>
                <p class="ir-note">
                  Try m = 1 vs m = 100 with Œ± = 0.05 and feel how uncorrected FWER explodes.
                </p>
              </div>
            </div>

            <!-- VIF -->
            <div class="ir-card">
              <h4>VIF Intuition ‚Äì How correlation inflates variance</h4>

              <div class="ir-controls-row">
                <div class="ir-slider-label">
                  <span>Correlation between similar features (œÅ)</span>
                  <span class="ir-slider-value" id="vif-rho-val">0.7</span>
                </div>
                <input id="vif-rho" type="range" min="0" max="0.99" step="0.01" value="0.70" />
              </div>

              <div class="ir-controls-row">
                <div class="ir-slider-label">
                  <span># of similarly correlated features (k)</span>
                  <span class="ir-slider-value" id="vif-k-val">3</span>
                </div>
                <input id="vif-k" type="range" min="1" max="10" step="1" value="3" />
              </div>

              <div class="ir-kpi">
                Approx. R¬≤ when regressing one feature on the others
                <span class="val" id="vif-r2">0.66</span>
              </div>
              <div class="ir-kpi">
                VIF = 1 / (1 ‚àí R¬≤)
                <span class="val" id="vif-val">2.94</span>
              </div>
              <div class="ir-kpi">
                Interpretation
                <span id="vif-verdict" class="ir-tag good">acceptable</span>
              </div>

              <div class="ir-explain" style="margin-top:0.5rem;">
                <p>
                  VIF tells you how much the variance of a coefficient is inflated by collinearity with other
                  predictors.
                </p>
                <ul>
                  <li>VIF ‚âà 1 ‚Äì almost no collinearity.</li>
                  <li>VIF 5‚Äì10 ‚Äì concerning; coefficients unstable and hard to interpret.</li>
                  <li>VIF &gt; 10 ‚Äì strong collinearity, rethink your design/features.</li>
                </ul>
                <p class="ir-note">
                  Increase œÅ or k and watch VIF explode while the underlying signal hasn‚Äôt changed at all.
                </p>
              </div>
            </div>
          </div>
        </div>
      </details>
    </div>
  </details>

  <script>
    // ============================
    // Playground 1: Outlier Impact
    // ============================
    (function () {
      const svg = document.getElementById("oi-svg");
      const nSlider = document.getElementById("oi-n");
      const noiseSlider = document.getElementById("oi-noise");
      const outlierYSlider = document.getElementById("oi-outlier-y");
      const nVal = document.getElementById("oi-n-val");
      const noiseVal = document.getElementById("oi-noise-val");
      const outlierYVal = document.getElementById("oi-outlier-y-val");
      const resampleBtn = document.getElementById("oi-resample-btn");
      const tableBody = document.querySelector("#oi-table tbody");

      let baseData = [];
      const TRUE_B0 = 1;
      const TRUE_B1 = 1.2;
      const X_MIN = -3;
      const X_MAX = 3;
      const OUTLIER_X = 2.4;
      let width = 480,
        height = 260;
      const margin = { top: 10, right: 12, bottom: 28, left: 32 };

      function updateSize() {
        const bbox = svg.getBoundingClientRect();
        if (bbox.width > 0) width = bbox.width;
        svg.setAttribute("viewBox", `0 0 ${width} ${height}`);
      }
      updateSize();
      window.addEventListener("resize", updateSize);

      function randn() {
        let u = 0,
          v = 0;
        while (u === 0) u = Math.random();
        while (v === 0) v = Math.random();
        return Math.sqrt(-2 * Math.log(u)) * Math.cos(2 * Math.PI * v);
      }

      function linearFit(xs, ys) {
        const n = xs.length;
        let sx = 0,
          sy = 0,
          sxx = 0,
          sxy = 0;
        for (let i = 0; i < n; i++) {
          const x = xs[i],
            y = ys[i];
          sx += x;
          sy += y;
          sxx += x * x;
          sxy += x * y;
        }
        const xbar = sx / n;
        const ybar = sy / n;
        const denom = sxx - n * xbar * xbar;
        const b1 = denom === 0 ? 0 : (sxy - n * xbar * ybar) / denom;
        const b0 = ybar - b1 * xbar;
        let ssTot = 0,
          ssRes = 0;
        for (let i = 0; i < n; i++) {
          const yhat = b0 + b1 * xs[i];
          const diff = ys[i] - yhat;
          ssRes += diff * diff;
          const dy = ys[i] - ybar;
          ssTot += dy * dy;
        }
        const r2 = ssTot === 0 ? 0 : 1 - ssRes / ssTot;
        return { b0, b1, r2 };
      }

      function generateBaseData() {
        const n = +nSlider.value;
        const noise = +noiseSlider.value;
        baseData = [];
        for (let i = 0; i < n; i++) {
          const x = X_MIN + ((X_MAX - X_MIN) * (i + 0.5)) / n + randn() * 0.1;
          const y = TRUE_B0 + TRUE_B1 * x + randn() * noise;
          baseData.push({ x, y });
        }
      }

      function getOutlier() {
        return { x: OUTLIER_X, y: +outlierYSlider.value };
      }

      function computeDomain() {
        const ys = baseData.map((d) => d.y);
        ys.push(getOutlier().y);
        const minY = Math.min(...ys, -6);
        const maxY = Math.max(...ys, 8);
        return { xMin: X_MIN - 0.2, xMax: X_MAX + 0.2, yMin: minY - 0.5, yMax: maxY + 0.5 };
      }

      function scale(domain) {
        const { xMin, xMax, yMin, yMax } = domain;
        const innerW = width - margin.left - margin.right;
        const innerH = height - margin.top - margin.bottom;
        const sx = (x) => margin.left + ((x - xMin) / (xMax - xMin || 1)) * innerW;
        const sy = (y) => margin.top + innerH - ((y - yMin) / (yMax - yMin || 1)) * innerH;
        return { sx, sy };
      }

      function clearSvg() {
        while (svg.firstChild) svg.removeChild(svg.firstChild);
      }

      function drawAxes(domain, sx, sy) {
        const innerH = height - margin.top - margin.bottom;
        const axis = document.createElementNS("http://www.w3.org/2000/svg", "line");
        axis.setAttribute("x1", margin.left);
        axis.setAttribute("x2", width - margin.right);
        axis.setAttribute("y1", margin.top + innerH);
        axis.setAttribute("y2", margin.top + innerH);
        axis.setAttribute("stroke", "rgba(148,163,184,0.7)");
        axis.setAttribute("stroke-width", "1");
        svg.appendChild(axis);
        [-2, 0, 2].forEach((val) => {
          if (val < domain.xMin || val > domain.xMax) return;
          const x = sx(val);
          const tick = document.createElementNS("http://www.w3.org/2000/svg", "line");
          tick.setAttribute("x1", x);
          tick.setAttribute("x2", x);
          tick.setAttribute("y1", margin.top + innerH);
          tick.setAttribute("y2", margin.top + innerH + 4);
          tick.setAttribute("stroke", "rgba(148,163,184,0.7)");
          tick.setAttribute("stroke-width", "1");
          svg.appendChild(tick);
          const label = document.createElementNS("http://www.w3.org/2000/svg", "text");
          label.textContent = val.toString();
          label.setAttribute("x", x);
          label.setAttribute("y", margin.top + innerH + 16);
          label.setAttribute("text-anchor", "middle");
          label.setAttribute("font-size", "9");
          label.setAttribute("fill", "rgba(148,163,184,0.9)");
          svg.appendChild(label);
        });
      }

      function drawLine(b0, b1, domain, sx, sy, color) {
        const x1 = domain.xMin;
        const x2 = domain.xMax;
        const y1 = b0 + b1 * x1;
        const y2 = b0 + b1 * x2;
        const line = document.createElementNS("http://www.w3.org/2000/svg", "line");
        line.setAttribute("x1", sx(x1));
        line.setAttribute("y1", sy(y1));
        line.setAttribute("x2", sx(x2));
        line.setAttribute("y2", sy(y2));
        line.setAttribute("stroke", color);
        line.setAttribute("stroke-width", "2");
        svg.appendChild(line);
      }

      function drawCircle(x, y, sx, sy, color, r) {
        const c = document.createElementNS("http://www.w3.org/2000/svg", "circle");
        c.setAttribute("cx", sx(x));
        c.setAttribute("cy", sy(y));
        c.setAttribute("r", r);
        c.setAttribute("fill", color);
        svg.appendChild(c);
      }

      function updateTable(baseFit, withFit) {
        const delta = withFit.b1 - baseFit.b1;
        tableBody.innerHTML = "";
        const fmt = (v) => v.toFixed(3);

        function addRow(name, fit, deltaSlope, isBase) {
          const tr = document.createElement("tr");
          const nameTd = document.createElement("td");
          nameTd.textContent = name;
          tr.appendChild(nameTd);

          ["b1", "b0", "r2"].forEach((key) => {
            const td = document.createElement("td");
            td.textContent = fmt(fit[key]);
            tr.appendChild(td);
          });

          const dTd = document.createElement("td");
          dTd.textContent = isBase ? "‚Äì" : fmt(deltaSlope);
          tr.appendChild(dTd);

          const sTd = document.createElement("td");
          if (isBase) {
            sTd.textContent = "reference";
          } else {
            const tag = document.createElement("span");
            tag.className = "ir-tag " + (Math.abs(deltaSlope) < 0.15 ? "good" : "bad");
            tag.textContent = Math.abs(deltaSlope) < 0.15 ? "stable" : "fragile";
            sTd.appendChild(tag);
          }
          tr.appendChild(sTd);
          tableBody.appendChild(tr);
        }

        addRow("Baseline (no outlier)", baseFit, 0, true);
        addRow("With outlier", withFit, delta, false);
      }

      function redraw() {
        nVal.textContent = nSlider.value;
        noiseVal.textContent = noiseSlider.value;
        outlierYVal.textContent = outlierYSlider.value;

        const outlier = getOutlier();
        const xsBase = baseData.map((d) => d.x);
        const ysBase = baseData.map((d) => d.y);
        const baseFit = linearFit(xsBase, ysBase);
        const xsWith = xsBase.concat(outlier.x);
        const ysWith = ysBase.concat(outlier.y);
        const withFit = linearFit(xsWith, ysWith);

        const domain = computeDomain();
        const { sx, sy } = scale(domain);
        clearSvg();
        drawAxes(domain, sx, sy);
        baseData.forEach((d) => drawCircle(d.x, d.y, sx, sy, "#38bdf8", 3));
        drawCircle(outlier.x, outlier.y, sx, sy, "#ef4444", 4);
        drawLine(baseFit.b0, baseFit.b1, domain, sx, sy, "#0ea5e9");
        drawLine(withFit.b0, withFit.b1, domain, sx, sy, "#f97316");
        updateTable(baseFit, withFit);
      }

      function init() {
        generateBaseData();
        redraw();
      }

      nSlider.addEventListener("input", () => {
        generateBaseData();
        redraw();
      });
      noiseSlider.addEventListener("input", () => {
        generateBaseData();
        redraw();
      });
      outlierYSlider.addEventListener("input", redraw);
      resampleBtn.addEventListener("click", () => {
        generateBaseData();
        redraw();
      });

      init();
    })();

    // ======================================
    // Playground 2: Cook's Distance Explorer
    // ======================================
    (function () {
      const svg = document.getElementById("cd-svg");
      const xSlider = document.getElementById("cd-x");
      const residSlider = document.getElementById("cd-resid");
      const xVal = document.getElementById("cd-x-val");
      const residVal = document.getElementById("cd-resid-val");
      const resampleBtn = document.getElementById("cd-resample-btn");
      const tableBody = document.getElementById("cd-table-body");

      const TRUE_B0 = 1;
      const TRUE_B1 = 1.2;
      const X_MIN = -3;
      const X_MAX = 3;
      let baseData = [];
      let width = 480,
        height = 260;
      const margin = { top: 10, right: 12, bottom: 28, left: 32 };

      function updateSize() {
        const bbox = svg.getBoundingClientRect();
        if (bbox.width > 0) width = bbox.width;
        svg.setAttribute("viewBox", `0 0 ${width} ${height}`);
      }
      updateSize();
      window.addEventListener("resize", updateSize);

      function randn() {
        let u = 0,
          v = 0;
        while (u === 0) u = Math.random();
        while (v === 0) v = Math.random();
        return Math.sqrt(-2 * Math.log(u)) * Math.cos(2 * Math.PI * v);
      }

      function generateBaseData() {
        baseData = [];
        const n = 25;
        const noise = 1.0;
        for (let i = 0; i < n; i++) {
          const x = X_MIN + ((X_MAX - X_MIN) * (i + 0.5)) / n + randn() * 0.1;
          const y = TRUE_B0 + TRUE_B1 * x + randn() * noise;
          baseData.push({ x, y });
        }
      }

      function linearFit(xs, ys) {
        const n = xs.length;
        let sx = 0,
          sy = 0,
          sxx = 0,
          sxy = 0;
        for (let i = 0; i < n; i++) {
          const x = xs[i],
            y = ys[i];
          sx += x;
          sy += y;
          sxx += x * x;
          sxy += x * y;
        }
        const xbar = sx / n;
        const ybar = sy / n;
        const denom = sxx - n * xbar * xbar;
        const b1 = denom === 0 ? 0 : (sxy - n * xbar * ybar) / denom;
        const b0 = ybar - b1 * xbar;
        let ssRes = 0;
        for (let i = 0; i < n; i++) {
          const yhat = b0 + b1 * xs[i];
          const diff = ys[i] - yhat;
          ssRes += diff * diff;
        }
        const sigma2 = ssRes / (n - 2);
        return { b0, b1, sigma2, xbar, sxx, n };
      }

      function cookDistanceForPoint(cand, fitBase) {
        const { b0, b1, sigma2, xbar, sxx, n } = fitBase;
        const p = 2; // intercept + slope
        const yhat = b0 + b1 * cand.x;
        const e = cand.y - yhat;
        const h = 1 / n + ((cand.x - xbar) ** 2) / (sxx - n * xbar * xbar || 1e-6);
        const D = (e * e / (p * sigma2)) * (h / (1 - h) ** 2);
        return { D, e, h };
      }

      function computeDomain(cand) {
        const ys = baseData.map((d) => d.y).concat(cand.y);
        const minY = Math.min(...ys, -6);
        const maxY = Math.max(...ys, 8);
        return { xMin: X_MIN - 0.2, xMax: Math.max(X_MAX, cand.x) + 0.5, yMin: minY - 0.5, yMax: maxY + 0.5 };
      }

      function scale(domain) {
        const { xMin, xMax, yMin, yMax } = domain;
        const innerW = width - margin.left - margin.right;
        const innerH = height - margin.top - margin.bottom;
        const sx = (x) => margin.left + ((x - xMin) / (xMax - xMin || 1)) * innerW;
        const sy = (y) => margin.top + innerH - ((y - yMin) / (yMax - yMin || 1)) * innerH;
        return { sx, sy };
      }

      function clearSvg() {
        while (svg.firstChild) svg.removeChild(svg.firstChild);
      }

      function drawAxes(domain, sx) {
        const innerH = height - margin.top - margin.bottom;
        const axis = document.createElementNS("http://www.w3.org/2000/svg", "line");
        axis.setAttribute("x1", margin.left);
        axis.setAttribute("x2", width - margin.right);
        axis.setAttribute("y1", margin.top + innerH);
        axis.setAttribute("y2", margin.top + innerH);
        axis.setAttribute("stroke", "rgba(148,163,184,0.7)");
        axis.setAttribute("stroke-width", "1");
        svg.appendChild(axis);
        [-2, 0, 2, 4].forEach((val) => {
          if (val < domain.xMin || val > domain.xMax) return;
          const x = sx(val);
          const tick = document.createElementNS("http://www.w3.org/2000/svg", "line");
          tick.setAttribute("x1", x);
          tick.setAttribute("x2", x);
          tick.setAttribute("y1", margin.top + innerH);
          tick.setAttribute("y2", margin.top + innerH + 4);
          tick.setAttribute("stroke", "rgba(148,163,184,0.7)");
          tick.setAttribute("stroke-width", "1");
          svg.appendChild(tick);
        });
      }

      function drawLine(b0, b1, domain, sx, sy, color) {
        const x1 = domain.xMin;
        const x2 = domain.xMax;
        const y1 = b0 + b1 * x1;
        const y2 = b0 + b1 * x2;
        const line = document.createElementNS("http://www.w3.org/2000/svg", "line");
        line.setAttribute("x1", sx(x1));
        line.setAttribute("y1", sy(y1));
        line.setAttribute("x2", sx(x2));
        line.setAttribute("y2", sy(y2));
        line.setAttribute("stroke", color);
        line.setAttribute("stroke-width", "2");
        svg.appendChild(line);
      }

      function drawCircle(x, y, sx, sy, color, r) {
        const c = document.createElementNS("http://www.w3.org/2000/svg", "circle");
        c.setAttribute("cx", sx(x));
        c.setAttribute("cy", sy(y));
        c.setAttribute("r", r);
        c.setAttribute("fill", color);
        svg.appendChild(c);
      }

      function redraw() {
        xVal.textContent = (+xSlider.value).toFixed(1);
        residVal.textContent = (+residSlider.value).toFixed(1);

        const baseXs = baseData.map((d) => d.x);
        const baseYs = baseData.map((d) => d.y);
        const fitBase = linearFit(baseXs, baseYs);

        const candX = X_MAX + +xSlider.value; // move to the right
        const cleanY = TRUE_B0 + TRUE_B1 * candX;
        const candY = cleanY + (Math.sign(Math.random() - 0.5) || 1) * +residSlider.value;
        const cand = { x: candX, y: candY };

        const domain = computeDomain(cand);
        const { sx, sy } = scale(domain);
        clearSvg();
        drawAxes(domain, sx);
        baseData.forEach((d) => drawCircle(d.x, d.y, sx, sy, "#38bdf8", 3));
        drawCircle(cand.x, cand.y, sx, sy, "#a855f7", 4);
        drawLine(fitBase.b0, fitBase.b1, domain, sx, sy, "#0ea5e9");

        const xsWith = baseXs.concat(cand.x);
        const ysWith = baseYs.concat(cand.y);
        const fitWith = linearFit(xsWith, ysWith);
        drawLine(fitWith.b0, fitWith.b1, domain, sx, sy, "#f97316");

        const info = cookDistanceForPoint(cand, fitBase);
        const fmt = (v) => v.toFixed(3);
        tableBody.innerHTML = "";
        const rows = [
          ["Leverage h", fmt(info.h), info.h > 0.3 ? "high leverage" : "ordinary"],
          ["Residual e", fmt(info.e), Math.abs(info.e) > 2 ? "large residual" : "moderate"],
          [
            "Cook's Distance D",
            fmt(info.D),
            info.D > 1
              ? "strongly influential"
              : info.D > 0.5
              ? "influential"
              : "not strongly influential"
          ]
        ];

        rows.forEach((r, i) => {
          const tr = document.createElement("tr");
          const q = document.createElement("td");
          q.textContent = r[0];
          const v = document.createElement("td");
          v.textContent = r[1];
          const interp = document.createElement("td");
          const tag = document.createElement("span");
          tag.className =
            "ir-tag " +
            (i === 2
              ? info.D > 0.5
                ? "bad"
                : "good"
              : "neutral");
          tag.textContent = r[2];
          interp.appendChild(tag);
          tr.appendChild(q);
          tr.appendChild(v);
          tr.appendChild(interp);
          tableBody.appendChild(tr);
        });
      }

      function init() {
        generateBaseData();
        redraw();
      }

      xSlider.addEventListener("input", redraw);
      residSlider.addEventListener("input", redraw);
      resampleBtn.addEventListener("click", () => {
        generateBaseData();
        redraw();
      });

      init();
    })();

    // ===============================
    // Playground 3: Noise Injection
    // ===============================
    (function () {
      const gauss = document.getElementById("ni-gauss");
      const label = document.getElementById("ni-label");
      const feat = document.getElementById("ni-feature");
      const irr = document.getElementById("ni-irrelevant");

      const gaussVal = document.getElementById("ni-gauss-val");
      const labelVal = document.getElementById("ni-label-val");
      const featVal = document.getElementById("ni-feature-val");
      const irrVal = document.getElementById("ni-irrelevant-val");

      const accBase = document.getElementById("ni-acc-base");
      const accTrain = document.getElementById("ni-acc-train");
      const accCv = document.getElementById("ni-acc-cv");
      const varCv = document.getElementById("ni-var");
      const stab = document.getElementById("ni-stab");
      const verdict = document.getElementById("ni-verdict");

      const BASE_ACC = 0.9;

      function update() {
        const g = +gauss.value;
        const lf = +label.value;
        const f = +feat.value;
        const k = +irr.value;

        gaussVal.textContent = g.toFixed(2);
        labelVal.textContent = lf.toFixed(2);
        featVal.textContent = f.toFixed(2);
        irrVal.textContent = k.toString();

        accBase.textContent = BASE_ACC.toFixed(2);

        const train = Math.min(0.99, BASE_ACC + 0.15 * f - 0.2 * lf);
        const cv =
          BASE_ACC -
          0.18 * g -
          0.4 * lf -
          0.15 * f -
          0.01 * Math.min(30, k) / 30;

        const varScore = 0.01 + 0.03 * g + 0.06 * lf + 0.04 * f + 0.03 * Math.min(30, k) / 30;
        const stabScore = Math.max(0, Math.min(1, cv - varScore));

        accTrain.textContent = Math.max(0, Math.min(1, train)).toFixed(2);
        accCv.textContent = Math.max(0, Math.min(1, cv)).toFixed(2);
        varCv.textContent = Math.max(0, varScore).toFixed(3);
        stab.textContent = stabScore.toFixed(2);

        verdict.classList.remove("good", "bad", "neutral");
        if (stabScore > 0.75 && cv > 0.8) {
          verdict.classList.add("good");
          verdict.textContent = "robust";
        } else if (stabScore < 0.45 || cv < 0.7) {
          verdict.classList.add("bad");
          verdict.textContent = "fragile / overfit risk";
        } else {
          verdict.classList.add("neutral");
          verdict.textContent = "moderate robustness";
        }
      }

      [gauss, label, feat, irr].forEach((el) => el.addEventListener("input", update));
      update();
    })();

    // ===================================
    // Playground 4: Bonferroni & VIF
    // ===================================
    (function () {
      // Bonferroni
      const mSlider = document.getElementById("bf-m");
      const aSlider = document.getElementById("bf-alpha");
      const mVal = document.getElementById("bf-m-val");
      const aVal = document.getElementById("bf-alpha-val");
      const fwerRawEl = document.getElementById("bf-fwer-raw");
      const aCorrEl = document.getElementById("bf-alpha-corr");
      const fwerCorrEl = document.getElementById("bf-fwer-corr");
      const verdict = document.getElementById("bf-verdict");

      function updateBonf() {
        const m = +mSlider.value;
        const a = +aSlider.value;
        mVal.textContent = m.toString();
        aVal.textContent = a.toFixed(3);

        const fwerRaw = 1 - Math.pow(1 - a, m);
        const aCorr = a / m;
        const fwerCorr = Math.min(1, m * aCorr); // ‚âà a

        fwerRawEl.textContent = fwerRaw.toFixed(3);
        aCorrEl.textContent = aCorr.toFixed(4);
        fwerCorrEl.textContent = fwerCorr.toFixed(3);

        verdict.classList.remove("good", "bad", "neutral");
        if (fwerRaw > 0.5 && m > 20) {
          verdict.classList.add("bad");
          verdict.textContent = "FWER out of control";
        } else if (aCorr < 0.0005 && m > 100) {
          verdict.classList.add("neutral");
          verdict.textContent = "very conservative";
        } else {
          verdict.classList.add("good");
          verdict.textContent = "reasonable";
        }
      }

      mSlider.addEventListener("input", updateBonf);
      aSlider.addEventListener("input", updateBonf);
      updateBonf();

      // VIF
      const rhoSlider = document.getElementById("vif-rho");
      const kSlider = document.getElementById("vif-k");
      const rhoVal = document.getElementById("vif-rho-val");
      const kVal = document.getElementById("vif-k-val");
      const r2El = document.getElementById("vif-r2");
      const vifEl = document.getElementById("vif-val");
      const vifVerdict = document.getElementById("vif-verdict");

      function updateVIF() {
        const rho = +rhoSlider.value;
        const k = +kSlider.value;
        rhoVal.textContent = rho.toFixed(2);
        kVal.textContent = k.toString();

        // crude cartoon: effective R^2 grows with rho^2 and k, capped at 0.99
        const r2 = Math.min(0.99, rho * rho * k);
        const vif = 1 / (1 - r2);

        r2El.textContent = r2.toFixed(2);
        vifEl.textContent = vif.toFixed(2);

        vifVerdict.classList.remove("good", "bad", "neutral");
        if (vif < 3) {
          vifVerdict.classList.add("good");
          vifVerdict.textContent = "acceptable";
        } else if (vif < 10) {
          vifVerdict.classList.add("neutral");
          vifVerdict.textContent = "concerning";
        } else {
          vifVerdict.classList.add("bad");
          vifVerdict.textContent = "severe collinearity";
        }
      }

      rhoSlider.addEventListener("input", updateVIF);
      kSlider.addEventListener("input", updateVIF);
      updateVIF();
    })();
  </script>
</section>













<!-- ===================================== -->
<!-- 7. MODEL SELECTION CRITERIA           -->
<!-- ===================================== -->
<details open>
<summary>Model Selection Criteria</summary>
<div class="table-responsive">
<table>
<thead>
<tr>
  <th>Metric</th>
  <th>Decision Criterion</th>
  <th>Purpose</th>
  <th>Description</th>
  <th>Working Mechanism</th>
  <th>Example</th>
  <th>Limitations</th>
</tr>
</thead>
<tbody>

<tr>
<td><strong>Akaike Information Criterion (AIC)</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-delta"></div>
    <div class="colorbar-scale">
      <span class="tick-delta0">0</span>
      <span class="tick-delta2">2</span>
      <span class="tick-delta10">10</span>
      <span class="tick-delta20">20+</span>
    </div>
  </div>

  Compare ŒîAIC relative to best (lowest).<br><br>
  <span class="good">ŒîAIC &lt; 2</span> = essentially equally good.<br><br>
  <span class="moderate">2‚Äì10</span> = some to strong evidence against.<br><br>
  <span class="poor">&gt; 10</span> = much worse than best model.
</td>
<td>Trade off fit vs complexity for out‚Äëof‚Äësample prediction quality.</td>
<td>
\[
\text{AIC} = 2k - 2\log(L),
\]
k = parameters, L = likelihood.</td>
<td>Lower AIC preferred among models fit to same data/response.</td>
<td>Models with ŒîAIC &lt; 2 are often considered similarly plausible.</td>
<td>Asymptotic; for small n, AICc is preferable. Not directly interpretable in
absolute terms.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Treating small AIC differences (&lt; 2) as meaningful when they are usually negligible.</li>
  <li>Comparing AIC values across different datasets or response variables.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Bayesian Information Criterion (BIC)</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-delta"></div>
    <div class="colorbar-scale">
      <span class="tick-delta0">0</span>
      <span class="tick-delta2">2</span>
      <span class="tick-delta10">10</span>
      <span class="tick-delta20">20+</span>
    </div>
  </div>

  ŒîBIC vs best:<br><br>
  <span class="good">&lt; 2</span> = weak evidence against.<br><br>
  <span class="moderate">2‚Äì6</span> = positive evidence against.<br><br>
  <span class="moderate">6‚Äì10</span> = strong.<br><br>
  <span class="poor">&gt; 10</span> = very strong evidence against model.
</td>
<td>More heavily penalise complexity, tending to pick more parsimonious
models as n grows.</td>
<td>
\[
\text{BIC} = k \log(n) - 2\log(L).
\]</td>
<td>Approximate Bayes factor under certain priors; lower is better.</td>
<td>BIC often selects smaller subset of predictors than AIC in large samples.</td>
<td>Assumes true model is in candidate set; may underfit when prediction, not
true model recovery, is goal.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using BIC to choose between models when the goal is pure prediction rather than parsimony.</li>
  <li>Comparing BIC across datasets with different numbers of observations.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Mallows‚Äô C<sub>p</sub></strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-cp"></div>
    <div class="colorbar-scale">
      <span class="tick-cp-neg10">-10</span>
      <span class="tick-cp-0">0</span>
      <span class="tick-cp-pos10">+10</span>
    </div>
  </div>

  For subset size p (predictors), good models have C<sub>p</sub> ‚âà p+1.<br><br>
  <span class="good">C<sub>p</sub> ‚àí (p+1) ‚âà 0</span> = balanced bias‚Äìvariance.<br><br>
  <span class="poor">Far above p+1</span> = underfitting (missing predictors).<br><br>
  <span class="poor">Far below p+1</span> = possible overfitting.
</td>
<td>Guide subset selection in linear regression.</td>
<td>Compares residual SS of subset model to full model‚Äôs error variance.</td>
<td>Plot C<sub>p</sub> vs p; models near diagonal line C<sub>p</sub> = p+1 are desirable.</td>
<td>Helps choose among many subset models with similar R¬≤.</td>
<td>Relies on full model as reference; computationally expensive for large
predictor sets without heuristics.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Ignoring the uncertainty in selecting the ‚Äúbest‚Äù C<sub>p</sub> model when many subsets have similar values.</li>
  <li>Using C<sub>p</sub> when the full model is itself misspecified.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Cross‚Äëvalidated Deviance / Log‚ÄëLoss</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-error"></div>
    <div class="colorbar-scale">
      <span class="tick-err0">low</span>
      <span class="tick-errmid">medium</span>
      <span class="tick-errhi">high</span>
    </div>
  </div>

  Lower is better.<br><br>
  <span class="good">Lowest CV deviance among candidates</span> = preferred model.<br><br>
  <span class="moderate">Differences &lt; 1‚Äì2%</span> = often negligible in practice.<br><br>
  <span class="poor">Much larger deviance</span> = clearly worse predictive model.
</td>
<td>Directly compare predictive models using out‚Äëof‚Äësample negative log‚Äëlikelihood.</td>
<td>Compute mean deviance / log‚Äëloss across CV folds.</td>
<td>Penalises wrong confident predictions more strongly than Brier score.</td>
<td>Standard for logistic / probabilistic models in ML competitions.</td>
<td>Can be dominated by rare but very miscalibrated regions; needs context
for what constitutes a big improvement.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Over‚Äëtuning hyperparameters to tiny differences in log‚Äëloss on noisy validation data.</li>
  <li>Ignoring class weights or imbalance when interpreting deviance alone.</li>
</ul>
</td>
</tr>

<tr>
<td><strong>Regularisation Strength (Œª)</strong></td>
<td>
  <div class="colorbar-wrapper">
    <div class="colorbar colorbar-biasvar"></div>
    <div class="colorbar-scale">
      <span class="tick-0">too small</span>
      <span class="tick-05">balanced</span>
      <span class="tick-10">too large</span>
    </div>
  </div>

  <span class="poor">Very small Œª</span> ‚Üí unregularised, risk of overfitting.<br><br>
  <span class="good">Œª chosen by CV</span> ‚Üí good bias‚Äìvariance compromise.<br><br>
  <span class="poor">Very large Œª</span> ‚Üí coefficients shrunk too much (underfitting).
</td>
<td>Control complexity of models like ridge, lasso, elastic‚Äënet.</td>
<td>Penalise large coefficients (L2) or enforce sparsity (L1) to improve
generalisation.</td>
<td>Typically chosen via cross‚Äëvalidated performance curves over Œª.</td>
<td>L1 (lasso) can produce sparse models; L2 (ridge) stabilises coefficients
with multicollinearity.</td>
<td>Interpretation depends on scaling; different Œª scales across algorithms and
implementations.<br><br>
<strong>Common pitfalls:</strong>
<ul>
  <li>Using default Œª without checking for under/over‚Äëregularisation via validation curves.</li>
  <li>Comparing Œª values across models with different feature standardisation or penalty definitions.</li>
</ul>
</td>
</tr>

</tbody>
</table>
</div>
</details>









<!-- ================================
     Cutting-Edge ML Metrics (2020‚Äì2025)
     ================================ -->
<details class="metric-section">
  <summary>Cutting-Edge ML Metrics (2020‚Äì2025)</summary>

  <section class="ref-card" style="background-color: #f8f9fa">
   
   
   
          <div class="cutting-header">
          <strong> </strong>
          <span  > </span>
        </div>
   
  
   
    <p class="ref-description">
      Modern machine-learning research uses metrics that go beyond classical AUC, RMSE, and p-values.
      These tools measure calibration quality, distribution shift, risk ranking, uncertainty, robustness,
      fairness, and high-dimensional model stability.
    </p>

    <!-- Optional tiny legend for the coloured badges -->

    <ul class="ref-list cutting-list">
      <li>
        <div class="cutting-header">
          <strong>Advanced Calibration Metrics</strong>
          <span class="cutting-badge cutting-badge--practice">research ‚Üí practice</span>
        </div>
        ACE (Adaptive Calibration Error), TCE (Thresholded Calibration Error),
        Squared-ECE, Adaptive Reliability Error.
      </li>

      <li>
        <div class="cutting-header">
          <strong>Skill Scores</strong>
          <span class="cutting-badge cutting-badge--prod">common in production</span>
        </div>
        Brier Skill Score (BSS) ‚Äî compares your model to a na√Øve baseline to give calibration improvement.
      </li>

      <li>
        <div class="cutting-header">
          <strong>Modern Ranking &amp; Risk Metrics</strong>
          <span class="cutting-badge cutting-badge--practice">research ‚Üí practice</span>
        </div>
        C-index, Somers‚Äô D, IDI (Integrated Discrimination Improvement),
        NRI (Net Reclassification Improvement).
      </li>

      <li>
        <div class="cutting-header">
          <strong>Bayesian / Predictive Model Selection</strong>
          <span class="cutting-badge cutting-badge--practice">research ‚Üí practice</span>
        </div>
        WAIC, PSIS-LOO, Expected Log Predictive Density (ELPD), Pareto-k diagnostics.
      </li>

      <li>
        <div class="cutting-header">
          <strong>Distribution Shift Diagnostics</strong>
          <span class="cutting-badge cutting-badge--practice">research ‚Üí practice</span>
        </div>
        PSI (Population Stability Index), KL Divergence, Maximum Mean Discrepancy (MMD),
        Wasserstein Distance.
      </li>

      <li>
        <div class="cutting-header">
          <strong>Uncertainty Diagnostics</strong>
          <span class="cutting-badge cutting-badge--research">primarily research</span>
        </div>
        Negative Log-Likelihood (NLL), epistemic vs aleatoric variance decomposition,
        Expected Sharpness.
      </li>

      <li>
        <div class="cutting-header">
          <strong>Robustness &amp; Stability</strong>
          <span class="cutting-badge cutting-badge--research">primarily research</span>
        </div>
        Model Stability Index (MSI), permutation effect-size curves, influence-function diagnostics.
      </li>

      <li>
        <div class="cutting-header">
          <strong>Fairness / Ethical ML Metrics</strong>
          <span class="cutting-badge cutting-badge--practice">research ‚Üí practice</span>
        </div>
        Equalized Odds, Demographic Parity, Predictive Parity,
        Calibration Within Groups.
      </li>

      <li>
        <div class="cutting-header">
          <strong>Interaction-Aware Interpretability</strong>
          <span class="cutting-badge cutting-badge--research">primarily research</span>
        </div>
        SHAP Interaction Values ‚Äî modern extension of SHAP for modelling non-linear pairwise effects.
      </li>

      <li>
        <div class="cutting-header">
          <strong>Leakage Diagnostics</strong>
          <span class="cutting-badge cutting-badge--research">primarily research</span>
        </div>
        Target Leakage Test (permutation with shuffled labels),
        Cross-fold Correlation Leakage Index.
      </li>
    </ul>
  </section>
</details>










<!-- ===================================== -->
<!-- 8. MODEL INTERPRETABILITY & EXPLAINS  -->
<!-- ===================================== -->
<details open>
  <summary>Model Interpretability &amp; Explainability</summary>
  <div class="table-responsive">
    <table>
      <thead>
        <tr>
          <th>Method / Metric</th>
          <th>Decision Criterion</th>
          <th>Purpose</th>
          <th>Description</th>
          <th>Working Mechanism</th>
          <th>Example</th>
          <th>Limitations</th>
        </tr>
      </thead>
      <tbody>

        <tr>
          <td><strong>Permutation Feature Importance</strong></td>
          <td>
            <div class="colorbar-wrapper">
              <div class="colorbar colorbar-standard"></div>
              <div class="colorbar-scale">
                <span class="tick-0">0</span>
                <span class="tick-05">medium</span>
                <span class="tick-10">large</span>
              </div>
            </div>

            <span class="good">Large performance drop when permuted</span> = strong importance.<br><br>
            <span class="poor">Near‚Äëzero drop</span> = little contribution (under current model).
          </td>
          <td>Global importance ranking for features in any black‚Äëbox model.</td>
          <td>Measures how much a model‚Äôs performance metric worsens when the
          values of a feature are randomly permuted.</td>
          <td>Break the relationship between a feature and target by shuffling that
          feature; recompute performance and compare to baseline.</td>
          <td>Rank features by drop in ROC AUC or RMSE in a tree ensemble to understand which inputs matter most.</td>
          <td>Correlated features can share importance (each appears less
          important alone). Requires many evaluations of the model; may be expensive.<br><br>
          <strong>Common pitfalls:</strong>
          <ul>
            <li>Interpreting low importance as ‚Äúirrelevant‚Äù when it may be redundant with correlated variables.</li>
            <li>Permuting time‚Äëseries or grouped features independently, breaking their structure.</li>
          </ul>
          </td>
        </tr>

        <tr>
          <td><strong>SHAP Values</strong></td>
          <td>
            <div class="colorbar-wrapper">
              <div class="colorbar colorbar-robustness"></div>
              <div class="colorbar-scale">
                <span class="tick-robust-low">unstable</span>
                <span class="tick-robust-mid">ok</span>
                <span class="tick-robust-high">stable</span>
              </div>
            </div>

            <span class="good">Stable patterns that match domain knowledge</span> are desirable.<br><br>
            Very large |SHAP| values indicate strongly influential features for a given prediction.
          </td>
          <td>Provide theoretically grounded, additive feature attributions for
          individual predictions and global patterns.</td>
          <td>Based on Shapley values from cooperative game theory; each feature
          gets a contribution to pushing the prediction away from a baseline.</td>
          <td>Approximate each feature‚Äôs marginal contribution by averaging over
          many coalitions of features; specialised fast algorithms exist for tree‚Äëbased models.</td>
          <td>Global SHAP summary plots highlight which variables drive model
          predictions overall; local plots explain single predictions
          (e.g. why a loan was rejected).</td>
          <td>Computationally expensive for complex models without specialised
          approximations; explanations can be misread as causal rather than correlational.<br><br>
          <strong>Common pitfalls:</strong>
          <ul>
            <li>Over‚Äëinterpreting SHAP values as causal effects instead of associations.</li>
            <li>Ignoring the impact of feature collinearity on SHAP attribution.</li>
          </ul>
          </td>
        </tr>

        <tr>
          <td><strong>LIME</strong> (Local Interpretable Model‚Äëagnostic Explanations)</td>
          <td>
            <div class="colorbar-wrapper">
              <div class="colorbar colorbar-robustness"></div>
              <div class="colorbar-scale">
                <span class="tick-robust-low">unstable</span>
                <span class="tick-robust-mid">ok</span>
                <span class="tick-robust-high">faithful</span>
              </div>
            </div>

            Good explanations are <span class="good">locally faithful</span> (approximate the model well
            near the point of interest) and sparse enough to be interpretable.
          </td>
          <td>Explain individual predictions by fitting a simple surrogate model
          around a local neighbourhood.</td>
          <td>Samples points near the instance, queries the black‚Äëbox model, and
          fits an interpretable model (e.g. linear or small tree) to those local outputs.</td>
          <td>Weights samples by proximity to the instance; the surrogate‚Äôs
          coefficients are reported as local feature importance.</td>
          <td>Useful for debugging why a specific prediction was made, especially
          in regulated domains where explanations must be human‚Äëreadable.</td>
          <td>Local surrogate may be unstable (different runs give different
          explanations); only valid near the point; can be misleading globally.<br><br>
          <strong>Common pitfalls:</strong>
          <ul>
            <li>Treating LIME‚Äôs local linear model as a global explanation.</li>
            <li>Ignoring randomness in neighbourhood sampling, leading to inconsistent explanations.</li>
          </ul>
          </td>
        </tr>

      </tbody>
    </table>
  </div>
</details>

  <!-- ===================================== -->
  <!-- 9. PYTHON CODE SNIPPETS               -->
  <!-- ===================================== -->
  <details open>
    <summary>Python Code Snippets (scikit‚Äëlearn / statsmodels / SHAP / LIME)</summary>

    <h2>Classification Metrics & Confusion Matrix</h2>
<a href="https://mybinder.org/v2/gh/slashennui/Classification-Metrics-Confusion-Matrix/4c3d2c41f4a9ae2038ba76865e07590712401d58?urlpath=lab%2Ftree%2FClassification%20Metrics%20%26%20Confusion%20Matrix.ipynb"
   target="_blank"
   style="text-decoration: none;">
    <img src="https://static.mybinder.org/badge_logo.svg" alt="Launch on Binder" />
</a>
<div id="metrics-code-block">
  <style>
    /* --- Local styles, only affect this block --- */
    #metrics-code-block pre {
      background: #2e3440;      /* dark editor-like background */
      color: #e5e9f0;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-family: "Fira Code", "JetBrains Mono", Menlo, Consolas, monospace;
      font-size: 0.9rem;
      margin: 1.5rem 0;
    }

    #metrics-code-block code {
      white-space: pre;
    }

    /* Minimal highlight.js theme, scoped to this block */
    #metrics-code-block .hljs        { color: #e5e9f0; }
    #metrics-code-block .hljs-keyword{ color: #81a1c1; font-weight: bold; }
    #metrics-code-block .hljs-built_in,
    #metrics-code-block .hljs-type   { color: #8fbcbb; }
    #metrics-code-block .hljs-string { color: #a3be8c; }
    #metrics-code-block .hljs-number { color: #b48ead; }
    #metrics-code-block .hljs-comment{ color: #616e88; font-style: italic; }
    #metrics-code-block .hljs-function .hljs-title { color: #88c0d0; }
    #metrics-code-block .hljs-params { color: #d8dee9; }
  </style>

  <pre><code class="language-python">
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Generate dummy data for demonstration
X, y = np.random.rand(100, 5), np.random.randint(0, 2, 100)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Train a simple classification model (e.g., Logistic Regression)
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# Define y_true and X for the existing metrics calculation cell
y_true = y_test
X = X_test

from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, average_precision_score,
    matthews_corrcoef, cohen_kappa_score
)

y_proba = model.predict_proba(X)[:, 1]
y_pred = (y_proba >= 0.5).astype(int)

cm = confusion_matrix(y_true, y_pred)
acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred)
rec = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
roc_auc = roc_auc_score(y_true, y_proba)
pr_auc = average_precision_score(y_true, y_proba)
mcc = matthews_corrcoef(y_true, y_pred)
kappa = cohen_kappa_score(y_true, y_pred)

print("Confusion Matrix:\\n", cm)
print("Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("F1-score:", f1)
print("ROC AUC:", roc_auc)
print("PR AUC:", pr_auc)
print("Matthews Corr. Coeff.:", mcc)
print("Cohen's Kappa:", kappa)
  </code></pre>

  <!-- Highlight.js scripts (only affect this block) -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
  <script>
    (function () {
      var block = document.querySelector('#metrics-code-block code');
      if (block) {
        hljs.highlightElement(block);
      }
    })();
  </script>
</div>



<p><br>




    </code></pre>

    <h2>Statistical Tests (t-test, ANOVA, chi-square)</h2>
<a href="https://mybinder.org/v2/gh/slashennui/Statistical-Tests-t-test-ANOVA-chi-square-/main?urlpath=%2Fdoc%2Ftree%2FStatistical+Tests+%28t-test%2C+ANOVA%2C+chi-square%29.ipynb"
   target="_blank"
   style="text-decoration: none;">
    <img src="https://static.mybinder.org/badge_logo.svg" alt="Launch on Binder" />
</a>
<div id="stats-tests-code-block">
  <style>
    /* --- Local styles, only affect this block --- */
    #stats-tests-code-block pre {
      background: #2e3440;      /* dark editor-like background */
      color: #e5e9f0;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-family: "Fira Code", "JetBrains Mono", Menlo, Consolas, monospace;
      font-size: 0.9rem;
      margin: 1.5rem 0;
    }

    #stats-tests-code-block code {
      white-space: pre;
    }

    /* Minimal highlight.js theme, scoped to this block */
    #stats-tests-code-block .hljs        { color: #e5e9f0; }
    #stats-tests-code-block .hljs-keyword{ color: #81a1c1; font-weight: bold; }
    #stats-tests-code-block .hljs-built_in,
    #stats-tests-code-block .hljs-type   { color: #8fbcbb; }
    #stats-tests-code-block .hljs-string { color: #a3be8c; }
    #stats-tests-code-block .hljs-number { color: #b48ead; }
    #stats-tests-code-block .hljs-comment{ color: #616e88; font-style: italic; }
    #stats-tests-code-block .hljs-function .hljs-title { color: #88c0d0; }
    #stats-tests-code-block .hljs-params { color: #d8dee9; }
  </style>

  <pre><code class="language-python">
import numpy as np
import pandas as pd
from scipy import stats
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Sample data for two-sample t-test
group1 = np.random.rand(20) * 10
group2 = np.random.rand(25) * 10 + 2  # Slightly different mean

# two-sample t-test
t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)
print(f"Two-sample t-test: t-statistic = {t_stat:.3f}, p-value = {p_value:.3f}")

# Sample data for chi-square test of independence
contingency_table = np.array([[10, 20], [15, 5]])

# chi-square test of independence
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
print(f"\nChi-square test: chi2 = {chi2:.3f}, p-value = {p:.3f}, dof = {dof}")
print(f"Expected frequencies:\n{expected}")

# Sample data for one-way ANOVA
data = {
    'y': np.concatenate([
        np.random.rand(10) * 5,
        np.random.rand(10) * 5 + 2,
        np.random.rand(10) * 5 + 1
    ]),
    'group': ['A'] * 10 + ['B'] * 10 + ['C'] * 10
}
df = pd.DataFrame(data)

# one-way ANOVA with statsmodels
model = smf.ols('y ~ C(group)', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(f"\nOne-way ANOVA:\n{anova_table}")
  </code></pre>

  <!-- Highlight.js scripts (only affect this block) -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
  <script>
    (function () {
      var block = document.querySelector('#stats-tests-code-block code');
      if (block) {
        hljs.highlightElement(block);
      }
    })();
  </script>
</div>



<p><br>






    <h2>Regression Metrics (MAE vs RMSE)</h2>
  <a href="https://mybinder.org/v2/gh/slashennui/Regression-Metrics-MAE-vs-RMSE-/main?urlpath=%2Fdoc%2Ftree%2FRegression+Metrics+%28MAE+vs+RMSE%29.ipynb"
   target="_blank"
   style="text-decoration: none;">
    <img src="https://static.mybinder.org/badge_logo.svg" alt="Launch on Binder" />
</a>
<div id="mae-rmse-code-block">
  <style>
    /* --- Local styles, only affect this block --- */
    #mae-rmse-code-block pre {
      background: #2e3440;      /* dark editor-like background */
      color: #e5e9f0;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-family: "Fira Code", "JetBrains Mono", Menlo, Consolas, monospace;
      font-size: 0.9rem;
      margin: 1.5rem 0;
    }

    #mae-rmse-code-block code {
      white-space: pre;
    }

    /* Minimal highlight.js theme scoped to this block */
    #mae-rmse-code-block .hljs        { color: #e5e9f0; }
    #mae-rmse-code-block .hljs-keyword{ color: #81a1c1; font-weight: bold; }
    #mae-rmse-code-block .hljs-built_in,
    #mae-rmse-code-block .hljs-type   { color: #8fbcbb; }
    #mae-rmse-code-block .hljs-string { color: #a3be8c; }
    #mae-rmse-code-block .hljs-number { color: #b48ead; }
    #mae-rmse-code-block .hljs-comment{ color: #616e88; font-style: italic; }
    #mae-rmse-code-block .hljs-function .hljs-title { color: #88c0d0; }
    #mae-rmse-code-block .hljs-params { color: #d8dee9; }
  </style>

  <pre><code class="language-python">
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

y_true = y_test
# y_pred = model.predict(X)  # y_pred is already available in the kernel state

mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
  </code></pre>

  <!-- Highlight.js scripts (only affect this block) -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
  <script>
    (function () {
      var block = document.querySelector('#mae-rmse-code-block code');
      if (block) {
        hljs.highlightElement(block);
      }
    })();
  </script>
</div>


<p><br>



    <h2>SHAP / LIME Basics</h2>
    <a href="https://mybinder.org/v2/gh/slashennui/SHAP-LIME-Basics/main?urlpath=%2Fdoc%2Ftree%2FSHAP-LIME+Basics.ipynb"
   target="_blank"
   style="text-decoration: none;">
    <img src="https://static.mybinder.org/badge_logo.svg" alt="Launch on Binder" />
</a>
<div id="lime-shap-code-block">
  <style>
    /* --- Local styles, only affect this block --- */
    #lime-shap-code-block pre {
      background: #2e3440;      /* dark editor-like background */
      color: #e5e9f0;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-family: "Fira Code", "JetBrains Mono", Menlo, Consolas, monospace;
      font-size: 0.9rem;
      margin: 1.5rem 0;
    }

    #lime-shap-code-block code {
      white-space: pre;
    }

    /* Minimal highlight.js theme scoped to this block */
    #lime-shap-code-block .hljs        { color: #e5e9f0; }
    #lime-shap-code-block .hljs-keyword{ color: #81a1c1; font-weight: bold; }
    #lime-shap-code-block .hljs-built_in,
    #lime-shap-code-block .hljs-type   { color: #8fbcbb; }
    #lime-shap-code-block .hljs-string { color: #a3be8c; }
    #lime-shap-code-block .hljs-number { color: #b48ead; }
    #lime-shap-code-block .hljs-comment{ color: #616e88; font-style: italic; }
    #lime-shap-code-block .hljs-function .hljs-title { color: #88c0d0; }
    #lime-shap-code-block .hljs-params { color: #d8dee9; }
  </style>

  <pre><code class="language-python">
import shap
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from lime.lime_tabular import LimeTabularExplainer
import numpy as np
import pandas as pd

# Create a dummy binary target variable for X (since none is provided)
y_binary = (X[:, 0] > np.mean(X[:, 0])).astype(int)  # Example: binarize based on first feature

# Split data for LIME, ensuring X_train and X_test are pandas DataFrames for feature_names
X_train_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
X_test_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])

# Create and train a Logistic Regression model as the 'linear model'
model = LogisticRegression(solver='liblinear')
model.fit(X_train_df, y_binary[:X_train_df.shape[0]])  # Ensure y_binary matches X_train size

# SHAP for the Linear model
explainer = shap.LinearExplainer(model, X_train_df)
shap_values = explainer.shap_values(X_test_df)

# Summary plot
shap.summary_plot(shap_values, X_test_df)

# LIME explainer
explainer = LimeTabularExplainer(
    training_data=X_train_df.values,
    feature_names=X_train_df.columns,
    class_names=['negative', 'positive'],
    mode='classification'
)

i = 0
exp = explainer.explain_instance(
    X_test_df.iloc[i].values,
    model.predict_proba,
    num_features=5
)

exp.show_in_notebook()
  </code></pre>

  <!-- Highlight.js scripts (only affect this block) -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
  <script>
    (function () {
      var block = document.querySelector('#lime-shap-code-block code');
      if (block) {
        hljs.highlightElement(block);
      }
    })();
  </script>
</div>

  </details>
  
  
  
  
  
  
  
  
  
  
  
  
  

<!-- Limits of Models, Metrics & Science -->
<div id="condensed-reference" class="ref-section soft-yellow" style="background-color: #f8f9fa; padding: 20px;">
  <div class="ref-header">
    <h2>Limits of Models, Metrics & Science</h2>
    <p class="ref-subtitle">
      
    </p>
  </div>

  <div class="metric-section__body">
    <p>
      This dashboard summarises many of the tools we currently use to make
      sense of data ‚Äì from classical statistics to modern ML metrics. They
      are powerful, but they are not the whole story.
    </p>

    <p>
      <strong>G√∂del‚Äôs incompleteness theorem</strong> shows that any formal
      system rich enough to express basic arithmetic will contain true
      statements that cannot be proven within that system. No system can be
      both complete and consistent.
    </p>

    <p>
      By analogy, no modelling framework or collection of metrics can ever
      fully capture the processes we study. We always work with:
    </p>

    <ul>
      <li>finite, noisy, and biased data</li>
      <li>simplified models of complex systems</li>
      <li>metrics that highlight some aspects while ignoring others</li>
      <li>assumptions that are never perfectly satisfied</li>
    </ul>

    <p>
      The goal here is not to pretend that AUC, RMSE, p-values, or any
      ‚Äúcutting-edge‚Äù metric delivers final truth. Instead, this dashboard
      makes our tools <strong>explicit</strong> ‚Äì showing where they are
      informative, where they are fragile, and where they leave important
      questions unanswered.
    </p>

    <p>
      In other words: this is a <em>map</em>, not the territory. The map is
      useful, and it keeps improving ‚Äì but if we ever forget that it is
      incomplete, we stop doing science.
    </p>

    <p style="margin-top:1rem; font-weight:500;">
      Thanks for your attention.<br>
      ‚Äî manu
      

    </p>
    <p style="margin-top:1rem; font-weight:500;">
      Email: x34mev@proton.me ‚Ä¢ GitHub: https://github.com/slashennui
    </p>
  </div>
</div>



  
  
  
  
  
  
  
  
 </main> 
  
  
  
  
  
  
  
  
  
  
<!-- ===========================
     FLOATING GLOSSARY PANEL
     =========================== -->
<div id="glossary-panel" class="glossary-panel" aria-label="Data science glossary">
  <div class="glossary-header">
    <span class="glossary-title">Plain-language Glossary</span>
    <div class="glossary-controls">
      <input
        id="glossary-search-input"
        type="text"
        placeholder="Search terms (e.g. p-value, Brier, CI)‚Ä¶"
        aria-label="Search glossary terms"
      />
      <button id="glossary-collapse-btn" title="Collapse glossary">‚àí</button>
      <button id="glossary-minimize-btn" title="Shrink to tiny button">‚ñ¢</button>
    </div>
  </div>

  <div class="glossary-body">
    <section class="glossary-current">
      <div class="glossary-current-title">Terms on this part of the page</div>
      <div id="glossary-current-list" class="glossary-current-list">
        Scroll the page ‚Äì I‚Äôll show terms here.
      </div>
    </section>

<details class="glossary-all" open>
  <summary>
    <div class="glossary-section-title">
      <span>All terms</span>
      <span class="glossary-categories">
        (Statistics ¬∑ Machine Learning ¬∑ Data Science)
      </span>
    </div>
  </summary>

  <div id="glossary-terms" class="glossary-terms"></div>
</details>

  </div>

  <!-- Tiny icon when minimized -->
  <div class="glossary-mini-icon" title="Click to reopen glossary">?</div>
</div>

<style>
  /* ===== Layout & look ===== */
  .glossary-panel {
    position: fixed;
    right: 1.5rem;
    bottom: 1.5rem;
    width: 340px;
    max-height: 70vh;
    background: #020617f2; /* dark glassy */
    color: #f9fafb;
    border-radius: 12px;
    box-shadow: 0 18px 40px rgba(0,0,0,0.45);
    font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
    font-size: 13px;
    z-index: 9999;
    display: flex;
    flex-direction: column;
    overflow: hidden;
    backdrop-filter: blur(10px);
  }

  .glossary-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: 6px 8px;
    background: linear-gradient(90deg, #111827, #020617);
    border-bottom: 1px solid rgba(148,163,184,0.4);
    gap: 6px;
  }

  .glossary-title {
    font-weight: 600;
    font-size: 13px;
    letter-spacing: 0.02em;
  }

  .glossary-controls {
    display: flex;
    align-items: center;
    gap: 4px;
  }

  #glossary-search-input {
    font-size: 11px;
    padding: 3px 6px;
    border-radius: 999px;
    border: 1px solid rgba(148,163,184,0.7);
    background: rgba(15,23,42,0.9);
    color: #e5e7eb;
    min-width: 150px;
  }

  #glossary-search-input::placeholder {
    color: #9ca3af;
  }

  #glossary-search-input:focus {
    outline: none;
    border-color: #38bdf8;
    box-shadow: 0 0 0 1px rgba(56,189,248,0.7);
  }

  .glossary-controls button {
    background: transparent;
    border: 1px solid rgba(148,163,184,0.8);
    color: #9ca3af;
    cursor: pointer;
    padding: 1px 6px;
    font-size: 11px;
    line-height: 1.2;
    border-radius: 6px;
  }

  .glossary-controls button:hover {
    color: #e5e7eb;
    border-color: #e5e7eb;
  }

  .glossary-body {
    padding: 8px 8px 10px;
    overflow: auto;
  }

  .glossary-current {
    margin-bottom: 8px;
    border-radius: 8px;
    background: rgba(15,23,42,0.9);
    padding: 6px 8px;
  }

  .glossary-current-title {
    font-weight: 600;
    font-size: 12px;
    margin-bottom: 4px;
  }

  .glossary-current-list {
    display: flex;
    flex-wrap: wrap;
    gap: 4px;
    font-size: 11px;
  }

  .glossary-chip {
    border: none;
    border-radius: 999px;
    padding: 2px 8px;
    font-size: 11px;
    cursor: pointer;
    background: #020617;
    color: #e5e7eb;
    white-space: nowrap;
  }

  .glossary-chip:hover {
    background: #2563eb;
    color: #f9fafb;
  }

  .glossary-all {
    margin-top: 6px;
  }

  .glossary-all summary {
    cursor: pointer;
    list-style: none;
    font-weight: 600;
    font-size: 12px;
    margin-bottom: 4px;
  }

  .glossary-all summary::-webkit-details-marker {
    display: none;
  }

  .glossary-terms {
    display: flex;
    flex-direction: column;
    gap: 6px;
    margin-top: 4px;
  }

  .glossary-term {
    padding: 6px 8px;
    border-radius: 8px;
    background: #020617;
    border: 1px solid rgba(148,163,184,0.4);
  }

  .glossary-term-title-row {
    display: flex;
    align-items: baseline;
    justify-content: space-between;
    gap: 8px;
    margin-bottom: 2px;
  }

  .glossary-term-name {
    font-weight: 600;
    font-size: 12px;
  }

  .glossary-term-tag {
    font-size: 10px;
    opacity: 0.75;
  }

  .glossary-term-def {
    font-size: 12px;
    color: #e5e7eb;
  }

  .glossary-term--highlight {
    outline: 2px solid #facc15;
    outline-offset: 1px;
    box-shadow: 0 0 0 1px #facc15;
  }

  /* Collapsed (header only) */
  .glossary-panel.collapsed .glossary-body {
    display: none;
  }

  /* Minimized: tiny square */
  .glossary-panel.minimized {
    width: 36px;
    height: 36px;
    padding: 0;
    border-radius: 10px;
    background: #020617;
    display: flex;
    align-items: center;
    justify-content: center;
    cursor: pointer;
  }

  .glossary-panel.minimized .glossary-header,
  .glossary-panel.minimized .glossary-body {
    display: none;
  }

  .glossary-mini-icon {
    display: none;
    font-weight: 700;
    font-size: 18px;
    color: #f9fafb;
  }

  .glossary-panel.minimized .glossary-mini-icon {
    display: block;
  }

  /* Mobile tweaks */
  @media (max-width: 768px) {
    .glossary-panel {
      right: 0.6rem;
      bottom: 0.6rem;
      width: 90vw;
      max-height: 60vh;
    }

    #glossary-search-input {
      min-width: 110px;
    }
    
    
        .glossary-all summary {
      list-style: none;
      cursor: pointer;
    }
    
    .glossary-all summary::-webkit-details-marker {
      display: none;
    }
    
    .glossary-section-title {
      display: flex;
      justify-content: space-between;
      align-items: baseline;
      font-weight: 600;
      gap: 0.5rem;
    }
    
    .glossary-categories {
      font-size: 0.78rem;
      opacity: 0.75;
      white-space: nowrap;
    }
    
    /* Stack nicely on small screens */
    @media (max-width: 700px) {
      .glossary-section-title {
        flex-direction: column;
        align-items: flex-start;
      }
    }

    
  }
</style>

<script>
(function () {
  /* ========= 1. GLOSSARY TERMS (A‚ÄìZ) ========= */
    const GLOSSARY = {
      // ---------- A ----------
      "ACE (Adaptive Calibration Error)": "A calibration error that uses adaptive bins instead of fixed ones, reducing bias in the estimate.",
      "Accuracy": "Share of predictions that are correct. Can be misleading when one class is much more common than the other.",
      "Adam optimization": "An extension of stochastic gradient descent that keeps running averages of gradients and squared gradients to give each parameter its own adaptive learning rate.",
      "Adaptive Reliability Error": "A modern calibration metric that uses adaptive binning and proper scoring-rule ideas to measure miscalibration more reliably, especially when data are sparse in some probability ranges.",
      "Adjusted R¬≤": "A version of R¬≤ that penalises adding extra features. It increases only if a new feature improves the model more than expected by chance.",
      "Aleatoric uncertainty": "Uncertainty that comes from inherent randomness or noise in the data-generating process; it cannot be reduced even with infinite data.",
      "Alpha (Œ±)": "The chosen significance level (often 0.05). It is the tolerated probability of a Type I error per test under the null hypothesis.",
      "Anderson‚ÄìDarling test": "A statistical test that checks whether data follow a specific distribution, giving more weight to deviations in the tails than tests like KS.",
      "ANOVA (Analysis of Variance)": "A family of tests that compare means across multiple groups by checking whether between-group variation is larger than expected from within-group noise.",
      "Apache Spark": "A distributed computing framework for big data that supports SQL, streaming, machine learning and works with Python, R, Scala and Java.",
      "Area under the Curve (AUC)": "Generic term for the area under a performance curve (ROC, Precision‚ÄìRecall). Summarises performance across thresholds.",
      "Autocorrelation": "Correlation of a series with itself at different time lags. In time series, residual autocorrelation violates independence assumptions.",
      "Autoregression": "A time-series model where the current value is predicted from its own past values (lags) using a regression equation.",
    
      // ---------- B ----------
      "Backpropagation": "The algorithm used in neural networks to propagate the loss gradient backwards through layers and update weights.",
      "Bagging": "Short for bootstrap aggregating: train many models on different bootstrap samples and average or vote their predictions to reduce variance.",
      "Bar chart": "A simple plot that shows counts or summary values for different categories using bars with lengths proportional to the values.",
      "Baseline hazard": "In the Cox model, the hazard pattern over time when all predictors are at zero. The model estimates how covariates shift this baseline risk.",
      "Bayes‚Äô theorem": "A rule for updating the probability of a hypothesis given new evidence: posterior ‚àù likelihood √ó prior.",
      "Bayesian statistics": "An approach that treats unknown quantities as random variables with priors and updates them using Bayes‚Äô theorem when new data arrive.",
      "BIC (Bayesian Information Criterion)": "A model-selection score that penalises model complexity. Lower BIC indicates a better trade-off between fit and simplicity.",
      "Bias": "Systematic deviation from the truth. High-bias models are too simple and miss real structure (underfitting).",
      "Bias‚Äìvariance tradeoff": "Simple models: high bias, low variance. Complex models: low bias, high variance. Good models balance both to minimise total error.",
      "Big data": "Datasets that are too large or fast for traditional tools and require distributed storage and processing.",
      "Binary variable": "A variable that only takes two values such as 0/1, yes/no or true/false.",
      "Binomial distribution": "A distribution for the number of ‚Äòsuccesses‚Äô in a fixed number of independent trials with constant success probability.",
      "Bonferroni correction": "A method used to counteract the problem (false positives/Type I errors) of multiple comparisons by adjusting the significance level (alpha) needed to consider a result statistically significant, typically by dividing the original alpha by the number of comparisons being made.",
      "Bootstrapping": "Resampling with replacement from the data to estimate uncertainty (for example confidence intervals) or stability of a statistic or model.",
      "Boosting": "An ensemble method that trains models sequentially, each focusing on errors of the previous ones, and combines them to improve performance.",
      "Box plot": "A visual summary of a distribution showing median, quartiles, range and potential outliers.",
      "Brier score": "A metric for probabilistic predictions between 0 and 1: the mean squared difference between predicted probabilities and actual outcomes (0 or 1). Lower is better.",
      "Brier Skill Score (BSS)": "Compares a model‚Äôs Brier score to that of a baseline (for example predicting the base rate). Positive values mean improvement over the baseline.",
      "Brown‚ÄìForsythe test": "A robust variant of Levene‚Äôs test that uses medians instead of means to check equality of variances; more robust when distributions are skewed or heavy-tailed.",
      "Business analytics": "Using data, statistics and modelling to understand performance and support business decisions.",
      "Business intelligence": "Tools and processes that collect, integrate and visualise data to support reporting and high-level decision making.",
    
      // ---------- C ----------
      "C-index (Concordance Index)": "A rank-based performance measure (often in survival analysis) similar to ROC AUC; probability that a randomly chosen case with the event gets a higher predicted risk.",
      "Calibration": "How well predicted probabilities match observed frequencies (for example, among predictions of 0.7, about 70% are actually positive).",
      "Calibration within groups": "Calibration checked separately in subgroups (for example by gender or age) to test whether probabilities are fair and well calibrated for everyone.",
      "CART (Classification and Regression Trees)": "A popular decision-tree algorithm used for both classification and regression.",
      "Categorical variable": "A variable whose values are labels or categories (for example city, colour).",
      "Categorical data tests": "Chi-square test of independence, Chi-square goodness-of-fit, Fisher‚Äôs exact test",
      "Censoring (right-censoring)": "When the event has not occurred by the end of follow-up, so we only know the event time is greater than the observed time.",
      "Chi-square goodness-of-fit": "A test that checks whether observed category counts match expected frequencies from a theoretical or historical distribution.",
      "Chi-square test of independence": "A test that checks whether two categorical variables are independent using a contingency table.",
      "Class imbalance": "When one class is much more common than the others; can make accuracy and some other metrics misleading.",
      "Classification": "A supervised learning problem where the target is a category label, such as yes/no or disease type.",
      "Classification threshold": "The probability cutoff used to turn a predicted probability into a class label (for example ‚â•0.5 ‚Üí positive).",
      "Clustering": "An unsupervised learning task that groups similar observations together without using labels.",
      "Cohen‚Äôs kappa": "Agreement between predictions and true labels, adjusted for agreement expected by chance. 0 = chance level, 1 = perfect agreement.",
      "Coefficient": "A number that multiplies a feature in a model. It describes how the prediction changes when that feature increases by one unit, holding others fixed.",
      "Competing risks": "When multiple mutually exclusive events can occur (e.g., death from different causes). Standard survival methods may overestimate risk unless competing-risk models are used.",
      "Computer vision": "Field that teaches computers to interpret images and video (for example object detection, face recognition).",
      "Concordant‚Äìdiscordant ratio": "A measure comparing how often pairs of observations are ranked in the same order (concordant) or opposite order (discordant).",
      "Concordance index (C-index)": "A discrimination metric for survival models: the probability that a subject who experiences the event earlier receives a higher predicted risk. 0.5 = random, 1.0 = perfect.",
      "Confidence interval (CI)": "A range of plausible values for a parameter, given the data and a chosen confidence level (for example 95%). Narrow intervals indicate more precision.",
      "Contingency table": "A table of counts for combinations of categories (for example treatment √ó outcome) used in chi-square and Fisher‚Äôs exact tests.",
      "Confusion matrix": "A table with counts of true positives, false positives, true negatives and false negatives. It is the raw bookkeeping for classification performance.",
      "Continuous variable": "A numeric variable that, in principle, can take infinitely many values in a range (for example height).",
      "Convergence": "When an iterative algorithm‚Äôs updates get smaller and its output stabilises near a limiting value.",
      "Convex function": "A function where the line segment between any two points on the graph lies above or on the graph. Convex functions make optimisation easier.",
      "Correlation": "A scaled measure of linear association between two variables, between ‚àí1 and +1.",
      "Cosine similarity": "Similarity between two vectors measured as the cosine of the angle between them; 1 = identical direction, 0 = orthogonal.",
      "Cost function": "A formula that measures how wrong a model‚Äôs predictions are; training minimises this quantity.",
      "Cook‚Äôs distance": "An influence measure in regression. Large values indicate points that strongly affect the fitted model and may need closer inspection.",
      "Covariance": "A measure of how two variables vary together. Positive covariance means they tend to increase or decrease together.",
      "Cox proportional hazards model": "A regression model for time-to-event outcomes. Estimates hazard ratios without making assumptions about the baseline hazard. Assumes proportional hazards over time.",
      "Cross entropy": "A loss function for probabilistic classification that compares predicted probability distributions to the true labels.",
      "Cross-fold Correlation Leakage Index": "A heuristic diagnostic that checks how strongly predictions or residuals correlate across cross-validation folds to detect potential data leakage or dependence between folds.",
      "Cross-validation (CV)": "Splitting data into folds to repeatedly train and test a model on different subsets to estimate performance on new data.",
      "Cumulative distribution function (CDF)": "Function that gives the probability that a random variable is less than or equal to a given value.",
    
      // ---------- D ----------
      "Dashboard": "A visual display (charts, tables, indicators) that summarises key metrics for monitoring.",
      "Data leakage": "When information from the future or from the target leaks into the features, making performance look better than it really is.",
      "Data mining": "Exploring large datasets to find patterns, associations, clusters or predictive models.",
      "Data pipeline": "An end-to-end sequence of steps that moves data from raw sources through cleaning and feature creation to model predictions.",
      "Data science": "Combining statistics, programming and domain knowledge to extract insight and value from data.",
      "Data transformation": "Converting variables into a new form (for example log transform, standardise) to make patterns clearer or meet model assumptions.",
      "Database": "An organised collection of data, usually managed with a query language such as SQL.",
      "Dataframe": "A 2-D table-like data structure (rows = observations, columns = variables) used in tools like pandas and R.",
      "Dataset": "A collection of data, typically structured as rows (cases) and columns (variables).",
      "DBSCAN": "A density-based clustering algorithm that groups points that have many neighbours within a distance and marks sparse points as noise.",
      "Decile": "One of ten cut-points that divide ordered data into ten equal-sized groups.",
      "Decision boundary": "The surface in feature space where a classifier switches from predicting one class to another.",
      "Decision tree": "A model that splits data into branches based on feature values, ending in leaf nodes with predictions.",
      "Deep learning": "Machine learning using deep neural networks with many layers that can learn complex patterns from large datasets.",
      "Degree of freedom": "Roughly, the number of independent pieces of information that can vary when estimating a parameter.",
      "Demographic parity": "Fairness criterion where the share of positive predictions is similar across groups (for example genders).",
      "Dependent variable": "The outcome or target whose variation we try to explain or predict using other variables.",
      "Descriptive statistics": "Numbers that summarise data‚Äôs centre and spread, such as mean, median, variance and IQR.",
      "Dimensionality reduction": "Techniques that reduce the number of features while preserving as much information as possible (for example PCA).",
      "Distribution": "A description of how values are spread out (for example normal, skewed, heavy-tailed). Many statistical methods assume a specific distribution.",
      "Dplyr": "An R package that provides a grammar of data manipulation (filter, select, mutate, summarise).",
      "Durbin‚ÄìWatson test": "A test of first-order autocorrelation in regression residuals. Values near 2 ‚âà no autocorrelation; near 0 ‚âà strong positive; near 4 ‚âà strong negative.",
      "Dropout": "A neural-network regularisation technique that randomly drops units during training to reduce overfitting.",
      "Dummy variable": "A 0/1 variable created from a category to indicate presence (1) or absence (0) of that category.",
    
      // ---------- E ----------
      "Early stopping": "Stopping training when performance on a validation set stops improving, to avoid overfitting.",
      "EDA": "Exploratory Data Analysis ‚Äì visually and statistically exploring data to understand structure, distributions, and potential problems.",
      "Effect size": "How big a difference or relationship is, not just whether it is statistically significant. In test contexts, it complements the p-value.",
      "EBIC (Extended BIC)": "An extension of BIC with additional penalty for high-dimensional models (many predictors).",
      "ELPD (Expected Log Predictive Density)": "A Bayesian model-comparison measure based on out-of-sample predictive accuracy; higher is better.",
      "Epistemic uncertainty": "Uncertainty that comes from limited knowledge or data about the model or process; in principle it can be reduced with more or better data.",
      "Equalized odds": "Fairness requirement that true positive and false positive rates be similar across protected groups.",
      "ETL": "Extract, Transform, Load ‚Äì pipelines that pull data from sources, clean/transform it, and load it into a destination system.",
      "Evaluation metrics": "Numbers used to judge model quality, such as accuracy, F1, AUC, RMSE or log-loss.",
      "Event (in survival analysis)": "The outcome of interest occurring in time-to-event data ‚Äî for example death, relapse, churn, equipment failure.",
      "Expected counts": "Theoretical cell counts a chi-square test uses to see how far observed counts deviate from what chance alone would produce.",
      "Expected Sharpness": "A probabilistic-forecast metric that prefers predictions which are both well calibrated and concentrated (sharp) rather than overly diffuse.",
    
      // ---------- F ----------
      "F1-score": "The harmonic mean of precision and recall. High only if both precision and recall are reasonably high.",
      "F-Score": "A generic name for F-measures that combine precision and recall, such as F1.",
      "Factor analysis": "A technique that explains many observed variables using a smaller number of underlying latent factors.",
      "Fairness metric": "Any metric that quantifies how similarly a model treats different groups (for example equalized odds or demographic parity).",
      "False Discovery Rate (FDR)": "Expected proportion of false positives among the tests we call ‚Äúsignificant‚Äù. Often controlled with Benjamini‚ÄìHochberg.",
      "False negative (FN)": "A case that is actually positive but was predicted negative. In hypothesis testing this corresponds to a Type II error.",
      "False positive (FP)": "A case that is actually negative but was predicted positive. In hypothesis testing this corresponds to a Type I error.",
      "Feature": "An input variable used by the model to make predictions. Also called predictor or independent variable.",
      "Feature hashing": "Mapping tokens to indices using a hash function to create fixed-length feature vectors without an explicit vocabulary.",
      "Feature importance": "Any method that quantifies how much each feature contributes to predictions (for example permutation importance or SHAP).",
      "Feature reduction": "Reducing the number of features while trying to keep most of the useful information.",
      "Feature selection": "Choosing a subset of informative features and dropping irrelevant or redundant ones.",
      "Feature space": "The multidimensional space defined by all features; each observation is a point in this space.",
      "Few-shot learning": "Training models to generalise from only a handful of labelled examples per class.",
      "Fisher‚Äôs exact test": "An exact test for 2√ó2 contingency tables, especially when expected counts are small. It avoids large-sample approximations.",
      "Flume": "A tool for collecting, aggregating and moving large volumes of log data into systems like Hadoop.",
      "Frequentist statistics": "Classical statistics that interprets probability as long-run relative frequency and uses sampling distributions and p-values.",
    
      // ---------- G ----------
      "Gated Recurrent Unit (GRU)": "A type of recurrent neural network cell with update and reset gates, simpler than LSTM but with similar goals.",
      "ggplot2": "A popular R package for creating layered, grammar-of-graphics plots.",
      "Go": "A compiled, statically typed language from Google designed for simple, efficient concurrent systems.",
      "Goodness of fit": "How well a model‚Äôs predictions match observed data; often summarised by metrics like R¬≤ or residual patterns.",
      "Gradient boosting": "An ensemble method that sequentially fits models to the residuals of previous models (XGBoost, LightGBM).",
      "Gradient descent": "An optimisation method that iteratively moves parameters in the direction that reduces the cost function the fastest.",
      "Grid search": "A systematic search over combinations of hyperparameters to find a good configuration.",
    
      // ---------- H ----------
      "Hadoop": "An open-source ecosystem for distributed storage (HDFS) and processing (MapReduce, Spark) of large datasets.",
      "Hazard": "The instantaneous risk of an event occurring at time t, given survival until t. Not a probability but a rate.",
      "Hazard ratio (HR)": "A multiplicative measure of how a predictor affects the hazard. HR > 1 increases risk; HR < 1 decreases risk.",
      "Hidden layer": "A layer in a neural network that is neither input nor output; its activations are not directly observed.",
      "Hidden Markov Model": "A probabilistic model where a sequence of hidden states generates observable outputs; used in speech, sequence labelling, etc.",
      "Hierarchical clustering": "A clustering method that builds a tree of clusters by repeatedly merging or splitting groups.",
      "Histogram": "A plot that shows the distribution of a numeric variable by counting observations in value bins.",
      "Hive": "A data warehouse system on top of Hadoop that lets you query large datasets with SQL-like syntax.",
      "Holdout sample": "A part of the data kept aside from training and used only to evaluate model performance.",
      "Holm correction": "A stepwise multiple-testing method that controls family-wise error rate and is usually less conservative than plain Bonferroni.",
      "Holt‚ÄìWinters forecasting": "An exponential smoothing method for time series that handles both trend and seasonality.",
      "Hyperparameter": "A setting chosen before training (for example learning rate, number of trees) that controls how a model is fit.",
      "Hyperplane": "A flat subspace of one dimension less than the surrounding space; used as a decision surface in methods like SVM.",
      "Hypothesis": "A tentative claim about a population or process that can be tested with data.",
    
      // ---------- I ----------
      "IDI (Integrated Discrimination Improvement)": "A metric that compares how much two risk models improve separation between cases and controls.",
      "IID (Independent and Identically Distributed)": "Assumption that observations are independent of each other and drawn from the same distribution.",
      "Immortal-time bias": "A bias created when individuals must survive long enough to enter an exposure group, creating 'event-free' time that improperly benefits that group.",
      "Imputation": "Filling in missing values using statistics (mean, median, mode) or models (for example kNN, regression).",
      "Independence (statistical)": "Assumption that observations do not influence each other. Violations (e.g. time dependence) can invalidate standard test results.",
      "Inferential statistics": "Using sample data to draw conclusions or test hypotheses about a larger population.",
      "Influence function": "A tool from robust statistics / ML that measures how small changes in a training point would change a model‚Äôs estimate.",
      "Iteration": "One pass or update step in an iterative algorithm during training or optimisation.",
      "IQR": "Interquartile range ‚Äì the distance between the 25th and 75th percentiles; a robust measure of spread.",
    
      // ---------- J ----------
      "Jackknife": "A resampling method that leaves out one observation (or group) at a time to estimate variance and influence.",
      "Julia": "A high-performance dynamic language designed for numerical and scientific computing.",
    
      // ---------- K ----------
      "Kaplan‚ÄìMeier estimator": "A non-parametric method for estimating the survival function ‚Äî the probability of an event (such as death) not occurring up to each time point. Handles right-censored data where some individuals' outcomes are not yet observed. Often visualised as a stepwise survival curve and used to compare groups.",
      "Kaplan‚ÄìMeier curve": "A non-parametric estimate of the survival function. Produces a step-shaped curve showing the fraction still event-free over time.",
      "KL divergence": "A measure of how one probability distribution differs from another reference distribution; often used to quantify distribution shift or model misspecification.",
      "K-Means": "A clustering algorithm that partitions data into k clusters by alternating assignment and centroid updates.",
      "Keras": "A high-level neural-network API in Python that runs on backends like TensorFlow.",
      "kNN": "k-nearest neighbours: a simple method that classifies or predicts based on the labels of the k closest training points.",
      "Kolmogorov‚ÄìSmirnov test": "A non-parametric test that compares a sample to a reference distribution or compares two samples to check for distributional differences.",
      "Kruskal‚ÄìWallis test": "A non-parametric analogue of one-way ANOVA for 3+ groups based on ranks, useful when normality or equal-variance assumptions fail.",
      "Kurtosis": "A measure of tail heaviness relative to a normal distribution: high kurtosis = more extreme outliers.",
    
      // ---------- L ----------
      "L1 regularisation": "Regularisation using absolute values of coefficients (Lasso); encourages sparse models with some coefficients exactly zero.",
      "L2 regularisation": "Regularisation using squared coefficients (Ridge); discourages large weights but rarely sets them exactly to zero.",
      "Lag (time-series)": "How far apart two observations are in time. Autocorrelation is often described as a function of lag.",
      "Labeled data": "Data where each example has a known target label, needed for supervised learning.",
      "Lasso regression": "Regression with L1 regularisation, which can shrink some coefficients exactly to zero and perform feature selection.",
      "Leverage": "How unusual a point‚Äôs predictor values are; high-leverage points can strongly pull the regression line.",
      "Levene's test": "A statistical test for checking whether several groups have equal variances. More robust to non-normal data than classic variance-equality tests.",
      "Line chart": "A chart that connects data points with lines to show trends over time or ordered values.",
      "Linear model": "A model where the prediction is a linear combination of inputs plus noise.",
      "Linear regression": "A model that predicts a numeric outcome as a weighted sum of predictors (plus noise).",
      "Ljung‚ÄìBox test": "A test that checks whether a set of autocorrelations is jointly zero, used to see if time-series residuals resemble white noise.",
      "Log loss": "A loss function for probabilistic classification that heavily penalises confident but wrong predictions.",
      "Log-rank test": "A hypothesis test comparing two or more Kaplan‚ÄìMeier curves. Tests whether survival differs systematically between groups.",
      "Log-transformation": "Applying the logarithm to a variable to reduce skewness, stabilise variance, or make multiplicative patterns more linear.",
      "Logistic regression": "A model for binary outcomes; it predicts log-odds that are converted to probabilities between 0 and 1.",
      "Long Short-Term Memory (LSTM)": "A recurrent neural network architecture with gates that help remember or forget information over long sequences.",
    
      // ---------- M ----------
      "MAE": "Mean Absolute Error ‚Äì the average absolute difference between predicted and actual values; easy to read as a ‚Äòtypical error‚Äô.",
      "Machine learning": "Using algorithms that learn patterns from data to make predictions or decisions without explicit rules.",
      "Mahout": "An Apache project providing scalable machine-learning algorithms, especially on Hadoop.",
      "MapReduce": "A programming model for distributed data processing using map and reduce steps over large datasets.",
      "Mann‚ÄìWhitney U test": "A non-parametric test that compares whether two independent groups differ in their typical (median/rank) values.",
      "Market basket analysis": "Finding products that are often purchased together to drive recommendations or bundling.",
      "Market mix modeling": "Using regression-style models to decompose sales into contributions from price, promotion, distribution, etc.",
      "Matthews correlation coefficient (MCC)": "A balanced measure for binary classification using all four confusion-matrix cells; works well with imbalanced data.",
      "Maximum likelihood estimation": "Choosing parameter values that make the observed data most probable under the model.",
      "Mean": "The simple average: add up all values and divide by how many you have.",
      "Means tests": "t-test, One-way ANOVA, Mann‚ÄìWhitney U test, Kruskal‚ÄìWallis test",
      "Median": "The middle value when the data is sorted. Half the values are above and half below; less sensitive to outliers than the mean.",
      "MIS": "Management Information System ‚Äì systems that gather and report data to support organisational decision making.",
      "ML-as-a-Service (MLaaS)": "Cloud services that provide ready-made machine-learning tools and APIs.",
      "Mode": "The value that occurs most frequently in a dataset.",
      "Model drift": "Gradual degradation of model performance because the relationship between features and target changes over time.",
      "Model selection": "Choosing between candidate models or configurations based on criteria like AIC, BIC, CV error or holdout performance.",
      "Model stability index (MSI)": "A summary of how consistent model predictions are under small changes in data or training; higher MSI means more stable.",
      "Monte Carlo simulation": "Using repeated random sampling from distributions to approximate how a complex system behaves.",
      "MMD (Maximum Mean Discrepancy)": "A non-parametric distance between distributions used to detect distribution shift or for domain adaptation.",
      "MSE": "Mean Squared Error ‚Äì the average of squared errors between predictions and actual values; penalises large errors strongly.",
      "Multi-class classification": "Classification problems with more than two possible target classes.",
      "Multicollinearity": "Strong correlation between predictors, making coefficient estimates unstable and hard to interpret.",
      "Multivariate analysis": "Analysing relationships among several variables at the same time.",
      "Multivariate regression": "Regression with multiple dependent variables predicted jointly from a set of predictors.",
      "Multiple testing": "Running many hypothesis tests on the same data. Greatly increases false positives unless controlled with FWER/FDR methods.",
    
      // ---------- N ----------
      "Naive Bayes": "A simple probabilistic classifier that applies Bayes‚Äô theorem assuming features are conditionally independent given the class.",
      "NaN": "Stands for ‚ÄòNot a Number‚Äô: a special value representing missing or undefined numeric results.",
      "Natural Language Processing (NLP)": "Techniques that allow computers to process and analyse human language data.",
      "Negative log-likelihood": "Loss equal to minus the log of the predicted probability of the observed outcomes; lower is better.",
      "Net Reclassification Improvement (NRI)": "A risk-model metric that measures how much a new model improves the correct movement of individuals between risk categories compared with a reference model.",
      "Nominal variable": "A categorical variable whose categories have no natural order (for example city names).",
      "Non-parametric": "Methods that make few assumptions about distribution shape or functional form, often using ranks or resampling.",
      "Normal distribution": "The classic bell-shaped continuous distribution; many methods assume data or residuals are approximately normal.",
      "Normality assumption": "The assumption that data or residuals follow a normal distribution. Many classical tests rely on this.",
      "Normality test": "A statistical test (for example Shapiro‚ÄìWilk, Kolmogorov‚ÄìSmirnov on residuals) that checks whether data follow a normal distribution.",
      "Normalization": "Rescaling features to a common scale (for example 0‚Äì1) to make them comparable.",
      "Normality / tests distribution shape": "Shapiro‚ÄìWilk test, Kolmogorov‚ÄìSmirnov test, Anderson‚ÄìDarling test, Q‚ÄìQ plot",
      "Norms": "Functions that measure vector size, such as L1 (sum of absolute values) or L2 (Euclidean length).",
      "NoSQL": "Databases that store data in non-tabular forms such as key‚Äìvalue, document, column or graph stores.",
      "Numpy": "A core Python library for fast numerical computing and n-dimensional arrays.",
      "Noise": "Random variation in the data that is not explained by the model or features.",
      "Noise feature": "A feature with no real relationship to the target; often added to test robustness or overfitting.",
    
      // ---------- O ----------
      "One hot encoding": "Representing a categorical variable as multiple 0/1 columns, one per category.",
      "One shot learning": "Training a classifier that can recognise a class from a single labelled example.",
      "One-way ANOVA": "A specific ANOVA design that compares means across three or more independent groups using one categorical factor.",
      "Oozie": "A workflow scheduler for Hadoop that coordinates and runs jobs like MapReduce, Hive or Pig in sequence or on a schedule.",
      "Ordinal variable": "A categorical variable with an inherent order (for example low, medium, high).",
      "Outlier": "An observation that is unusual compared to the rest of the data. It can strongly affect many statistics.",
      "Overfitting": "The model fits the noise and quirks of the training data but performs poorly on new data.",
    
      // ---------- P ----------
      "Paired t-test": "A t-test that compares the mean difference between paired or matched observations (for example before‚Äìafter measurements).",
      "Pandas": "A Python library providing DataFrame objects and tools for data wrangling and analysis.",
      "Parameters": "Model quantities learned from data (for example regression coefficients, neural-network weights).",
      "Pareto-k diagnostics": "Diagnostics used with PSIS-LOO; large k values flag influential observations where the importance-sampling approximation may be unreliable.",
      "Pattern recognition": "Finding and classifying recurring patterns in data, often using machine-learning methods.",
      "Percentile": "A value below which a given percentage of observations fall (for example 90th percentile is higher than 90% of the data).",
      "Pie chart": "A circular chart divided into slices that represent parts of a whole.",
      "Pig": "A scripting platform on Hadoop that lets you write data-flow jobs in a higher-level language instead of Java MapReduce.",
      "Polynomial regression": "Regression where the predictors include powers of a variable (for example x, x¬≤, x¬≥) to model curves.",
      "Population Stability Index (PSI)": "A measure of distribution shift between two datasets (for example training vs production); larger PSI indicates more drift.",
      "Post-hoc test": "A follow-up test performed after a significant global test (such as ANOVA) to determine which specific groups differ, usually with multiple-testing correction.",
      "Power": "The probability that a study will detect a real effect (reject the null when it is false). Higher power means fewer missed effects.",
      "PR AUC": "Area under the precision‚Äìrecall curve; more informative than ROC AUC when the positive class is rare.",
      "Pre-trained model": "A model already trained on a large related dataset and reused as a starting point for a new task.",
      "Precision": "Of all predicted positives, how many are actually positive? Focuses on avoiding false positives.",
      "Precision and recall": "Paired metrics: precision focuses on quality of positive predictions; recall focuses on finding all true positives.",
      "Predictive parity": "A fairness criterion where the positive predictive value (precision) is similar across groups; if the model predicts positive, the chance of being truly positive is similar in each group.",
      "Predictor variable": "A feature used to explain or predict the outcome (synonym of independent variable).",
      "Principal Component Analysis (PCA)": "A dimensionality reduction method that transforms correlated variables into a smaller set of uncorrelated components.",
      "Proportional hazards assumption": "The assumption in a Cox model that hazard ratios between groups remain constant over time. Violations can invalidate inference.",
      "PSIS-LOO": "Pareto-smoothed importance-sampling leave-one-out cross-validation; an efficient approximation to full LOO with diagnostics.",
      "P-value": "If there really were no effect, how surprising would this result (or more extreme) be? Small p-values suggest the data are hard to explain under ‚Äòno effect‚Äô.",
      "Python": "A widely used high-level programming language, very popular for data science and machine learning.",
      "PyTorch": "A deep-learning framework in Python that uses dynamic computation graphs and is widely used in research.",
    
      // ---------- Q ----------
      "Q‚ÄìQ plot": "A plot that compares quantiles of the data to quantiles of a reference distribution to visually assess distributional assumptions (for example normality).",
      "Quartile": "Values that split ordered data into four equal parts (Q1, Q2/median, Q3).",
    
      // ---------- R ----------
      "R": "An open-source language and environment for statistical computing, graphics and data analysis.",
      "Range": "The difference between the largest and smallest value in a dataset.",
      "Rank-based test": "A non-parametric test that uses the ordering (ranks) of values instead of the raw values, making it robust to outliers and non-normality.",
      "Recommendation engine": "A system that suggests items (products, movies, etc.) likely to interest a user based on behaviour or similarity.",
      "Recall (Sensitivity)": "Of all true positives in the data, how many did we catch? Focuses on avoiding false negatives.",
      "Regression": "Supervised learning where the target is numeric (for example price, weight).",
      "Regression spline": "A regression model that fits piecewise polynomials joined smoothly at knots, allowing flexible curves.",
      "Regularisation": "Adding a penalty to large coefficients to reduce variance and overfitting (examples: Lasso/L1, Ridge/L2).",
      "Reinforcement learning": "An agent learns to take actions in an environment to maximise long-term reward through trial and error.",
      "Residual": "The difference between the observed value and the model‚Äôs predicted value. Residual patterns help diagnose model problems.",
      "Residual diagnostics": "Plots and tests used to check whether residuals obey model assumptions (normality, equal variance, independence).",
      "Residual plot": "A plot of residuals versus fitted values or time, used to check for patterns that violate model assumptions.",
      "Response variable": "Another term for dependent variable or target; the outcome we model.",
      "Restricted Mean Survival Time (RMST)": "Average survival time up to a specified horizon. A robust alternative to comparing medians or hazard ratios.",
      "Ridge regression": "Regression with L2 regularisation that shrinks coefficients toward zero but rarely to exactly zero.",
      "RMSE": "Root Mean Squared Error ‚Äì the square root of MSE. A typical size of prediction error, in the same units as the target.",
      "Robustness": "How resistant a method or metric is to outliers, noise and assumption violations.",
      "ROC AUC": "Area under the ROC curve; measures how well the model separates positives from negatives across thresholds (0.5 = random, 1.0 = perfect).",
      "Root Mean Squared Error (RMSE)": "Same as RMSE; included for people searching by the long name.",
      "Rotational invariance": "A property where results do not change when the coordinate system is rotated.",
    
      // ---------- S ----------
      "Sample size": "How many observations are in the dataset or in the part of the data being analysed.",
      "Sample size effect (tests)": "The phenomenon that large samples make tiny effects statistically significant, while small samples may miss real effects.",
      "Scala": "A JVM language that combines object-oriented and functional programming.",
      "Semi-supervised learning": "Learning from a mix of labelled and unlabelled data.",
      "SHAP interaction values": "An extension of SHAP that decomposes a prediction into main effects and pairwise interaction effects between features, capturing non-linear interactions.",
      "SHAP values": "Feature-attribution method based on Shapley values that decomposes a prediction into contributions of each feature.",
      "Shapiro‚ÄìWilk test": "A powerful normality test for small to medium samples. Very sensitive to even mild deviations with large n.",
      "Significance (statistical)": "Shorthand for ‚Äòstatistically significant‚Äô; usually means the p-value is below alpha. It does not guarantee a large or important effect.",
      "Signal": "The real underlying pattern or effect in the data that we care about.",
      "Skewness": "How asymmetric a distribution is: positive skew has a longer right tail; negative skew has a longer left tail.",
      "SMOTE": "Synthetic Minority Oversampling Technique ‚Äì creates synthetic minority-class examples to rebalance imbalanced datasets.",
      "Softmax": "Function that converts a vector of scores into a probability distribution over classes.",
      "Somers‚Äô D": "Rank-correlation measure related to the C-index, often used to assess risk models.",
      "Spatial‚Äìtemporal reasoning": "Reasoning about how things change across both space and time, often important in advanced AI systems.",
      "Specificity": "Of all true negatives, how many did we correctly predict as negative?",
      "Squared error": "The square of the difference between predicted and actual values; used in MSE and related metrics.",
      "Stability": "How consistent a coefficient, metric or decision is under resampling or small data changes.",
      "Standard deviation": "The square root of variance. Roughly, a typical distance from the mean in the same units as the data.",
      "Standard error": "The standard deviation of a statistic‚Äôs sampling distribution; measures how precisely the statistic estimates the population value.",
      "Standardization": "Rescaling features to have mean 0 and standard deviation 1 (z-scores).",
      "Statistics": "The discipline that studies how to collect, analyse, present and interpret data.",
      "Stochastic Gradient Descent": "Gradient descent that updates parameters using one or a few training examples at a time.",
      "Sum of squared errors (SSE)": "The sum of all squared errors between predictions and actual values; basis for MSE and RMSE.",
      "Supervised learning": "Learning a mapping from inputs to outputs using labelled data.",
      "Support Vector Machine (SVM)": "A classifier that finds a hyperplane with maximum margin between classes, possibly in a transformed feature space.",
      "Survival analysis": "Statistical methods for modelling time until an event occurs (death, failure, recovery). Often involves censoring, Kaplan‚ÄìMeier curves, and hazard models like Cox regression. It handles censoring, where for some subjects we only know that the event has not yet happened by the end of follow-up.",
      "Survival analysis / time-to-event methods": "Kaplan‚ÄìMeier curve, Log-rank test, Cox proportional hazards model, Hazard ratio, Censoring, C-index, Time-dependent AUC, Brier score (time-dependent), RMST, Competing risks",
      "Survival function": "The probability of being event-free beyond time t. Estimated using Kaplan‚ÄìMeier or parametric survival models.",
      "Survival / Time-to-Event Performance": "Kaplan‚ÄìMeier, C-index, Brier over time, Cox model",  
      "Squared ECE": "A variant of Expected Calibration Error that uses squared differences between predicted and observed frequencies, putting extra weight on larger miscalibration.",
    
      // ---------- T ----------
      "Tail behaviour": "How the extreme values (tails) of a distribution behave. Heavy tails mean more extreme outliers than a normal distribution would predict.",
      "Target": "The outcome the model is trying to predict. Also called label, response or dependent variable.",
      "Target leakage test": "A diagnostic in which target labels are shuffled and the model is retrained; if performance remains high, it suggests that leaked target information is present in the features or pipeline.",
      "Time-series tests": "Durbin‚ÄìWatson test, Ljung‚ÄìBox test",
      "TCE (Thresholded Calibration Error)": "Calibration metric that focuses on predictions above a chosen confidence threshold, highlighting miscalibration where the model is most confident.",
      "TensorFlow": "An open-source library from Google for numerical computation and building machine-learning models, especially deep learning.",
      "Time-dependent AUC": "ROC-style discrimination computed at a specific time point (e.g., 1-year or 5-year event prediction).",
      "Time-dependent covariates": "Predictors whose values change over time and can be incorporated into extended Cox models.",
      "Time-to-event outcome": "A response variable measuring the time until an event occurs, possibly censored if the event has not been observed.",
      "t-test": "A family of tests that compare means to a reference value or between groups, typically assuming normal residuals.",
      "t-test (independent)": "A t-test comparing means of two independent groups. Assumes normal residuals and often equal variances (or use Welch‚Äôs version otherwise).",
      "Tokenization": "Splitting text into tokens (words, subwords, etc.) as a first step in many NLP pipelines.",
      "Torch": "A machine-learning library (and its successor PyTorch) that provides many deep-learning building blocks.",
      "Train‚Äìtest split": "Dividing the data into a training set (to fit the model) and a test set (to evaluate how it generalises).",
      "True negative (TN)": "A case that is actually negative and was predicted negative.",
      "True positive (TP)": "A case that is actually positive and was predicted positive.",
      "Type I error": "Rejecting a true null hypothesis. In practice: a false alarm or false positive.",
      "Type II error": "Failing to reject a false null hypothesis. In practice: a missed effect or false negative.",
    
      // ---------- U ----------
      "UMAP": "A non-linear dimensionality-reduction method often used for 2D/3D visualisation of high-dimensional data.",
      "Underfitting": "The model is too simple to capture the structure in the data and performs poorly even on training data.",
      "Univariate analysis": "Analysing the distribution or relationship of a single variable (for example histogram of one feature).",
      "Unsupervised learning": "Learning patterns or structure from data without labelled outcomes (for example clustering).",
    
      // ---------- V ----------
      "Validation set": "A set of data held out from training and used to tune hyperparameters or choose between models.",
      "Variance": "How much values or estimates spread around their mean; high variance means a lot of variability or instability.",
      "Variance Inflation Factor (VIF)": "Quantifies how much the variance of a regression coefficient is inflated by multicollinearity; large values flag problems.",
      "Variance / spread": "Levene's test, Brown‚ÄìForsythe test",
    
      // ---------- W ----------
      "WAIC": "Widely Applicable Information Criterion ‚Äì a Bayesian model comparison score; lower WAIC usually means better out-of-sample prediction.",
      "Wasserstein distance": "A distance between probability distributions that measures how much ‚Äòmass‚Äô must be moved to transform one into the other; used in optimal transport and robust ML.",
      "Welch‚Äôs t-test": "A version of the independent t-test that does not assume equal variances across groups.",
      "White-noise residuals": "Residuals that show no autocorrelation and roughly constant variance over time; a goal for well-specified time-series models.",
    
      // ---------- X ----------
      "XGBoost": "A highly optimised implementation of gradient-boosted decision trees, widely used for tabular machine-learning problems.",
    
      // ---------- Z ----------
      "Z-score": "A standardised value equal to (value ‚àí mean) / standard deviation; measures how many SDs a point is from the mean.",
      "Z-test": "A hypothesis test that uses the normal distribution to assess how far a statistic is from the null value in standard-deviation units.",
      "Zookeeper": "A coordination service for distributed systems that manages configuration, naming and synchronisation."
    };







    // Treat anything inside the floating glossary panel as "off-limits"
    function isInsideGlossary(el) {
      return !!el.closest('#glossary-panel');
    }
    
    
    
    






  /* ========= 2. BUILD UI ========= */
  const panel = document.getElementById("glossary-panel");
  const termsContainer = document.getElementById("glossary-terms");
  const currentList = document.getElementById("glossary-current-list");
  const collapseBtn = document.getElementById("glossary-collapse-btn");
  const minimizeBtn = document.getElementById("glossary-minimize-btn");
  const miniIcon = panel.querySelector(".glossary-mini-icon");
  const searchInput = document.getElementById("glossary-search-input");

  if (!panel || !termsContainer) return;

  const termNames = Object.keys(GLOSSARY).sort((a, b) =>
    a.localeCompare(b, "en", { sensitivity: "base" })
  );
  const termElements = {}; // name -> element

  function makeId(term) {
    return "glossary-" + term.toLowerCase().replace(/[^a-z0-9]+/gi, "-");
  }

  termNames.forEach(name => {
    const card = document.createElement("div");
    card.className = "glossary-term";
    card.id = makeId(name);

    const titleRow = document.createElement("div");
    titleRow.className = "glossary-term-title-row";

    const termSpan = document.createElement("span");
    termSpan.className = "glossary-term-name";
    termSpan.textContent = name;

    const tagSpan = document.createElement("span");
    tagSpan.className = "glossary-term-tag";
    // Optional extra tag ‚Äì here we just leave blank or reuse a short hint:
    tagSpan.textContent = "";

    const def = document.createElement("div");
    def.className = "glossary-term-def";
    def.textContent = GLOSSARY[name];

    titleRow.appendChild(termSpan);
    titleRow.appendChild(tagSpan);
    card.appendChild(titleRow);
    card.appendChild(def);
    termsContainer.appendChild(card);

    termElements[name] = card;
  });

  /* ========= 3. COLLAPSE / MINIMISE ========= */
  collapseBtn.addEventListener("click", (ev) => {
    ev.stopPropagation();
    panel.classList.toggle("collapsed");
  });

  minimizeBtn.addEventListener("click", (ev) => {
    ev.stopPropagation();
    panel.classList.toggle("minimized");
    if (!panel.classList.contains("minimized")) {
      panel.classList.remove("collapsed");
    }
  });

  miniIcon.addEventListener("click", (ev) => {
    ev.stopPropagation();
    panel.classList.remove("minimized");
  });

  /* ========= 4. SCROLL TO TERM ========= */
  function highlightTerm(name) {
    const el = termElements[name];
    if (!el) return;
    panel.classList.remove("minimized");
    panel.classList.remove("collapsed");
    el.scrollIntoView({ behavior: "smooth", block: "nearest" });
    el.classList.add("glossary-term--highlight");
    setTimeout(() => el.classList.remove("glossary-term--highlight"), 1500);
  }

  /* ========= 5. SEARCH FILTER ========= */
  if (searchInput) {
    searchInput.addEventListener("input", () => {
      const q = searchInput.value.trim().toLowerCase();
      termNames.forEach(name => {
        const el = termElements[name];
        if (!el) return;
        const haystack =
          (name + " " + (GLOSSARY[name] || "")).toLowerCase();
        const match = !q || haystack.includes(q);
        el.style.display = match ? "" : "none";
      });
    });
  }

/* ========= 6. TERMS ON THIS PART OF THE PAGE ========= */
function updateVisibleTerms() {
  const textEls = document.querySelectorAll(
    "p, li, td, th, summary, h1, h2, h3, h4, h5"
  );
  const viewportTop = 0;
  const viewportBottom = window.innerHeight;
  const seen = new Set();

  textEls.forEach(el => {
    // NEW: ignore anything inside the floating glossary
    if (isInsideGlossary(el)) return;

    const rect = el.getBoundingClientRect();
    if (rect.bottom < viewportTop || rect.top > viewportBottom) return;

    const t = (el.innerText || "").toLowerCase();
    if (!t) return;

    termNames.forEach(name => {
      if (seen.has(name)) return;
      const simple = name.toLowerCase();
      if (t.includes(simple)) {
        seen.add(name);
      }
    });
  });

  currentList.innerHTML = "";
  if (!seen.size) {
    currentList.textContent = "Scroll the page ‚Äì I‚Äôll show terms here.";
    return;
  }

  Array.from(seen)
    .sort((a, b) => a.localeCompare(b, "en", { sensitivity: "base" }))
    .slice(0, 12)
    .forEach(name => {
      const btn = document.createElement("button");
      btn.className = "glossary-chip";
      btn.textContent = name;
      btn.addEventListener("click", () => highlightTerm(name));
      currentList.appendChild(btn);
    });
}

updateVisibleTerms();
window.addEventListener("scroll", () => {
  window.requestAnimationFrame(updateVisibleTerms);
});

})();
</script>

  
  
  
  
  





<script>
  document.addEventListener("DOMContentLoaded", function () {
    // 1) Make sure we have the glossary + content container
    if (typeof GLOSSARY === "undefined") return;

    const contentRoot = document.getElementById("page-content");
    if (!contentRoot) return;

    // 2) Prepare term lookup (case-insensitive ‚Üí canonical key)
    const canonicalByLower = {};
    Object.keys(GLOSSARY).forEach((key) => {
      canonicalByLower[key.toLowerCase()] = key;
    });

    // Sort terms by length (longest first) to avoid partial matches
    const sortedTerms = Object.keys(GLOSSARY).sort(
      (a, b) => b.length - a.length
    );

    // Escape for regex
    function escapeRegex(str) {
      return str.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
    }

    const pattern = sortedTerms.map(escapeRegex).join("|");
    // Word-ish boundaries; good enough for UI text
    const termRegex = new RegExp("\\b(" + pattern + ")\\b", "gi");

    // 3) Decide where NOT to touch text
    const SKIP_SELECTOR =
      "script, style, code, pre, a, button, input, textarea, svg, " +
      "aside#glossary-panel, .glossary-inline-term";

    function shouldSkipNode(textNode) {
      const parent = textNode.parentElement;
      if (!parent) return false;
      return Boolean(parent.closest(SKIP_SELECTOR));
    }

    // 4) Walk the DOM and wrap text matches
    function walk(node) {
      if (node.nodeType === Node.TEXT_NODE) {
        const text = node.nodeValue;
        if (!text || !text.trim()) return;
        if (shouldSkipNode(node)) return;

        termRegex.lastIndex = 0;
        if (!termRegex.test(text)) return; // no match in this node

        // Build a fragment with wrapped terms
        const frag = document.createDocumentFragment();
        let lastIndex = 0;
        termRegex.lastIndex = 0;
        let match;

        while ((match = termRegex.exec(text)) !== null) {
          const start = match.index;
          const end = termRegex.lastIndex;
          const matchedText = match[0];

          // Add text before the term
          if (start > lastIndex) {
            frag.appendChild(
              document.createTextNode(text.slice(lastIndex, start))
            );
          }

          // Find canonical key for the matched text (case-insensitive)
          const canonicalKey =
            canonicalByLower[matchedText.toLowerCase()] || matchedText;

          // Create inline span
          const span = document.createElement("span");
          span.className = "glossary-inline-term";
          span.dataset.term = canonicalKey;
          // Native browser tooltip ‚Äì can be replaced later with fancy tooltip
          if (GLOSSARY[canonicalKey]) {
            span.title = GLOSSARY[canonicalKey];
          }
          span.textContent = matchedText;

          frag.appendChild(span);
          lastIndex = end;
        }

        // Remaining text after last match
        if (lastIndex < text.length) {
          frag.appendChild(document.createTextNode(text.slice(lastIndex)));
        }

        // Replace original text node
        node.parentNode.replaceChild(frag, node);
      } else if (node.nodeType === Node.ELEMENT_NODE) {
        // Don‚Äôt recurse into the glossary panel or already-processed spans
        if (
          node.closest("aside#glossary-panel") ||
          node.classList.contains("glossary-inline-term")
        ) {
          return;
        }

        let child = node.firstChild;
        while (child) {
          const next = child.nextSibling;
          walk(child);
          child = next;
        }
      }
    }

    walk(contentRoot);

    // 5) (Optional but handy) ‚Äì click on inline term opens glossary panel
    document.addEventListener("click", function (e) {
      const termSpan = e.target.closest(".glossary-inline-term");
      if (!termSpan) return;

      const termKey = termSpan.dataset.term;
      const panel = document.getElementById("glossary-panel");
      if (!panel) return;

      // If you have a minimised state, expand it here
      panel.classList.remove("glossary-minimised");

      // If you have a search box in the glossary, you can auto-fill it:
      const searchInput = document.getElementById("glossary-search-input");
      if (searchInput) {
        searchInput.value = termKey;
        const event = new Event("input", { bubbles: true });
        searchInput.dispatchEvent(event);
      }

      // Optional: if your glossary renders a list with term elements,
      // you can also scroll to / highlight the specific entry there.
      // (We can wire this once I see the exact glossary markup.)
    });
  });
</script>





  
  
  
  
  
</body>
</html>


